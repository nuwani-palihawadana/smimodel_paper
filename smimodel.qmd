---
title: "Sparse Multiple Index Models for High-dimensional Nonparametric Forecasting"
author:
- familyname: Palihawadana
  othernames: Nuwani
  address:
    - Department of Econometrics & Business Statistics
    - Clayton VIC 3800
    - Australia
  email: nuwani.kodikarapalihawadana@monash.edu
  correspondingauthor: true
- familyname: Hyndman
  othernames: Rob J
  address:
    - Department of Econometrics & Business Statistics
    - Clayton VIC 3800
    - Australia
  email: rob.hyndman@monash.edu
- familyname: Wang
  othernames: Xiaoqian
  address:
    - Department of Econometrics & Business Statistics
    - Clayton VIC 3800
    - Australia
  email: xiaoqian.wang@monash.edu
abstract: High-dimensionality is a common phenomenon in real-world forecasting problems. Oftentimes, forecasts are contingent on a long history of predictors, while the relationships between some predictors and the response of interest exhibit complex nonlinear patterns. In such a situation, a nonlinear "transfer function" model, with additivity constraints to mitigate the issue of *curse of dimensionality*, is a conspicuous choice. Particularly, nonparametric *additive index models* greatly reduce the number of parameters to be estimated in comparison to a general additive model. In this paper, we present a novel algorithm for estimating high-dimensional nonparametric additive index models, with simultaneous variable selection, which we call ***SMI*** (***S***parse ***M***ultiple ***I***ndex) ***Model***. The SMI Modelling algorithm is based on an iterative procedure that applies mixed integer programming to solve an $\ell_{0}$-regularised nonlinear least squares problem. We demonstrate the functionality and the characteristics of the proposed algorithm through a simple simulation exercise. We also illustrate the use of the SMI Modelling algorithm in two empirical applications related to forecasting heat exposure related daily mortality and daily solar intensity.
keywords: "Additive index models, Variable selection, Dimension reduction, Mixed integer programming"
wpnumber: no/yr
jelcodes: C10,C14,C22
blind: false
cover: true
toc: false
number-sections: true
fig-height: 5
fig-width: 8
cite-method: biblatex
bibliography: references.bib
biblio-style: authoryear-comp
keep-tex: true
format:
  wp-pdf:
    knitr:
      opts_chunk:
        dev: "CairoPDF"
execute:
  echo: false
  warning: false
  message: false
  cache: true
---

```{r}
#| label: load
#| cache: false
# Load all required packages
library(tsibble)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(knitr)
library(kableExtra)

theme_set(theme_get() + theme(text = element_text(family = "Source Sans Pro")))
```

# Introduction {#sec-introduction}

Forecasts are often contingent on a very long history of predictors. For example, when forecasting half-hourly electricity demand, it is common to use at least a week of historical half-hourly temperatures and other weather observations [@HF2010]. Similarly, when forecasting bore levels, rainfall data from up to thousand days earlier can impact the result [@Bakker2019] due to the complex flow dynamics of rainfall into aquifers.

On the other hand, in most of these applications, the relationships between the predictors and the response variable exhibit complex nonlinear patterns. For instance, the relationship between electricity demand and temperature is often nonlinear [@HF2010; @FH2012].

These examples suggest a possible nonlinear "*transfer function*" model of the form
$$
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}, y_{t-1},\dots,y_{t-k}) + \varepsilon_{t},
$$ {#eq-transferfun}
where $y_{t}$ is the observation of the response variable at time $t$, $\bm{x}_{t}$ is a vector of predictors at time $t$, and $\varepsilon_{t}$ is the random error. By including lagged values of $y_{t}$ along with the lagged predictors, we allow for any serial correlation in the data. However, it makes the resulting function difficult to interpret. An alternative formulation is
$$
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}) + g(\varepsilon_{t}, \varepsilon_{t-1},\dots,\varepsilon_{t-k}),
$$
which is more difficult to estimate, but makes it simpler to interpret the effect of the predictors on the response variable.

When applying the transfer function model to forecast lengthy time series with complex patterns, the form of $f$ is nonlinear, involving complicated interactions, and with a high value of $p$.

Typically, the form of $f$ involves many ad hoc model choices. It is essentially impossible to estimate a $p$-dimensional function for large $p$ due to the curse of dimensionality [@Bellman57; @Stone82]. Instead, we normally impose some form of additivity, along with some low-order interactions.

For example, @FH2012 proposed a ***semi-parametric additive model*** to obtain short-term forecasts of the half-hourly electricity demand for power systems in the Australian National Electricity Market. In this model, $f$ is assumed to be fully additive, and is used to capture the effects of recent predictor values on the demand. The main objective behind the use of this proposed semi-parametric model is to allow nonparametric components in a regression-based modelling framework with serially correlated errors [@FH2012]. The model fitted for each half-hourly period ($q$) can be written as
$$
 \log(y_{t,q}) = h_{q}(t) + f_{q}(w_{1,t},w_{2,t}) + a_{q}(y_{t-,p}) + \varepsilon_{t},
$$
where the response variable is the logarithm of electricity demand at time $t$ (measured in the half-hourly intervals) during period $q$. The term $h_{q}(t)$ models several calendar effects that are included as linear terms. The temperature effects are modelled using the nonparametric component $f_{q}(w_{1,t},w_{2,t})$, while the nonparametric term $a_{q}(y_{t-,p})$ captures the lagged effects of the response. It is important to notice here that the error term $\varepsilon_{t}$ is serially uncorrelated in each half-hourly model, because the serial correlation is eliminated by the inclusion of the lagged responses in the model. However, there will still be some correlation between the residuals from the various half-hourly models [@FH2012].

Similarly, a ***distributed lag model*** was proposed by @Wood2017 to forecast daily death rate in Chicago using measurements of several air pollutants. In this model, the response variable is modelled via a sum of smooth functions of lagged predictor variables, which is quite similar in nature to the semi-parametric additive model used by @FH2012. However, unlike in @FH2012, @Wood2017 suggested to allow the smooth functions for lags of the same covariate to vary smoothly over lags, preventing large differences in estimated effects between adjacent lags. Thus, the model is of the form
$$
 \log(y_{t}) = f_{1}(t) + \sum_{k=0}^{K} f_{2}(p_{t-k}, k) + \sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k),
$$
where $y_{t}$ is the death rate at day $t$, and $f_{1}$ is a nonparametric term to capture the *time* effect. The model incorporates the current value ($k = 0$) and several lagged values ($k = 1, \dots, K$) of the predictors, where the *distributed lag effect* of a single predictor variable, and of an interaction of two predictor variables are captured by the sum of nonparametric terms $\sum_{k=0}^{K} f_{2}(p_{t-k}, k)$ and $\sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k)$ respectively. The smooth functions $f_{2}$ and $f_{3}$ are proposed to be estimated using *tensor product smooths*.

For more examples, @Ho2020 used semi-paramteric additive models to estimate ground-level $PM_{2.5}$ concentrations in Taiwan, while nonparametric additive models were utilised by @Ibrahim2021 for predicting census survey response rates. Furthermore, @Ravindra2019 provided a comprehensive review of the applications of additive models for environmental data, with a special focus on air pollution, climate change, and human health related studies.

While such models have been used to address problems including electricity demand, air quality related mortality rate and groundwater level forecasting etc. [@FH2012; @HF2010; @Wood2017; @Peterson2014; @Rajaee2019], there are still a number of unresolved issues in their applications. In this paper, we attempt to address two of those issues. Firstly, even though nonparametric additive models act as a remedy to the curse of dimensionality as we discussed earlier, the estimation of the model is still challenging in a high-dimensional setting due to the large number of nonparametric components to be estimated. Secondly, there is a noticeable subjectivity in the selection of predictor variables (from the available predictors) for the model, where in most of the applications of interest we discussed above, the predictor choices in the final model are mainly based on empirical explorations or domain expertise.

There are a number of previous studies that have attempted to address the issue of variable selection in nonparametric/semi-parametric additive models to some extend, using various techniques. For example, @Huang2010 used a *Least Absolute Shrinkage and Selection Operator (LASSO)* [@Tibshirani1996] based procedure for variable selection in nonparametric additive models, whereas @FH2012 used a straightforward backward elimination technique to achieve selection. Moreover, @Ibrahim2021 and @Hazimeh2023 used Mixed Integer Programming based methodologies to provide a solution to the *best subset selection* problem in nonparametric additive models. More details of these methods are discussed later in @sec-background.

In this paper, however, we are interested in high-dimensional applications that exhibit complicated interactions among predictors (specially in the presence of large number of lagged variables), as well as correlated errors. In such a situation, *"index models"* (refer @sec-Index) seem to be useful for improving the flexibility of the broader class of nonparametric additive models [@Radchenko2015], while mitigating the difficulty of estimating a nonparametric component for each individual predictor. 

To our knowledge, no previous research has been done to look at how the predictor choices can be made more objective and principled in nonparametric additive index models. Hence, our goal was to develop a methodology for optimal predictor selection in the context of high-dimensional nonparametric additive index models. Moreover, due to computational advancements in the field, the use of *Mathematical Optimisation* concepts in solving statistical problems has gained a lot of interest in the recent past [@Theusl2020]. This motivated us to develop a variable selection algorithm based on mathematical optimisation techniques.

Additionally, it is crucial to point out that any such variable selection methodology naturally renders inferential statistics invalid, since we do not assume the resulting model obtained through the variable selection procedure to represent the true data generating process. Hence, our focus in this paper is only on improving forecasts, but not on making inferences on the resulting parameter estimates.

The rest of this paper is organised as follows. In @sec-background, we provide a concise exposition of related ideas and previous work, while establishing the foundation for this paper. @sec-SMI presents our proposed model, *Sparse Multiple Index Model* (SMI Model), and describes the variable selection algorithm and estimation procedure. In @sec-simulation, we demonstrate the functionality and the characteristics of the proposed algorithm through a simulation experiment. @sec-application illustrates two empirical applications of the proposed estimation and variable selection methodology, related to forecasting heat exposure related daily mortality and daily solar intensity. Concluding remarks are given in @sec-conclusion.

# Background {#sec-background}

## Variable Selection in Nonparametric Additive Models

As discussed in @sec-introduction, the estimation of nonparametric function $f$ (@eq-transferfun) becomes infeasible in high-dimensional settings (i.e. number of predictors is very large) due to curse of dimensionality. As a result, *nonparametric additive models* have been employed with growing popularity. Let $(y_{i}, \bm{x}_{i}), i = 1, \dots, n$, be independent and identically distributed (i.i.d) observations, and $\bm{x}_{i} = (x_{i1}, \dots, x_{ip})^{T}$ be a $p$-dimensional vector of predictor values. Then a nonparametric additive model can be written as
$$
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \varepsilon_{i}, \quad i = 1, \dots, n,
$$ {#eq-add}
where $f_{j}$'s are unknown functions (probably non-linear and smooth), and $\varepsilon_{i}$ is the random error [@Lian2012]. Even such an additivity condition is imposed, estimating the optimal predictive model will still be troublesome when $p$ is very large (probably even larger than the sample size $n$) due to over-fitting [@Lian2012]. Thus, it is natural to bring in the sparsity assumption, and assume that some of $f_{j}$'s are zero, which gives rise to the need of a variable selection method to differentiate between zero and non-zero components, while estimating the non-zero components [@Huang2010].

### Backward Elimination {#sec-backward}

In the problem of forecasting long-term peak electricity demand, @HF2010 used a stepwise procedure for variable selection through cross-validation. In the each half-hourly model fitted, the data is split into training and validation sets, and the predictors are selected into the model based on the Mean Squared Error (MSE) calculated for the validation set. Starting from the full model, the predictive power of each variable is evaluated by dropping one at a time. A predictor, the removal of which contributed to a decrease in the validation MSE, is omitted from the model in subsequent steps [@HF2010]. @FH2012 used a similar method except for the fact that they considered the Mean Absolute Percentage Error (MAPE) as the selection criterion. Therefore, both of these prior work use stepwise variable selection methodology based on out-of-sample forecasting performance.

### Penalisation Methods

According to @Huang2010, there are numerous penalised methods for variable selection and parameter estimation in high-dimensional settings, including the *bridge estimator* proposed by @Frank1993, the *Least Absolute Shrinkage and Selection Operator (LASSO)* by @Tibshirani1996, the *Smoothly Clipped Absolute Deviation Penalty* (SCAD) by @Fan2001, and the *Minimum Concave Penalty* (MCP) by @Zhang2010. Among them, we observe that the LASSO and the SCAD penalties are appearing popularly in literature.

@Tibshirani1996 introduced the regularisation method, ***LASSO***, for estimating linear models, which minimises the sum of squared residuals subject to the $\ell_{1}$ penalty on the coefficients. Assume the classical linear regression model $y_{i} = \sum_{j=1}^{p} {\beta_{j}x_{ij}} +\varepsilon_{i}$, fitted for the data $(y_{i}, \bm{x_{i}})$, $i = 1, \dots, n$, where $y_{i}$ is the response, $\bm{x_{i}} = (x_{i1}, \dots, x_{ip})^T$ is a $p$-dimensional vector of predictors, $\bm{\beta} = (\beta_{1}, \dots, \beta_{p})^{T}$ is the parameter vector corresponding to $\bm{x_{i}}$, and $\varepsilon_{i}$ is the random error. Then, the LASSO estimator, $\bm{\hat{\beta}}_{LASSO}$, can be obtained by
$$
 \bm{\hat{\beta}}_{LASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {|\beta_{j}|}\right\},
$$
where $\bm{x_{j}} = \left (x_{1j}, \dots, x_{nj}\right )^{T}$, and $\lambda$ is a non-negative tuning parameter. The LASSO estimator reduces to the Ordinary Least Squares (OLS) estimator if $\lambda$ is equal to zero [@Konzen2016]. Due to the nature of the penalty applied, LASSO shrinks some of the coefficients towards zero, and sets the others exactly to zero, where the estimation of coefficients and variable selection are performed simultaneously [@Konzen2016].

While showing that the LASSO is not consistent for variable selection in certain situations, @Zou2006 introduced ***Adaptive Lasso*** (popularly known as "adaLASSO"); an extension of the LASSO method, which uses adaptive weights to penalise coefficients using the LASSO (i.e. $\ell_{1}$) penalty. Thus, the adaLASSO objective function can be written as
$$
 \bm{\hat{\beta}}_{adaLASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {w_{j}|\beta_{j}|}\right\},
$$
where the vector of weights $\bm{w} = \left (w_{1}, \dots, w_{p} \right )^{T}$ is estimated by $\bm{\hat{w}} = 1/|\bm{\hat{\beta}}|^{\gamma}$ for $\gamma > 0$, which is a tuning parameter, and $\bm{\hat{\beta}}$ being any consistent estimator of $\bm{\beta}$ [@Zou2006].

@Yuan2006 considered the problem of selecting groups of variables, and discussed extensions of three variable selection and estimation methods namely, *LASSO* [@Tibshirani1996], *Least Angle Regression Selection* [LARS, @Efron2004], and *Non-negative Garrotte* [@Breiman1995]. Consider an $n$-dimensional response vector $\bm{y}$, and an $n \times p$ matrix of predictor values $\bm{X}$. Then the ***Group Lasso*** estimator of the coefficients vector $\bm{\beta}$ is obtained by minimising
$$
 \frac{1}{2}\left\lVert\bm{y} - \sum_{\ell=1}^{L} {\bm{X}_{\ell}\bm{\beta}_{\ell}}\right\rVert_{2}^{2} + \lambda\sum_{\ell=1}^{L} {\lVert\bm{\beta}_{\ell}\rVert}_{\bm{K}_{\ell}},
$$
where $\bm{X}_{\ell}$ is an $n \times p_{\ell}$ sub-matrix in $\bm{X}$ that corresponds to the $\ell^{th}$ group of predictors ($p_{\ell}$ is the number of predictors in $\ell^{th}$ group), $\bm{\beta}_{\ell}$ is the corresponding vector of coefficients, $\ell = 1, \dots, L$, $\lVert\bm{\beta}_{\ell}\rVert_{\bm{K}_{\ell}} = (\bm{\beta}_{\ell}' \bm{K}_{\ell} \bm{\beta}_{\ell})^{\frac{1}{2}}$ with $\bm{K}_{1}, \dots, \bm{K}_{L}$ being a set of given positive definite matrices, and $\lambda$ is a non-negative tuning parameter. Moreover, @Simon2013 proposed ***Sparse-Group Lasso***, which is a convex combination of general Lasso and Group Lasso methods, where the focus is on both "groupwise sparsity" (the number of groups with at least one nonzero coefficient), and "within group sparsity" (the number of nonzero coefficients within each nonzero group).

According to @Fan2001, a penalty function used in penalised least squares approaches should have three properties. Firstly, it should be singular at origin to generate a solution that is sparse. Secondly, it should fulfill certain conditions to be stable in model selection. Finally, it should be able to generate unbiased estimates for large coefficients via being bounded by a constant. They argued that all those three conditions are not satisfied by the penalisation methods such as the bridge regression [@Frank1993] and the LASSO [@Tibshirani1996]. Hence they proposed the ***SCAD*** penalty function, which is defined in terms of its first derivative as
$$
 p_{\lambda}'(\theta) = \lambda\left\{I(\theta\le\lambda) + \frac{(a\lambda - \theta)_{+}}{(a - 1)\lambda}I(\theta > \lambda)\right\},
$$
for some $a > 2$, and $\theta > 0$ [@Fan2001]. According to @Fan2001, the SCAD penalty function retains the favourable properties of both best subset selection and ridge regression, while having all three desired features, i.e., sparsity, stability, and unbiasedness.

Based on the above penalisation methods that are originally developed for linear models, @Huang2010 proposed a new penalisation method for variable selection in nonparametric additive model (@eq-add), named ***Adaptive Group Lasso***. They approximated $f_{j}$'s using normalised B-spline bases, so that a linear combination of B-spline basis functions is used to represent an individual nonparametric component $f_{j}$. The proposed method is a generalisation of Adaptive Lasso method [@Zou2006] to the Group Lasso method [@Yuan2006].

When the nonparametric additive model in @eq-add is considered, an obvious possibility is that some of the additive components (i.e. $f_{j}$'s) are being linear. For example, recall the electricity demand forecasting problem [@HF2010; @FH2012], where some of the calendar effects are included into the model as linear variables, whereas lagged temperature and lagged demand variables are included using nonlinear additive components. Such situations suggest the use of *semi-parametric partially linear additive models* that can be mathematically represented as
$$
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \sum_{k=1}^{q} {w_{ik}\beta_{k}} + \varepsilon_{i}, \quad i = 1, \dots, n,
$$
where $\bm{x}_{j}$'s, $j = 1, \dots, p$, are a set of predictors that enter the model as nonparametric components, whereas $\bm{w}_{k}$'s, $k = 1, \dots, q$ are another set of predictors that are included as linear components. While several studies have assumed that the number of nonparametric components are fixed, and performed variable selection only among the linear components of the model [@Lian2012; @Guo2013; @Liu2011], @Wang2014 introduced a methodology for selecting both linear and nonlinear components simultaneously, in the context of correlated, longitudinal data. They proposed the use of a ***Penalised Quadratic Inference Function (PQIF) with double SCAD penalties*** for variable selection and model estimation, where the correlation structure of the data was incorporated into the estimation method (see @Wang2014 for details).

### Time Series Aspect

It is worthwhile to briefly mention that there are extensions of the penalisation methods discussed above, which have specifically proposed to take the autocorrelation and lag structures in time series data into account.

@Wang2007 proposed an extension of the LASSO method for Regression with Autoregressive Error (REGAR) models. @Park2013 and @Konzen2016 proposed modifications to Adaptive Lasso method to incorporate the lag structures presented in Autoregressive Distributed Lag (ADL) models into the variable selection and estimation methodology. The ***Ordered Lasso*** was introduced by @Tibshirani2016 to deal with time-lagged regression problems, where we forecast the response value at time $t$ using the predictor values from $K$ previous time points, assuming that the magnitude of regression coefficients decreases as the lagged predictor moves away from time $t$.

However, it is important to note that all the models considered in the above time series related work are linear; none of them include nonparametric terms.

## Index Models {#sec-Index}

### Single Index Model

The nonparametric additive model (@eq-add) estimates the relationship between the response and the predictors using a sum of univariate nonlinear functions corresponding to each individual predictor variable. Hence, it is incapable of handling the interactions among the predictors, which are ubiquitous in real-world problems [@Zhang2008].

As a remedy, the ***Single Index Model***, a generalisation of the linear regression model where the linear predictor is replaced by a semi-parametric component, is popularly being used in the literature [@Radchenko2015]. Let $y_{i}$ be the response, and $\bm{x}_{i}$ be a $p$-dimensional predictor vector. Then the single index model can be written as
$$
  y_{i} = g \left ( \bm{\alpha}^{T} \bm{x}_{i} \right ) + \varepsilon_{i}, \quad i = 1, \dots, n,
$$
where $\bm{\alpha}$ is a $p$-dimensional vector of unknown coefficients (i.e. parameters), $g$ is an unknown univariate function, and $\varepsilon_{i}$ is the random error [@Stoker1986; @Hardle1993]. The linear combination $\bm{\alpha}^{T} \bm{x}_{i}$ is called the *index*. Single index model is viewed as a viable alternative to the additive model since it offers more flexibility and interpretability [@Radchenko2015].

According to @Radchenko2015, single index models have widely been used in scenarios with fairly low and moderate dimensionality, where the corresponding estimation and variable selection techniques are not directly applicable to the high-dimensional setting. The error sum of squares of the model being non-convex with respect to index coefficients, is the main reason behind the existence of very limited number of methods in high-dimensional case [@Radchenko2015]. For an extensive summary of available methods, we refer to @Radchenko2015.

### Multiple Index Models {#sec-multi-index}

**Projection Pursuit Regression**

@Friedman1981 introduced ***Projection Pursuit Regression (PPR)*** by extending the nonparametric additive model (@eq-add) to enable the modelling of interactions among predictor variables. On the other hand, PPR is an extension of the single index model to an  ***Additive Index Model ***, given by
$$
  y_{i} = \sum_{j=1}^{q} {g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} + \varepsilon_{i}, \quad i = 1, \dots, n,
$$
where $y_{i}$ is the response, $\bm{x}_{i}$ is a $p$-dimensional predictor vector, $\bm{\alpha}_{j} = \left ( \alpha_{j1}, \dots, \alpha_{jp} \right )^{T}, j = 1, \dots, q$ are $p$-dimensional projection vectors (or vectors of *"index coefficients"*), $g_{j}$'s are unknown univariate functions, and $\varepsilon_{i}$ is the random error.

Instead of estimating a single index, PPR estimates multiple indices and connects them to the response through a sum of univariate nonlinear functions. These indices are constructed through a *Projection Pursuit (PP)* [@Kruskal1969; @Friedman1974] algorithm, which is considered to be "interesting" low-dimensional projections of a high-dimensional feature space, obtained through the maximisation of an appropriate objective function or a "projection index" [@Huber1985].

According to @Zhang2008, PPR increases the power of additive models in high-dimensional settings, but it has two major drawbacks. Firstly, since PP increases the freedom of the additive model, it tends to overfit in a situation, where there are a lot of unimportant predictors. Secondly, the interpretation of the model estimated by PPR will be troublesome as many non-zero elements will be present in each projection vector $\bm{\alpha}_{j}$. To overcome these issues, @Zhang2008 introduced an $\ell_{1}$ regularised projection pursuit algorithm, where the resultant regression model is named as ***Sparse Projection Pursuit Regression*** (SpPPR). In SpPPR, an $\ell_{1}$ penalty (i.e. a LASSO penalty) on index coefficients is added to the cost function (the squared error) at each iteration of the PP, thereby performing variable selection and model estimation simultaneously. See @Zhang2008 for more details.

Although @Zhang2008 claimed that the SpPPR algorithm can detect important predictors even in a noisy data set, our experiments show that it is not particularly scalable for large data sets with both higher number of predictors and observations. \newline

**Group-wise Additive Index Model** 

Even though PPR introduces flexibility and the ability to model interactions among predictors into additive models, the indices obtained through PPR contain all the predictors at hand. Hence, even with a variable selection mechanism like SpPPR [@Zhang2008], PPR creates indices possibly by mixing heterogeneous variables in a single linear combination, making very little sense in terms of interpretability [@Masselot2022].

Typically, in many real-world problems, natural groupings can be identified in predictor variables. For example, naturally interacting variables can be grouped together, such as several lags of a predictor, weather related variables, and genes or proteins that are grouped by biological pathways in a biological study [@Masselot2022; @Wang2015]. 

This suggests the use of a ***Group-wise Additive Index Model (GAIM)***, which can be written as
$$
  y_{i} = \sum_{j = 1}^{p} g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \varepsilon_{i}, \quad i = 1, \dots, n, 
$$
where $y_{i}$ is the univariate response, $\bm{x}_{ij} \in \mathbb{R}^{l{j}}$, $j = 1, \dots, p$ are naturally occurring $p$ groups of predictors, which are $p$ non-overlapping subsets of $\bm{x}_{i}$ - the vector of all predictors, $\bm{\alpha}_{j}$ is a $l_{j}$-dimensional vector of index coefficients corresponding to the index $h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}$, $g_{j}$ is an unknown (possibly nonlinear) component function, and $\varepsilon_{i}$ is the random error, which is independent of $\bm{x}_{i}$ [@Wang2015-mp; @Masselot2022].

Since GAIM uses groups of predictors that are naturally or logically belonging together to construct indices, such derived indices will be more expressive and interpretable. However, at the same time, this introduces a certain level of subjectivity into the model formulation as different users can group the available predictors in different ways based on different logical reasoning. 

In this paper, our aim is to reduce that subjectivity induced by personal judgment or domain expertise. Hence, we propose a methodology that injects more objectivity into the estimation of multiple index models by algorithmically grouping predictors into indices, resulting in a model with a higher predictive accuracy. \newline

**Constrained Group-wise Additive Index Model**

The ***Constrained Group-wise Additive Index Model (CGAIM)*** was proposed by @Masselot2022 for constructing comprehensive and easily interpretable indices from a large set of explanatory variables. The model of interest is a *semi-parametric group-wise additive index model* given by
$$
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
$$ 
where $y_{i}$ is the univariate response, $\beta_{0}$ is the model intercept, $\bm{x}_{ij} \in \mathbb{R}^{l_{j}}$, $j = 1, \dots, p$ are naturally occurring $p$ groups of predictor vectors (i.e. it is assumed that the predictor groupings are known in advance), which are $p$ subsets of $\bm{x}_{i}$ - the $q$-dimensional vector of all predictors entering indices, $\bm{\alpha}_{j}$ is the vector of index coefficients corresponding to the index $h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}$, and $g_{j}$ is the corresponding nonlinear link function (possibly estimated by a spline). The additional predictor variables that are helpful in predicting $y_{i}$, but do not enter any of the indices are two-fold: a covariate that relates to the response through a nonlinear function $f_{k}$, denoted by $w_{ik}$, and the vector of linear covariates denoted by $\bm{u}_{i}$.

This is an extension of the GAIM that allows to impose constraints on the index coefficients as well as on the nonlinear link functions. In CGAIM, linear constraints of the form $\bm{C}_{j}\bm{\alpha}_{j} \ge 0$ can be imposed on the index coefficients, where $\bm{C}_{j} \in \mathbb{R}^{d_{j} \times l_{j}}$, and $d_{j}$ is the number of constraints. Moreover, shape constraints such as monotonicity, convexity or concavity can be imposed on the nonparametric functions. This modification allows to incorporate prior knowledge or operational requirements into the model estimation.

First, considering only the additive index part of the model, and given $(y_{i}, x_{i1}, \dots, x_{iq}), \quad i = 1, \dots, n$ be the observed data, where the $q$ predictors are grouped into $p$ groups, the estimation problem of the CGAIM can be formulated as
$$
\begin{aligned}
  \min_{\bm{\alpha}, \beta_{0}} \quad & \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij})\right ]^{2}, \\
  \text{s.t.} \quad & \bm{C}\bm{\alpha} \ge 0, \quad g_{j} \in m,
\end{aligned}
$$ {#eq-3}
where $\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}$, $\beta_{0}$ is the model intercept, $\bm{C} \in \mathbb{R}^{d \times q}$, $d$ is the number of constraints on the index coefficients vector $\bm{\alpha}$, and $m$ is a shape constraint imposed on $g_{j}$ [@Masselot2022].

Notice that $\bm{\alpha}_{j}$s behave non-linearly in @eq-3, and hence, this is a non-linear least squares problem. Accordingly, @Masselot2022 introduced an efficient iterative algorithm for estimating the CGAIM based on ***Sequential Quadratic Programming*** (SQP), one of the most successful techniques for solving nonlinear constrained optimisation problems [@Boggs1995]. For details of the CGAIM algorithm refer to @Masselot2022. 

## Mathematical Optimisation for Variable Selection

### Mathematical Optimisation

*Optimisation* plays a major role in both decision science and physical systems evaluation. ***Mathematical Optimisation*** or ***Mathematical Programming*** can be defined as the minimisation (or maximisation) of a function subject to restrictions on the unknowns/parameters of that function [@Nocedal2006]. Hence, a mathematical optimisation problem can be written as
$$
\begin{aligned}
  \min_{\bm{x}} \quad & f_{0}(\bm{x})\\
  \text{s.t.} \quad & f_{i}(\bm{x}) \le b_{i}, \quad i = 1, \dots, m
\end{aligned}
$$ {#eq-opt}
where the vector of unknowns or parameters of the problem is given by $\bm{x} = \left ( x_{1}, \dots, x_{n} \right )^{T}$, the *objective function* is denoted by $f_{0} : \mathbb{R}^{n} \rightarrow \mathbb{R}$, the *constraint functions* are given by $f_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}$, $i = 1, \dots, m$, and the bounds of the constraints are denoted by $\bm{b} = \left (b_{1}, \dots, b_{m} \right )^{T}$. A vector of values $\bm{x^{*}}$ that results in the smallest value for the objective function among all vectors that satisfy the stated constraints, is called the *optimal* value or the *solution* to the problem [@Boyd2004]. After mathematically formulating the optimisation problem as above (@eq-opt), an appropriate *optimisation algorithm* is used to obtain the solution $\bm{x^{*}}$ [@Nocedal2006].

Based on the form of the objective function and the constraints, various types of optimisation problems are identified.

An optimisation problem is known as a ***Linear Programming*** (LP) when both the objective function and the constraints in @eq-opt (i.e. all $f_{i}$, $i = 0, \dots, m$) are linear. Hence, a LP can be written as
$$
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
$$ {#eq-lp}
where $\bm{x}$ is the vector that contains the parameters to be optimised, and $\bm{a_{0}} \in \mathbb{R}^{n}$ is the vector of coefficients of the objective function. The matrix of coefficients in the constraints is denoted by $\bm{A} \in \mathbb{R}^{m \times n}$, and $\bm{b}$ is the vector containing the upper bounds of the constraints. All LPs are *convex* optimisation problems [@Theusl2020].

The LP problem given in @eq-lp can be generalised to involve a quadratic term in the objective function, in which case it is called a ***Quadratic Programming*** (QP). A QP can be written as
$$
\begin{aligned}
  \min_{\bm{x}} \quad & \frac{1}{2} \bm{x}^{T} \bm{Q_{0}} \bm{x} + \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
$$
where $\bm{Q_{0}} \in \mathbb{R}^{n \times n}$. Unless the matrix $\bm{Q_{0}}$ is positive semi-definite, a QP is non-convex [@Theusl2020].

If a linear objective function is minimised over a *convex cone*, such an optimisation problem is called a ***Conic Programming*** (CP), which can be written as
$$
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x} \\
  \text{s.t.} \quad & \bm{A}\bm{x} + s = \bm{b}, \quad s \in \mathcal{K},
\end{aligned}
$$
where $\mathcal{K}$ denotes a nonempty closed convex cone. CPs are designed to model convex optimisation problems [@Theusl2020].

If we restrict some of the unknowns/parameters in an optimisation problem to take only integer values, then that optimisation problem is called a ***Mixed Integer Programming*** (MIP). For example, if we constraint $x_{k} \in \mathbb{Z}$ for at least one $k$, $k \in \{1, \dots, n\}$ in the optimisation problem given by @eq-opt, then the optimisation problem becomes a MIP. If all the unknowns of an optimisation problem are constrained to be integers, such a problem is referred to as a pure ***Integer Programming*** (IP), whereas if all the unknowns are bounded between zero and one (i.e. $\bm{x} \in \{ 0, 1 \}^{n}$), the optimisation problem is referred to as a ***Binary (Integer) Programming*** [@Theusl2020]. MIPs are hard to solve as they are non-convex due to the integer constraints. However, a growth in the number of commercial as well as non-commercial MIP solvers has made it possible to solve MIP problems conveniently and directly.

### Variable Selection

Mathematical optimisation is fundamentally important in statistics, as many statistical problems including regression, classification, and other types of estimation/approximation problems can be re-interpreted as optimisation problems [@Theusl2020]. Thus, the problem of variable selection - one of the prolonged interests of statisticians, has also benefited from using optimisation concepts, particularly MIP and convex optimisation, in the recent past.

For example, @Bertsimas2016 used a mixed integer optimisation procedure to solve the classical best subset selection problem in a linear regression. They developed a discrete optimisation method by extending modern first-order continuous optimisation techniques. The method can produce near-optimal solutions that would serve as warm starts for a MIP algorithm, which would choose the best $k$ features out of $p$ predictors. Similarly, @Hazimeh2020 developed fast and efficient algorithms based on coordinate descent and local combinatorial optimisation to solve the same best subset selection (or $\ell_{0}$-regularised least squares) problem through re-formulating local combinatorial search problems as structured MIPs.

Furthermore, @Hazimeh2023 proposed a group-wise variable selection methodology, based on discrete mathematical optimisation, which is applicable to both $\ell_{0}$-regularised linear regression and nonparametric additive models in a high-dimensional setting. They formulated the group $\ell_{0}$-based estimation problem as a ***Mixed Integer Second Order Cone Programming*** (MISOCP), and proposed a new customised Branch-and-Bound (BnB) algorithm [@Land1960; @Little1963] to obtain the global optimal solution to the MISOCP.

Through the study of above literature, we noticed that the mathematical optimisation based algorithms reduce computational cost of variable selection procedures in high-dimensional settings. This is largely due to the availability of efficient commercial solvers such as *Gurobi* and *CPLEX*. This motivated us to focus on a mathematical optimisation based procedure for developing our variable selection methodology.


# Sparse Multiple Index Model {#sec-SMI}

In this section, we develop a ***Sparse Multiple Index Model*** (hereafter referred to as SMI Model) to establish an objective and a principled methodology for estimating high-dimensional nonparametric additive index models, highlighting predictor grouping along with optimal predictor selection.

## The SMI Model {#sec-model}

The model of interest is a semi-parametric additive index model, which can be written as
$$
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
$$ {#eq-semipara}
where $y_{i}$ is the univariate response, $\beta_{0}$ is the model intercept, $\bm{x}_{ij} \in \mathbb{R}^{l_{j}}$, $j = 1, \dots, p$ are $p$ subsets of all the predictors entering indices, $\bm{\alpha}_{j}$ is a vector of index coefficients corresponding to the index $h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}$, $g_{j}$ is a nonlinear link function (possibly estimated by a spline). Note that we also allow for the inclusion of predictors that do not enter any of the indices. These additional predictors are two-fold: a covariate $w_{ik}$ that relates to the response through the nonlinear function $f_{k}$, $k = 1, \dots, d$, and linear covariates denoted by $\bm{u}_{i}$.

We make three main assumptions as follows to define the **SMI Model**:

1.    The number of indices (i.e. the number of subsets of predictors) $p$ is unknown;

2.    The predictor grouping among indices is unknown; and

3.    No predictor enters more than one indices (i.e. overlapping of predictors among indices is not allowed).

These assumptions further imply that the index coefficients $\bm{\alpha}_{j}$s and the nonlinear link functions $g_{j}$s are also unknown, and will need to be estimated.

One of the key features of the proposed SMI model is to allow for zero index coefficients for predictors, so that the predictors with zero coefficients are dropped out from the model, achieving variable selection. The other key feature of the SMI model is its flexibility in allowing a variable number of indices, ranging from $1$ (i.e. all $q$ predictors are paased to a single index) to $q$ (i.e. each predictor goes into a separate index). Hence, both the Single Index Model and the Additive Model are special cases of the proposed SMI Model.

## Optimisation Problem Formulation

Let $q$ be the total number of predictors entering $p$ non-overlapping subsets of size $l_{j}$, $j = 1, \dots, p$ (i.e. $\sum_{j = 1}^{p} l_{j} = q$). The optimisation problem we seek to address is of the form below, where the sum of the squared error of the model (@eq-semipara) is minimised together with an $\ell_{0}$ penalty term and an $\ell_{2}$ (ridge) penalty term:
$$
\begin{aligned}
  \min_{\beta_{0}, p, \bm{\alpha}, \bm{g}, \bm{f}, \bm{\theta}} \quad \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) - \sum_{k = 1}^{d}f_{k}(w_{ik}) - \bm{\theta}^{T}\bm{u}_{i}\right]^{2} \\
  + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}\bm{1}\left(\alpha_{jm} \neq 0\right) + \lambda_{2}\sum_{j = 1}^{p}\|\bm{\alpha}_{j}\|_2^2
\end{aligned}
$$ {#eq-smi}
where $\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}$, $\bm{g} = \{g_{1}, g_{2}, \dots, g_{p}\}$, $\bm{f} = \{f_{1}, f_{2}, \dots, f_{d}\}$, $\bm{1}(\cdot)$ is the indicator function, $\lambda_{0} > 0$ is a tuning parameter that controls the number of selected predictors entering indices, and $\lambda_{2} \ge 0$ is another tuning parameter that controls the strength of the additional shrinkage imposed on the estimated index coefficients.

Applying an $\ell_{2}$-penalty in addition to the $\ell_{0}$-penalty is motivated by related literature [@Hazimeh2020; @Mazumder2022; @Hazimeh2023], where it is suggested that the prediction performance of best-subset selection is enhanced by the inclusion of an additional ridge penalty, especially when a low signal-to-noise ratio (SNR) is present.

## MIQP Formulation

To solve the optimisation problem in @eq-smi, we present a big-M based ***Mixed Integer Quadratic Programming*** (MIQP) formulation:
$$
\begin{aligned}
  \min_{\beta_{0}, p, \bm{\alpha}, \bm{g}, \bm{f}, \bm{\theta}, \bm{z}} \quad & \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}{g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} - \sum_{k = 1}^{d} {f_{k}(w_{ik})} - \bm{\theta}^{T}\bm{u}_{i}\right ]^{2} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q} \alpha_{jm}^{2} \\
  \text{s.t.} \quad & \left |\alpha_{jm}\right | \le Mz_{jm} \quad \forall j, \forall m, \\
  & \sum_{j = 1}^{p}z_{jm} \le 1 \quad \forall m, \\
  & z_{jm} \in \{0, 1\}, \\
  & j = 1, \dots, p, \quad m = 1, \dots, q,
\end{aligned}
$$ {#eq-smi-mip}
where $p$ is the (unknown) number of indices, $\bm{x}_{i}$ is the $q$-dimensional vector of all predictors entering indices, and $\bm{z} = \left (\bm{z}_{1}^{T}, \dots, \bm{z}_{p}^{T} \right )^{T}$, $\bm{z}_{j} = \left (z_{j1}, \dots, z_{jq} \right )^{T}$, $j = 1, \dots, p$ such that $z_{jm} \in \{0, 1\}, m = 1, \dots, q$ for all $j$. In other words, we introduce a binary (i.e. indicator) variable corresponding to each predictor in each index. A pre-specified *big-M parameter* is denoted by $M < \infty$, and it should be sufficiently large. If $\bm{\alpha^{*}}$ is the optimal solution to the problem given in @eq-smi-mip, then the big-M parameter should satisfy $\max \left (\left |\alpha_{jm}^{*}\right | \right ) \le M$, where $j \in \{1, \dots, p\}$, and $m \in \{1, \dots, q\}$.

Notice that, here we formulate the MIQP to include all $q$ predictors in each index so that in this case, $\bm{\alpha}_{j}$ is a $q$-dimensional vector of index coefficients. However at the same time, as mentioned earlier, we introduce a set of binary variables corresponding to each predictor in each index, which serves two main purposes: firstly, these binary variables are used to make the *"on-or-off"* decisions of the predictors in the model; secondly, they contribute to decide which predictors belong to which index.

To further elaborate, first, the big-M constraints ensure that $\alpha_{jm}$ is zero if and only if $z_{jm}$ is zero, and if $z_{jm} = 1$, then $\left |\alpha_{jm}\right | \le M$. At the same time, the $\ell_{0}$-penalty term $\lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm}$ influences some of the binary variables $z_{jm}$ to be zero, while the $\ell_{2}$-penalty term $\lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q}\alpha_{jm}^{2}$ enforces additional shrinkage on the estimated coefficients. Therefore, these components together perform a variable selection.

Next, when a set of binary variables $\{z_{1m}, z_{2m}, \dots, z_{pm}\}$ corresponding to the $m^{th}$, $m = 1, \dots, q$, predictor in all $p$ indices is considered, according to the constraint $\sum_{j = 1}^{p}z_{jm} \le 1$, only one or no binary variables in the set can take the value one, ensuring that the $m^{th}$ predictor does not repeat in more than one index. If all the elements of the set are zero, then the $m^{th}$ predictor will be dropped out from the model.

Thus, our main contribution in this paper is two-fold. Firstly, we propose a novel algorithm to objectively estimate a semi-parametric additive index model, while contributing towards an estimated model with a higher forecasting accuracy. Secondly, the proposed methodology will contribute towards estimating a parsimonious model in a high-dimensional setting, a crucial aspect of interpretability, even if the required domain knowledge for selecting the optimal set of predictors is unavailable.

## Estimation Algorithm

In this section, we show how to efficiently find a minimiser for the problem given in @eq-smi-mip. Since the number of indices $p$, the vector of index coefficients $\bm{\alpha}$, as well as the set of nonparametric functions $\bm{g}$ are unknown, it is mathematically impossible to solve the above MIQP given in @eq-smi-mip directly. Hence, we propose an iterative algorithm to solve it.

### Initialising the Index Structure and Index Coefficients {#sec-step1}

First, we need to provide an initialisation for the index structure (i.e. number of indices $p$ and the grouping of predictors among indices) and the index coefficients ($\bm{\alpha}$) of the model in order to start solving the MIQP given in @eq-smi-mip.

Based on several pre-experiments on the new algorithm, we propose five alternative methods for initialising the SMI Model as follows.

**1. "PPR" - Projection Pursuit Regression Based Initialisation:**

As discussed in @sec-Index, Projection Pursuit Regression model is a multiple index model, where each index consists of all the available predictors. Since in SMI Model we assume that there are no overlapping indices, it is impossible to use an estimated PPR model directly as a starting model for the algorithm. Thus, we follow the steps presented below to come up with a feasible initialisation for the index structure and the index coefficients.

i.    Scale all the variables of the data set by dividing each variable by its standard deviation (so that it is possible to compare the estimated coefficients among predictors).

ii.   Fit a PPR model and obtain estimated index coefficients. (The user can decide the number of indices to be estimated through *num_ind*; we use *num_ind = 5* as the default value.)

iii.  Calculate a threshold as $threshold = 0.1 \times \max(\text{PPR coefficients})$.

iv.   Set to zero all coefficients that fall below the calculated threshold.

v.    For predictors appearing in multiple indices, assign them to the index with the maximum coefficient and zero out their coefficients in other indices.

vi.   After performing the above steps i-v, if any originally estimated index has all $\bm{z}$ero coefficients, it will be excluded from the model. 

Now, the index structure and the index coefficients obtained through the above steps are considered to be a feasible initialisation for the proposed algorithm. Once the optimal SMI Model is obtained through the algorithm, each index coefficient will be back-transformed into the original scale of the respective predictor variable, reversing the scaling effect applied at the beginning.

**2. "Additive" - Nonparametric Additive Model Based Initialisation:**

As mentioned previously in @sec-model, the nonparametric additive model is a special case of the SMI Model, where the number of indices equals the number of predictors entering indices ($p = q$) (i.e. each index contains only one predictor). Hence, it is a feasible starting point for our algorithm.

**3. "Linear" - Linear Regression Based Initialisation:**

In this option, we first regress the response variable on the predictors using a multiple linear regression. Then, we construct a single index (i.e. $p = 1$) using the estimated regression coefficients as the index coefficients of the predictors. Since single index model is also a special case of the SMI Model, this will be a candidate starting point.

**4. "Multiple" - Selecting an Initial Model by Comparing Multiple Models:**

Through our pre-experiments on the new algorithm, we identified that in some situations, the final optimised SMI Model often changes based on the initialisation provided to the algorithm. Hence, for this initialisation option, we consider several different models as initialisations, optimise the SMI Model for each of them, and pick the initial model that results in the lowest loss for the MIP problem.

Here, users can decide on the number of models to be considered (*num_models*) as well as the the number of indices for all models (*num_ind* - same for all models). We use *num_models = 5* and *num_ind = 5* as default values.

**5. "User Input" - User Specified Initialisation:**

This option allows users to provide their desired initialisation for the algorithm by specifying the number of indices, the grouping of predictors among indices and the initial index coefficients. This option provides the freedom for users to utilise their domain expertise or prior knowledge in initialising the algorithm.

In all of the above initialisation options, once the estimate for $\bm{\alpha}$ is obtained, the estimated initial index coefficients for each index ($\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j, init}$) are scaled to have unit norm to ensure identifiability.

The characteristics and the performance of the proposed algorithm differ based on the chosen initialisation options, depending on the application scenario. Further insights into these aspects will be discussed in @sec-simulation.

### Estimating Nonlinear Functions {#sec-step2}

Once we have an estimate for $\bm{\alpha}$, estimating the SMI Model is equivalent to estimating a GAM as
$$
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\hat{h}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
$$
where $y_{i}$ is the response, and $\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{i}$ is the estimated index.

The R packages ***mgcv*** [@Wood2011] and ***gam*** [@Hastie2023], for example, can be used to fit GAMs.

### Updating the Index Structure and Index Coefficients {#sec-step3}

We update index coefficients $\bm{\alpha}^{new}$ through a MIQP:
$$
\begin{aligned}
  \min_{\bm{\alpha}^{new}, \bm{z}^{new}} & (\bm{\alpha}^{new} - \bm{\alpha}^{old})^{T}\bm{V}^{T}\bm{V}(\bm{\alpha}^{new} - \bm{\alpha}^{old}) - 2(\bm{\alpha}^{new} - \bm{\alpha}^{old})^{T}\bm{V}^{T}\bm{r} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm}^{new} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q}\alpha_{jm}^{(new)2} \\
  \text{s.t. } & \left |\alpha_{jm}^{new}\right | \le Mz_{jm}^{new} \quad \forall j, \forall m,\\
  & z_{jm}^{new} \in \{0, 1\}, \\
  & \sum_{j = 1}^{p}z_{jm}^{new} \le 1 \quad \forall m, \\
  & j = 1, \dots, p, \quad m = 1, \dots, q,
\end{aligned}
$$ {#eq-smi-update}
where $\bm{\alpha}^{old}$ is the current value of $\bm{\alpha}$, and $z_{jm}^{new}$ are the updated set of binary variables to be estimated. $\bm{V}$ is the matrix of partial derivatives of the right hand side of @eq-semipara, with respect to $\bm{\alpha}_{j}$. The $i^{th}$ line of $\bm{V}$ contains $\left [ \bm{v}_{i1}, \dots, \bm{v}_{ip} \right ]$, where $\bm{v}_{ij} = \bm{x}_{i}g_{j}^{'}(h_{ij})$. The current residual vector, which contains $r_{i} = y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{(old)T}\bm{x}_{i})$, is denoted by $\bm{r}$. It is important to note that the additional covariates $w_{ik}$ and $\bm{u}_{i}$ do not step in to the process of updating $\bm{\alpha}_{j}$, because they are constants with respect to $\bm{\alpha}_{j}$, and thus they disappear from $\bm{V}$.

Similar to the explanation given by @Masselot2022, the MIQP objective function in above @eq-smi-update ignores the Hessian (or the matrix of second derivatives of @eq-semipara, with respect to $\bm{\alpha}_{j}$), and considers only the matrix of first derivatives, which is a quasi-Newton step. The quasi-Newton Method is an alternative to the Newton's Method, avoiding the calculation of the Hessian to circumvent its computational burden [@Peng2022]. Therefore, the $\bm{\alpha}$ updating step given in above @eq-smi-update is assured to be in a descent direction.

When $\bm{\alpha}^{new}$ is obtained, if any of the estimated individual index coefficient vectors $\bm{\alpha}_{j}^{new}$ contains all $\bm{z}$eros (i.e. zero index), such indices will be dropped out from the model. Furthermore, similar to @sec-step1, once the new estimate $\bm{\alpha}^{new}$ is obtained, we scale each estimated index coefficient vector $\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j}^{new}$ to have unit norm.

The algorithm alternates updating the index coefficients $\bm{\alpha}$ and estimating nonlinear functions $\bm{g}$ with the updated $\bm{\alpha}$ until meeting one of the three criteria: (i) the reduction ratio of the objective (loss) function value in @eq-smi-mip, calculated between consecutive iterations, reaches a pre-specified convergence tolerance; (ii) the loss increases consecutively for three iterations; or (iii) the maximum number of iterations is reached. The selection of convergence tolerance and maximum iterations depends on the specific problem or data. In the empirical applications in Section @sec-application, we used a convergence tolerance of $0.001$ and a maximum of $50$ iterations, stopping at the first reached criterion.

Next, we consider changing the index structure of the model to exploit any benefits in terms of further minimising the loss function in @eq-smi-mip. As indices can be automatically reduced by dropping zero indices in each optimisation iteration, this step focuses on potential index additions to the current model. Specifically, we consider adding a new index to the current model by identifying dropped predictors.
If applicable, a new index is constructed with these dropped predictors and the alternating updating process in the previous step is repeated. This increment step continues until one of these termination criteria is met: (i) the number of indices reaches $q$, selecting the final model as output; (ii) loss increases after the increment, selecting the previous iteration model as the final SMI model; or (iii) The solution maintains the same number of indices as the previous iteration, and the absolute difference between two successive iterations is not larger than a pre-specified tolerance, choosing the model with the smaller loss as the final SMI model in this case.

Note that, to obtain an estimated model with the best possible forecasting accuracy, it is important to select appropriate values for the non-negative penalty parameters $\lambda_{0}$ and $\lambda_{2}$. One possible way to do this is to estimate the model over a grid of possible values for $\lambda_{0}$ and $\lambda_{2}$, and then select the combination that yields the lowest loss function value. Moreover, it is also crucial to choose a suitable value for the big-M parameter, as the strength of the MIP formulation depends on the choice of a good lower bound [@Bertsimas2016]. According to @Hazimeh2023, several methods have been used to select $M$ in practice. For a description on estimating $M$ in a linear regression setting, refer to @Bertsimas2016.

The following ***Algorithm 1*** summarises the key steps of the SMI Modelling algorithm. \newline

***Algorithm 1: SMI Modelling Algorithm***

1.  Initialise index structure and index coefficients $\bm{\alpha}$:\
    Initialise $p$, predictors grouping among indices, and obtain $\bm{\alpha}^{init}$ using one of the five options in @sec-step1. Then scale each $\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j}^{init}$ to have unit norm.

2.  Estimate nonlinear functions $g_{j}$s:\
    Estimate $g_{j}$s using a GAM taking $y_{i}$ as the response, $\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{i}$s as predictors.

3.  Update index coefficients $\bm{\alpha}$:\
    Estimate updated value $\bm{\alpha}^{new}$ through the MIQP in @eq-smi-update, and scale each $\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j}^{new}$ to have unit norm.
    
4.  Iterate steps 2 and 3 until convergence, loss increase for three consecutive iterations, or reaching the maximum iterations.

5.  Update index structure:\
    Include a new index consisting of dropped predictors if applicable, and proceed to step 4. Otherwise, terminate the algorithm.
    
6.  Iterate step 5 with increased number of indices $p$:\
    Increase $p$ by one in each iteration of step 5 until meeting one of the termination criteria below.
    - The number of indices in the iteration reaches $q$; select the final fitted model as output.
    - Loss increases after the increment; select previous iteration model as the final SMI model.
    - The solution maintains the same number of indices as the previous iteration, and the absolute difference of index coefficients between two successive iterations is not larger than a pre-specified tolerance; select the model with smaller loss as the final SMI model.

Throughout the experiments in the paper, we use $M = 10$, a convergence tolerance of $0.001$, and a maximum of $50$ iterations in step 4 of Algorithm 1, and a convergence tolerance of $0.001$ for coefficients in estimating all the SMI Models.

# Simulation Experiment {#sec-simulation}

This section presents the results of a simple simulation experiment designed to demonstrate the performance and characteristics of the proposed SMI Modelling algorithm. Particularly, we try to investigate how the estimated SMI Model varies depending on the initialisation (as discussed in @sec-step1) used. 

## Data Generation {#sec-datagen}

**Generating predictor variables:**

First, we generate two series each of length 1205: $x_{0}$, from a uniform distribution on the interval $[0, 1]$, and $z_{0}$, from random normal distribution $N(5, 4)$. Next, we construct lagged series up to $5^{th}$ lag of both $x_{0}$ and $z_{0}$. These current and lagged series of $x_{0}$ and $z_{0}$ (i.e. $\bm{x} = \{x_{0}, x_{1}, \dots, x_{5}\}$, and $\bm{z} = \{z_{0}, z_{1}, \dots, z_{5}\}$) were taken as predictors in the simulation experiment.

**Generating response variables:**

We generated two response variables $y_{1}$ and $y_{2}$, with two different index structures: single-index and 2-index, and added a random normal noise component with two different strengths as follows:

-   Low noise level - $N(\mu = 0, \sigma = 0.1)$:\
      $y_{1} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + \epsilon, \quad \epsilon\sim N(0, 0.01)$\
      $y_{2} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + (0.35*x_{2} + 0.7*x_{5})^2 + \epsilon, \quad \epsilon\sim N(0, 0.01)$
      
-   High noise level - $N(\mu = 0, \sigma = 0.5)$:\
      $y_{1} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + \epsilon, \quad \epsilon\sim N(0, 0.25)$\
      $y_{2} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + (0.35*x_{2} + 0.7*x_{5})^2 + \epsilon, \quad \epsilon\sim N(0, 0.25)$
      
Hence, the response $y_{1}$ is constructed using a single index consisting of the predictor variables $x_{0}, x_{1}$, and $x_{3}$, whereas the other response $y_{2}$ is constructed using two indices, where the first index consists of the predictors $x_{0}, x_{1}$, and $x_{3}$, and the second index consists of $x_{2}$ and $x_{5}$. Neither the variable $x_{4}$ nor any of the $z$ variables were used in generating $y_{1}$ and $y_{2}$.

Once the data set is generated, the first five observations are discarded due to the missing values introduced by lagged variables, leaving a data set of 1200 observations. We use the first 1000 observations as the training set, while the remaining 200 observations are kept aside as the test set for evaluating the estimated models. 

## Experiment Setup {#sec-exp}

We estimated SMI Models through the proposed algorithm for each of the two response variables (the two "true models"), using three different sets of predictors as inputs. Our aim was to assess the algorithm's capability to correctly pick the relevant predictor variables (and drop the irrelevant predictors), and to estimate the correct index structure of the true model. 

The three different sets of predictors considered are as follows:

1.    All $\bm{x}$ variables (denoted as all $\bm{x}$);

2.    All $\bm{x}$ variables and all $\bm{z}$ variables (denoted as all $\bm{x}$ + all $\bm{z}$);

3.    A part of $\bm{x}$ variables (i.e. $x_{0}, x_{1}$ and $x_{2}$) and all $\bm{z}$ variables (denoted as some $\bm{x}$ + all $\bm{z}$).

We applied the proposed SMI Modelling algorithm with each of the above predictor combinations, for both variations of the responses concerning the noise level. Moreover, we considered each of the first four initialisation options that we discussed in @sec-step1, for each of the two responses.

## Results {#sec-sim-results}

We summarise the results of the simulation experiment in @tbl-simulation. In the columns, we indicate the index structure (i.e. the number of indices and the predictor grouping among indices) estimated by the proposed algorithm under each of the four initialisation options. This is detailed for each combination explored, considering response, input predictors, and noise levels.

In the simulation experiment, we did not perform any tuning for the penalty parameters $\lambda_{0}$ and $\lambda_{2}$. Our experiments indicated that, for this simple example, different values of penalty parameters have a negligible impact on the estimated models. The default values $\lambda_{0} = 1$ and $\lambda_{2} = 1$ were used in estimating all the models presented in @tbl-simulation.

\textcolor{orange}{XQ: Please replace $x$ and $z$ in the Predictors column with bold $\bm{x}$ and $\bm{z}$. It's}

```{r}
#| label: tbl-simulation
#| tbl-cap: Simulation experiment results.
results_sim <- readr::read_csv("data/simulation_table_final.csv")
kable(results_sim,
      format = "latex",
      booktabs = TRUE,
      digits = 2,
      escape = FALSE,
      col.names = c("True Model", "Predictors", "PPR", "Additive", "Linear", "Multiple")) %>%
  #add_header_above(c("", "", "Low noise level" = 4, "High noise level" = 4), align = "c") %>%
  pack_rows(index = c("Low noise level" = 12, "High noise level" = 12)) %>%
  kable_styling(latex_options = c("hold_position", "repeat_header", "scale_down")) %>%
  row_spec(0, bold = TRUE) %>%
  collapse_rows(columns = 1:2, valign = "middle")
```

\textcolor{orange}{XQ: I think it's too lengthy to explain table 1 with 14 paragraphs. Please simplify the results analysis using no more than five paragraphs. Use one paragraph each to interpret results under high and low noise levels respectively, with other two paragraphs discussing the insights, algorithm benefits and application scenarios based on the results. I've made a few changes, but you need to do some further work on it. I provided more detailed comments below.}

Firstly, when the low noise level is considered, for the first true model $y_{1}$, when either "all $\bm{x}$" or "all $\bm{x}$ + all $\bm{z}$" predictors are provided, all four initialisation options enable the algorithm to output the correct index structure of the model, while selecting the correct set of predictors and dropping out the irrelevant ones. However, when only some $\bm{x}$ variables are provided, the models estimated under all four initialisations include some noise variables. This indicates that when the available predictors are insufficient to capture the data signal, the algorithm might select irrelevant variables to make up for the missing signal.

In the case of $y_{2}$ with low noise level, for both "all $\bm{x}$" and "all $\bm{x}$ + all $\bm{z}$" cases, the algorithm correctly estimates the index structure with all initialization options, except for the "Linear" option. The "Linear" option includes the correct variables in the model, however it fails to identify the 2-index structure. This suggests that initializing the algorithm with a higher number of indices might be more effective than a lower number. When only some $\bm{x}$ variables are provided, similar to the case of $y_{1}$, models estimated for $y_{2}$ under all four initialisations, include noise variables.

Furthermore, we evaluated the forecasting error of the fitted models on the test set using Mean Squared Error (MSE). For $y_{1}$, in both the cases "all $\bm{x}$" and "all $\bm{x}$ + all $\bm{z}$", the models estimated using all four initialisations resulted in a test MSE of $\approx 0.01$, which is the random squared error of the true model. This confirms the accuracy with which the SMI Modelling algorithm estimated the index structure for $y_{1}$. When only a part of $\bm{x}$ variables are provided, the test MSE increased to $\approx 0.23$, as the estimated models contain noise variables.

For $y_{2}$, in both the cases "all $\bm{x}$" and "all $\bm{x}$ + all $\bm{z}$", all the models estimated resulted in a test MSE of $\approx 0.16$. This is an interesting result as the test MSEs of the estimated models with correct index structure: "PPR", "Additive", and "Multiple", are not very different to the test MSE of an estimated model with incorrect index structure, but with correct predictors: "Linear". This suggests that the selection of the predictor variables is more important than determining the index structure of the model.

Similar to the case of $y_{1}$, when only a part of $\bm{x}$ variables are provided, the test MSE of the estimated models for $y_{2}$ increased to $\approx 0.34$, probably due to the inclusion of noise variables.

Moreover, when the above test MSE values of the models estimated for $y_{2}$ are considered, in contrast to the case of $y_{1}$, those values are higher than the random squared error of the true model. Intuitively, this might be due to the increased complexity of the model $y_{2}$ in comparison to $y_{1}$, where the total estimation error of two nonlinear link functions (corresponding to the two indices) for $y_{2}$, might be higher than the error of estimating a single nonlinear function for $y_{1}$.

\textcolor{orange}{XQ: The above six paragraphs should be merged into one short paragraph, comparing the index structure and forecast errors in the case of low noise level.}

As expected, the accuracy with which the SMI Modelling algorithm estimates the index structure is in general lower with the high noise level, in comparison to the low noise level. In the case of $y_{1}$, when "all $\bm{x}$" variables are provided, except for "Additive" option, all the other initialisations have correctly estimated the model. When "all $\bm{x}$ + all $\bm{z}$" variables are provided, only the "PPR" and "Linear" options have estimated the correct model, whereas "Additive" and "Multiple" options have included a noise variable. Similar to the low noise level case, when only a part of $\bm{x}$ variables are provided, all four initialisations have led the algorithm to estimate models with noise variables.

Next, when $y_{2}$ is considered at high noise level, when "all $\bm{x}$" variables are provided, the estimated models using all four initialisation options have included the variable $x_{4}$ (as an additional index), which should have been omitted. When "all $\bm{x}$ + all $\bm{z}$" variables are provided, the option "Multiple" has led the algorithm to the correct model, while all the other three options have included a noise variable. Similar to the previous cases, when only a part of $\bm{x}$ variables are provided, the models estimated using all four initialisation options have included multiple noise variables, probably to cover up for the missing signal variables.

In both the cases "all $\bm{x}$" and "all $\bm{x}$ + all $\bm{z}$", all the models estimated for $y_{1}$, except for "Additive" option in "all $\bm{x}$" case, resulted in a test MSE of $\approx 0.23$ (which is slightly lower than the random squared error of the true model; this probably indicates a slight level of over-fitting), irrespective of the fact that in "all $\bm{x}$ + all $\bm{z}$" case, "Additive" and "Multiple" options included a noise variable. This is an indication of the effect of the low signal-to-noise ratio in the data. The model estimated using "Additive" option in "all $\bm{x}$" case has resulted in a slightly higher test MSE ($\approx 0.27$), probably due to the incorrect index structure. When only a part of $\bm{x}$ variables are provided, the test MSE has increased ($\approx 0.47$), which is expected.

Similar to previously discussed low noise level case, the estimated models for $y_{2}$ have resulted in higher test MSE values in comparison to the models for $y_{1}$. When "all $\bm{x}$" and "all $\bm{x}$ + all $\bm{z}$" variables are provided, all the estimated models resulted in test MSE values in the range $\approx (0.35, 0.37)$, irrespective of the different index structure and predictor choices, which is again an indication of the low signal-to-noise ratio in the data. Due to missing relevant predictors (and the inclusion of noise predictors in the model), the estimated models in "some x + all $\bm{z}$" case resulted in test MSEs $\approx 0.57$.

\textcolor{orange}{XQ: The above four paragraphs should be merged into one short paragraph, comparing the index structure and forecast errors in the case of high noise level.}

It is worth mentioning here that in real-world forecasting problems, the true data generating process (DGP) is unknown, and we do not expect an estimated model to precisely capture the true DGP. Therefore, as long as the estimated model demonstrates good forecasting accuracy, the index structure of the estimated model is less important.

Finally, the simulation study indicates that the choice of the initialisation depends on the data and application. Thus, the users are encouraged to follow a trial-and-error procedure to determine the most suitable initial model for a given application.

# Empirical Applications {#sec-application}

## Forecasting Daily Mortality {#sec-mortality}

We apply the SMI Modelling algorithm to a data set from @Masselot2022, to forecast daily mortality based on heat exposure. Studying the effects of various environmental exposures such as weather related variables, pollutants and man-made environmental conditions etc. on human health, is of significant importance in environmental epidemiology. Therefore, forecasting daily deaths taking heat related variables as predictors is an interesting application.

### Description of the Data

For this analysis, we consider daily mortality and heat exposure data for the Metropolitan Area of Montreal, Province of Quebec, Canada, from 1990 to 2014, for the months June, July, and August (i.e. summer season). The daily all-cause mortality data were obtained from the National Institute of Public Health, Province of Quebec, while *DayMet* --- a 1 km  1 km grid data set [@Thornton2021] was used to extract daily temperature and humidity data [@Masselot2022].

@fig-deaths shows the time plots of daily deaths during the summer for the years from 1990 to 1993. The series for each of the four years are presented separately in a faceted grid for visual clarity.

```{r}
#| label: fig-deaths
#| fig-cap: Daily mortality in summer in Montreal, Canada from 1990 to 1993.
dataHeat <- readRDS("data/heat_data_corrected.rds")
dataHeat %>%
  mutate(
    Date = as.POSIXct(Date)
  ) %>%
  filter(Date <= "1993-08-31") %>%
  ggplot(aes(x = Date, y = Death_lag_000)) +
  geom_line(color = "grey40") +
  geom_point(color = "grey40", size = 0.5) +
  facet_wrap(vars(Year), scales="free_x", ncol = 1, strip.position = "left") +
  scale_x_datetime(date_breaks = "1 month") +
  xlab("Date") +
  ylab("Number of Deaths") +
  ggtitle("")
```

The three main predictors considered in this empirical study are maximum temperature, minimum temperature, and vapour pressure (to represent the level of humidity). The number of daily deaths are plotted against each of these predictors in @fig-Tmax, @fig-Tmin, and @fig-Vp, respectively, where we can observe that the relationships between these predictors and the response are slightly non-linear.

```{r}
#| label: fig-Tmax
#| fig-cap: Daily mortality in summer (from 1990 to 2014) plotted against maximum temperature.
dataHeat %>%
  ggplot(aes(x = Tmax_lag_000, y = Death_lag_000)) +
  geom_point(color = "grey40", alpha = 0.5) +
  xlim(c(0, 35)) +
  xlab("Maximum Temperature (Degrees Celsius)") +
  ylab("Number of Deaths") +
  ggtitle("")
```

```{r}
#| label: fig-Tmin
#| fig-cap: Daily mortality in summer (from 1990 to 2014) plotted against minimum temperature.
dataHeat %>%
  ggplot(aes(x = Tmin_lag_000, y = Death_lag_000)) +
  geom_point(color = "grey40", alpha = 0.5) +
  xlim(c(0, 35)) +
  xlab("Minimum Temperature (Degrees Celsius)") +
  ylab("Number of Deaths") +
  ggtitle("")
```

```{r}
#| label: fig-Vp
#| fig-cap: Daily mortality in summer (from 1990 to 2014) plotted against vapour pressure.
dataHeat %>%
  ggplot(aes(x = Vp_lag_000, y = Death_lag_000)) +
  geom_point(color = "grey40", alpha = 0.5) +
  xlab("Vapour Pressure") +
  ylab("Number of Deaths") +
  ggtitle("")
```

### Predictors Considered

**1) Current maximum/minimum temperatures and lags:**

In addition to current maximum and minimum temperatures, the temperature measurements up to 14 days prior (i.e. $0^{th}$ to $14^{th}$ lag) are considered as predictors in the forecasting model. This accounts for the cumulative impact of both current and recent past temperatures on a person's heat exposure.

**2) Current vapour pressure and lags:**

Similar to temperature variables, the current value and 14 lags of vapour pressure are considered as predictors, as a proxy to the level of humidity.

**3) Calendar effects:**

Finally, a couple of calendar variables; *day of the season (DOS)* and *Year*, are incorporated into the model to capture annual trend and seasonality, and also to control the autocorrelation in residuals, which is a common practice in environmental epidemiology [@Masselot2022].

### Modelling Framework

Maximum temperature lags, minimum temperature lags, and vapour pressure lags are considered as predictors entering indices. The two calendar variables, *DOS* and *Year*, are included into the model as separate nonparametric components that do not enter any of the indices.

Hence, the relevant SMI Model can be written as
$$
\begin{aligned}
  \textbf{Deaths} = & \beta_{0} + \sum_{j = 1}^{p}{g_{j}(\bm{X}\bm{\alpha}_{j})} + f_{1}(\textbf{DOS}) + f_{2}(\textbf{Year})+ \bm{\varepsilon},
\end{aligned}
$$ {#eq-heat}
where

-   $\textbf{Deaths}$ is the vector containing daily deaths observations;

-   $\beta_{0}$ is the model intercept; 

-   $p$ is the unknown number of indices that need to be estimated through the algorithm;

-   $\bm{X}$ is the matrix containing the predictor variables that are entering indices (i.e. maximum temperature lags, minimum temperature lags, and vapour pressure lags);

-   $\bm{\alpha}_{j}, j = 1, \dots, p$ are the index coefficient vectors, each with a length equal to the number of predictors entering indices ($q=45$);

-   $g_{j}, j = 1, \dots, p$, $f_{1}$, and $f_{2}$ are unknown nonparametric functions; and

-   $\bm{\varepsilon}$ is the error term.

The data from 1990 to 2012 are used as the training set to estimate the model, while the data of year 2014 are separated to be the test set for evaluating forecasting performance. The data from the three summer months of year 2013 are kept aside as a validation set, which is used to estimate benchmark models for comparison purposes.

Then we apply the proposed SMI Modelling algorithm to the training set to estimate the model. Finally, the forecasting accuracy on the test set is evaluated using MSE and Mean Absolute Error (MAE).

### Results

We estimated SMI Models for the mortality data using three different initialisation options: "PPR", "Additive" and "Linear", for comparison purposes. \textcolor{orange}{XQ: Add a short sentence to explain why other two options were not used here.} Further, we tuned the penalty parameters $\lambda_{0}$ and $\lambda_{2}$, over ranges of integers from 1 to 12, and 0 to 12 respectively, through a greedy search based on in-sample MSE. Here, a greedy search is used instead of a grid search to reduce computational time.

The penalty parameter combination $(\lambda_{0} = 12, \lambda_{2} = 0)$ was selected for the model fitted with "PPR" initialisation. The estimated model, ***SMI Model (12, 0) - PPR ***, resulted in five indices wihout dropping any of the index variables. The optimal penalty parameter combination for the model initiated with "Additive" was $(\lambda_{0} = 1, \lambda_{2} = 0)$, resulting in the ***SMI Model (1, 0) - Additive ***, equivalent to a nonparametric additive model (no index variables or indices were dropped). The model estimated with "Linear" initialisation selected $(\lambda_{0} = 12, \lambda_{2} = 5)$ (***SMI Model (12, 5) - Linear ***), and resulted in two indices, without dropping any of the index variables.

We evaluated forecasting errors of the estimated models using two subsets of the original test set:

1. ***Test Set 1:*** original test set spanning 3 months (June, July and August 2014); and
2. ***Test Set 2:*** a test set covering 1 month (June 2014).

Note that in this application, we assumed that the future values of the maximum/minimum temperatures and vapour pressure are known to use in the forecasting model.

The MSE and MAE values for the estimated SMI Models on two different test sets are presented in @tbl-heat. We observe that the SMI Model estimated with "PPR" initialisation, ***SMI Model (12, 0) - PPR ***, shows the best forecasting performance on both test sets, compared to the other two estimated SMI models.

Furthermore, we present the forecasting errors of three benchmark models in @tbl-heat for comparison with the estimated SMI Models. The first benchmark is a nonparametric additive model formulated through backward elimination, as proposed by @FH2012 (@sec-backward). Next, a GAIM (@sec-multi-index) is also presented. In the case of GAIM, maximum temperature lags, minimum temperature lags, and vapour pressure lags are categorised into three groups, where an index estimated for each group. Finally, we present the forecasting errors of a PPR model. The number of indices in the PPR model was taken as 3, matching the number of indices estimated by the GAIM.

```{r}
#| label: tbl-heat
#| tbl-cap: Daily mortality forecasting - Out-of-sample point forecast results.
results_heat <- readr::read_csv("data/heat_results_corrected.csv")
kable(results_heat,
      format = "latex",
      booktabs = TRUE,
      digits = 3,
      escape = FALSE,
      col.names = c("Model", "Predictors", "Indices", "MSE", "MAE", "MSE", "MAE")) %>%
  add_header_above(c("", "", "", "Test Set 1" = 2, "Test Set 2" = 2), align = "c") %>%
  kable_styling(latex_options = c("hold_position", "repeat_header")) %>%
    row_spec(0, align = "c") %>%
  column_spec(4, bold = if_else(results_heat$MSE1 == min(results_heat$MSE1), TRUE, FALSE)) %>%
  column_spec(5, bold = if_else(results_heat$MAE1 == min(results_heat$MAE1), TRUE, FALSE)) %>%
  column_spec(6, bold = if_else(results_heat$MSE2 == min(results_heat$MSE2), TRUE, FALSE)) %>%
  column_spec(7, bold = if_else(results_heat$MAE2 == min(results_heat$MAE2), TRUE, FALSE))
```

@tbl-heat shows that ***SMI Model (12, 0) - PPR*** outperforms all three benchmark models in terms of forecasting accuracy, for both *Test Set 1* and *Test Set 2*. However, the SMI Models estimated using "Additive" or "Linear" initialisations have inferior forecasting performance compared to all benchmark models considered. The actual number of deaths and the predicted values from the ***SMI Model (12, 0) - PPR*** and benchmark models on *Test Set 2* are plotted in @fig-heatPred for further comparison.

```{r}
#| label: fig-heatPred
#| fig-cap: Actual number of deaths vs. predicted number of deaths from "SMI Model (12, 0) - PPR" and benchmark models for Test Set 2.
HeatPreds <- readr::read_csv("data/heat_predictions_corrected.csv")
HeatPreds %>%
  mutate(
    Date = as.POSIXct(Date)
  ) %>%
  ggplot() +
  geom_line(aes(x = Date, y = Actual, linetype = "Actual", colour = "Actual")) +
  geom_line(aes(x = Date, y = SMImodel_PPR, linetype = "SMI Model (12, 0) - PPR", colour = "SMI Model (12, 0) - PPR")) +
  geom_line(aes(x = Date, y = PPR, linetype = "PPR", colour = "PPR")) +
  geom_line(aes(x = Date, y = Backward, linetype = "Backward Elimination", colour = "Backward Elimination")) +
  geom_line(aes(x = Date, y = GAIM, linetype = "GAIM", colour = "GAIM")) +
  scale_x_datetime(date_breaks = "5 days") +
  xlab("Date") +
  ylab("Number of Deaths") +
  ggtitle("Actual vs. Predicted Number of Deaths") +
  scale_linetype_manual(name = "Series", values = c("Actual" = "solid", 
                                                    "SMI Model (12, 0) - PPR" = "solid",
                                                    "PPR" = "longdash",
                                                    "Backward Elimination" = "dotdash",
                                                    "GAIM" = "dotted")) +
  scale_colour_manual(name = "Series", values = c("Actual" = "grey", 
                                                  "SMI Model (12, 0) - PPR" = "black", 
                                                  "PPR" = "black",
                                                  "Backward Elimination" = "black",
                                                  "GAIM" = "black")) +
  theme(legend.position="bottom")
```

## Forecasting Daily Solar Intensity {#sec-solar}

Next, we utilise the SMI Modelling algorithm to forecast daily solar intensity, using other weather conditions. As reported by @EI2023, renewable energy (excluding hydroelectricity) contributed to 7.5% of the world's primary energy consumption in 2022. Solar and wind power saw a combined capacity addition of 266 GW, with solar energy accounting for 72% of the increase. Given this, accurate forecasting of solar power generation, closely linked to solar intensity, is crucial for effective power system planning and management.

### Description of the Data

We use solar intensity and other weather variables measured at a Davis weather station in Amherst, Massachusetts, obtained from the *UMass Trace Repository* [@Umass2023]. The data was recorded at every five minutes, from 21th February 2006 to 27th February 2013, using sensors for measuring temperature, wind chill, humidity, dew point, wind speed, wind direction, rain, pressure, solar intensity, and UV.

However, the data contained missing entries recorded as "-100000", which we removed from the data set. Moreover, for this analysis, we converted the five minutes data to daily data by averaging each variable over days. 

@fig-solar shows the time plot of daily solar intensity for the entire period, which clearly depicts the annual seasonality in the data. As observed in @fig-solar, there are days for which the observations were missing. We excluded those days from the analysis, and used only the days for which the data are available.

```{r}
#| label: fig-solar
#| fig-cap: Daily solar intensity in Amherst, Massachusetts - from February 2006 to February 2013.
dataSolar <- readRDS("data/solar_daily_large_final.rds")
dataSolar %>%
  mutate(
    Date = as.POSIXct(Date)
  ) %>%
  ggplot(aes(x = Date, y = Solar_lag_000)) +
  geom_line(color = "grey40") +
  scale_x_datetime(date_breaks = "1 year") +
  ggtitle("") +
  ylab("Solar Intensity (Watts per square metre)") 
  #ylab(expression("Solar Intensity (Watts/m"^2*")"))
```

The variables temperature, dew point, wind, rain and humidity were considered to be the main set of predictors in the model. The daily solar intensity is plotted against each of these predictors in @fig-preds, where we can observe that the relationships between these predictors and the response are non-linear.

```{r}
#| label: fig-preds
#| fig-cap: Daily solar intensity against other weather variables.
p1 <- dataSolar %>%
  ggplot(aes(x = Temp_lag_000, y = Solar_lag_000)) +
  geom_point(color = "grey40", alpha = 0.5, size = 0.3) +
  xlim(c(-25, 90)) +
  xlab("Temperature (Degrees Fahrenheit)") +
  ylab("Solar Intensity") +
  ggtitle("Solar Intensity against Temperature") +
  theme(axis.title = element_text(size = 6),
        plot.title = element_text(size = 8))

p2 <- dataSolar %>%
  ggplot(aes(x = Dewpt_lag_000, y = Solar_lag_000)) +
  geom_point(color = "grey40", alpha = 0.5, size = 0.3) +
  xlim(c(-25, 90)) +
  xlab("Dew Point (Degrees Fahrenheit)") +
  ylab("Solar Intensity") +
  ggtitle("Solar Intensity against Dew Point") +
  theme(axis.title = element_text(size = 6),
        plot.title = element_text(size = 8))

p3 <- dataSolar %>%
  ggplot(aes(x = Wind_lag_000, y = Solar_lag_000)) +
  geom_point(color = "grey40", alpha = 0.5, size = 0.3) +
  xlab("Wind Speed (Miles per hour)") +
  ylab("Solar Intensity") +
  ggtitle("Solar Intensity against Wind Speed") +
  theme(axis.title = element_text(size = 6),
        plot.title = element_text(size = 8))

p4 <- dataSolar %>%
  ggplot(aes(x = Rain_lag_000, y = Solar_lag_000)) +
  geom_point(color = "grey40", alpha = 0.5, size = 0.3) +
  xlab("Rain (Inches)") +
  ylab("Solar Intensity") +
  ggtitle("Solar Intensity against Rain") +
  theme(axis.title = element_text(size = 6),
        plot.title = element_text(size = 8))

p5 <- dataSolar %>%
  ggplot(aes(x = Humid_lag_000, y = Solar_lag_000)) +
  geom_point(color = "grey40", alpha = 0.5, size = 0.3) +
  xlab("Humidity (Percentage)") +
  ylab("Solar Intensity") +
  ggtitle("Solar Intensity against Humidity") +
  theme(axis.title = element_text(size = 6),
        plot.title = element_text(size = 8))

grid.arrange(p1, p2, p3, p4, p5, nrow = 3, ncol = 2)
```

### Predictors Considered

**1) Solar intensity lags:**

Three lags of the daily solar intensity itself are used as predictors to incorporate the serial correlations presented in the data into the modelling process. Intuitively, the solar intensity of a particular day would have a relationship to the solar intensity of adjacent days. 

**2) Current weather variables and lags:**

In addition to current temperature, dew point, wind speed, rain and humidity, the measurements of three previous days (i.e. $0^{th}$ to $3^{rd}$ lag) for each of the these weather variables are also included as predictors in the forecasting model.

**3) Calendar effects:**

Finally, a couple of calendar variables; *Month* (12 months of the year) and *Season* (the four seasons: Spring, Summer, Autumn and Winter), are incorporated into the model to capture annual seasonality, and control for autocorrelation in residuals.

### Modelling Framework

The lags of solar intensity, and the lags of weather variables are considered as predictors that are entering indices. The two calendar variables, *Month* and *Season*, are included into the model as linear (categorical) predictor variables. 

Hence, the relevant SMI Model can be written as
$$
\begin{aligned}
  \textbf{Solar} = & \beta_{0} + \sum_{j = 1}^{p}{g_{j}(\bm{X}\bm{\alpha}_{j})} + \theta_{1}\textbf{Month} + \theta_{2}\textbf{Season}+ \bm{\varepsilon},
\end{aligned}
$$ {#eq-solar}
where

-   $\textbf{Solar}$ is the vector containing daily observations of solar intensity;

-   $\beta_{0}$ is the model intercept; 

-   $p$ is the unknown number of indices that will be estimated through the algorithm;

-   $\bm{X}$ is the matrix containing the predictor variables that are entering indices (i.e. solar intensity, temperature, dew point, wind speed, rain and humidity lags);

-   $\bm{\alpha}_{j}, j = 1, \dots, p$ are the index coefficient vectors, each of lenth equal to the number of predictors entering indices ($q= 23$);

-   $g_{j}, j = 1, \dots, p$ are unknown nonparametric functions;

-   $\theta_{1}$ and $\theta_{2}$ are the two coefficients corresponding to the two linear predictor variables; and

-   $\bm{\varepsilon}$ is the error term.

The data from February 2006 to October 2012 are used as the training set to estimate the model, while the data of the months January and February 2013 are separated to be the test set to evaluate the forecasting performance. The data from the months November and December 2013 are kept aside as a validation set, which is required to estimate some of the benchmark models for comparison.  

Then we apply the proposed SMI Modelling algorithm to the training set to estimate the model, and the forecasting accuracy on the test set is evaluated using MSE and MAE.

### Results

Similar to the previous empirical application, we estimated SMI Models for the solar intensity data using three different initialisation options: "PPR", "Additive" and "Linear", for comparison purposes. We also tuned the penalty parameters $\lambda_{0}$ and $\lambda_{2}$, over ranges of integers from 1 to 12, and 0 to 12 respectively.

The penalty parameter combination $(\lambda_{0} = 12, \lambda_{2} = 0)$ was selected for the model fitted with "PPR" initialisation. The estimated model, ***SMI Model (12, 0) - PPR ***, resulted in five indices without dropping any of the index variables. The optimal penalty parameter combination for the model estimated taking "Additive" model as the starting point was $(\lambda_{0} = 1, \lambda_{2} = 0)$. The estimated SMI Model did not drop any index variables or indices, and thus the final model, ***SMI Model (1, 0) - Additive ***, is equivalent to a nonparametric additive model. The model estimated with "Linear" initialisation also selected $(\lambda_{0} = 1, \lambda_{2} = 0)$. Unlike the above models, this SMI Model dropped all index variables and resulted in null indices, and hence, the final model, ***SMI Model (1, 0) - Linear ***, is just a linear model with the two linear variables *Month* and *Season*. Notice that all three estimated SMI Models have $\lambda_{2} = 0$, indicating that all three models have omitted the $\ell_{2}$-penalty in the estimation process.

Note that similar to the previous application of heat related mortality forecasting, we assumed that the future values of the weather variables are known to use in the forecasting model.

@tbl-solar presents the MSE and MAE values for the estimated SMI Models on the test set. The results indicate that the SMI Model estimated with "Additive" initialisation, ***SMI Model (1, 0) - Additive***, shows the best forecasting performance among the three estimated SMI Models.

Similar to @sec-mortality, we also present forecasting errors of three benchmark models in @tbl-solar, to compare with the estimated SMI Models. Here, the GAIM is fitted by grouping the lags of each weather variable into a different group, resulting in six indices. The number of indices of the PPR model was taken as six, matching the number of indices estimated by the GAIM. Note that here, the two categorical calendar variables were excluded when estimating the PPR model.

```{r}
#| label: tbl-solar
#| tbl-cap: Daily solar intensity forecasting - Out-of-sample point forecast results.
results_solar <- readr::read_csv("data/solarResults_table_final.csv")
kable(results_solar,
      format = "latex",
      booktabs = TRUE,
      digits = 3,
      escape = FALSE,
      col.names = c("Model", "Predictors", "Indices", "MSE", "MAE")) %>%
  add_header_above(c("", "", "", "Test Set" = 2), align = "c") %>%
  kable_styling(latex_options = c("hold_position", "repeat_header")) %>%
    row_spec(0, align = "c") %>%
  column_spec(4, bold = if_else(results_solar$MSE == min(results_solar$MSE), TRUE, FALSE)) %>%
  column_spec(5, bold = if_else(results_solar$MAE == min(results_solar$MAE), TRUE, FALSE))
```

According to @tbl-solar, the forecasting errors of ***SMI Model (1, 0) - Additive*** is lower than the GAIM. However, the ***SMI Model (1, 0) - Additive*** is unable to outperform both the semi-parametric additive model with backward elimination and the PPR model, where in this case, the estimated PPR model has resulted in the best forecasting accuracy.

Here, it is worth considering the differences between the SMI Model and the benchmark models that show superior forecasting performance. The method proposed by @FH2012 formulates a semi-parametric additive model using a backward elimination of predictors. When estimating a PPR model, both the number of indices and the predictors within each index (each index includes all provided predictors that are entering indices) are pre-determined. In contrast, the SMI Model takes a more general and objective approach, where the number of indices as well as predictors within each index are automatically determined through the proposed algorithm. Thus, the SMI Model faces a more challenging estimation task due to the limited prior information provided.

The actual solar intensity and the predicted values from the SMI Model (1, 0) - Additive and benchmark models are plotted in @fig-solarPred for further comparison.

```{r}
#| label: fig-solarPred
#| fig-cap: Actual solar intensity vs. predicted solar intensity from "SMI Model (1, 0) - Additive" and benchmark models.
SolarPreds <- readr::read_csv("data/solar_predictions_3lags.csv")
SolarPreds %>%
  mutate(
    Date = as.POSIXct(Date)
  ) %>%
  ggplot() +
  geom_line(aes(x = Date, y = Actual, linetype = "Actual", colour = "Actual")) +
  geom_line(aes(x = Date, y = SMImodel_Additive, linetype = "SMI Model (1, 0) - Additive", colour = "SMI Model (1, 0) - Additive")) +
  geom_line(aes(x = Date, y = PPR, linetype = "PPR", colour = "PPR")) +
  geom_line(aes(x = Date, y = Backward, linetype = "Backward Elimination", colour = "Backward Elimination")) +
  geom_line(aes(x = Date, y = GAIM, linetype = "GAIM", colour = "GAIM")) +
  scale_x_datetime(date_breaks = "12 days") +
  xlab("Date") +
  ylab("Solar Intensity (Watts per square metre)") +
  ggtitle("Actual vs. Predicted Solar Intensity") +
  scale_linetype_manual(name = "Series", values = c("Actual" = "solid", 
                                                    "SMI Model (1, 0) - Additive" = "solid",
                                                    "PPR" = "longdash",
                                                    "Backward Elimination" = "dotdash",
                                                    "GAIM" = "dotted")) +
  scale_colour_manual(name = "Series", values = c("Actual" = "grey", 
                                                  "SMI Model (1, 0) - Additive" = "black", 
                                                  "PPR" = "black",
                                                  "Backward Elimination" = "black",
                                                  "GAIM" = "black")) +
  theme(legend.position="bottom")
```

In summary, the two empirical applications presented above highlight the challenge of finding a universally applicable initialisation option for the SMI Model across various applications. As mentioned in @sec-simulation, we encourage users to follow a trial-and-error procedure to identify the most effective initialisation option for their specific application.

The two empirical applications were performed using ***R statistical software*** [@R2023], and the ***Rstudio*** integrated development environment (IDE) [@Rstudio2024]. We used the commercial MIP solver ***"Gurobi"*** [@gurobi2023] to solve the MIQPs related to the proposed SGAIM algorithm, through the ***Gurobi plug-in (ROI.plugin.gurobi)*** [@Schwendinger2023] available from the ***R Optimization Infrastructure (ROI)*** [@Hornik2023; @Theusl2020] package. Furthermore, the GAMs were fitted using the R package ***mgcv*** *(v1.9.1)* [@Wood2011].

# Conclusions and Further Research {#sec-conclusion}

In this paper, we presented a novel algorithm for the estimation of a nonparametric additive index model with optimal predictor selection, which we refer to as *Sparse Multiple Index (SMI) Model*. The SMI Modelling algorithm is an iterative procedure that is developed based on mixed integer programming to solve an $\ell_{0}$-regularised nonlinear least squares problem.

The proposed SMI Modelling algorithm has a number of special features: 1) It performs automatic selection of both the number of indices and the predictors within in each index, and estimate the resulting nonparametric additive index model. The set of predictors entering indices, and a starting model (index structure and a set of index coefficients) to initiate the algorithm are the information that a user should provide as inputs. 2) It performs automatic variable selection, which is specially useful for estimating a parsimonious model in a high-dimensional setting. Thus, the SMI Modelling algorithm is an objective and a principled estimation and variable selection method that reduces any subjectivity induced by different users. 3) It is capable of estimating a wide spectrum of models starting from *single index models* (number of indices = 1) to *additive models* (number of indices = number of predictors (entering indices)). Hence, the SMI Modelling algorithm is a more general estimation algorithm in the domain of nonparametric additive models. 4) It provides the ability to include separate non-liner as well as linear predictors in the model that are not entering any of the indices, and hence creates the possibility of estimating semi-parametric additive models. 

Due to the limited input information provided to the algorithm, the estimation of a SMI Model is a challenging problem. We demonstrated the characteristics and the performance of the proposed algorithm through a simple simulation exercise and two empirical applications. Since we observed that the final estimated model changes with the initialisation provided, one limitation of the proposed algorithm is the difficulty of specifying an initialisation that works in general. Hence, an interesting future research problem would be to explore the possibility of determining a generalised initialisation for the SMI Modelling algorithm that will work in many different applications. 

Moreover, we admit that the empirical examples presented in the paper may not be diverse enough to make any conclusions about the unique strengths or weaknesses of the proposed algorithm. This study should be viewed as an attempt to develop a more objective variable selection and model estimation methodology in the broader class of nonparametric additive models for forecasting. An important future research problem is therefore, to experiment with the proposed algorithm using many different data sets with diverse properties to establish the situations in which the proposed SMI Modelling algorithm performs better compared to other benchmark methods. 

Furthermore, the MIQP related to the algorithm is somewhat analogous to the *best subset selection* problem in an ordinary least squares problem. Thus, another limitation of the proposed algorithm is the increase in computational time as the number of predictors and number of indices increase. Therefore, it would be an interesting research to obtain further insights regarding the algorithm to see what improvements can be made to the algorithm design to reduce the computational cost in a high-dimensional context. 

Forecasting problems frequently arise in many different fields. Therefore, we hope that the proposed SMI Modelling algorithm and possible future extensions of it will be useful for different users in different fields, with different levels of specific domain knowledge, as an objective and a general forecasting tool. \newline

\textbf{\large{Acknowledgements}}

We thank Professor Louise Ryan for joining the discussions during the initial stage of the project, and for her valuable comments and feedback on this research work.

Furthermore, this research is partially supported by the Monash eResearch Centre through the use of the MonARCH (Monash Advanced Research Computing Hybrid) HPC Cluster. 