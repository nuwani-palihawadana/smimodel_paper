% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
  a4paper,
]{article}

\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{2}


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{todonotes}
\usepackage{orcidlink}
\definecolor{mypink}{RGB}{219, 48, 122}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Sparse Multiple Index Models for High-dimensional Nonparametric Forecasting},
  pdfauthor={Nuwani K Palihawadana; Rob J Hyndman; Xiaoqian~Wang},
  pdfkeywords={Additive index models, Variable selection, Dimension
reduction, Predictor grouping, Mixed integer programming},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

%% CAPTIONS
\usepackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\usepackage{bera}
\usepackage[charter]{mathdesign}
\usepackage[scale=0.9]{sourcecodepro}
\usepackage[lf,t]{FiraSans}

%% HEADERS AND FOOTERS
\usepackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\usepackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%% SECTION TITLES
\usepackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries}{}{1em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.2}

%% BIBLIOGRAPHY.

\makeatletter
\@ifpackageloaded{biblatex}{
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false}
\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[article]{url}{}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}
\AtEveryBibitem{\clearfield{month}}
\AtEveryCitekey{\clearfield{month}}
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\renewcommand*{\finalnamedelim}{\addspace\&\space}
}{}
\makeatother

%% PAGE BREAKING to avoid widows and orphans
\clubpenalty = 10000
\widowpenalty = 10000
\usepackage{microtype}% Placement of logos

\RequirePackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}
\def\placefig#1#2#3#4{\begin{textblock}{.1}(#1,#2)\rlap{\includegraphics[#3]{#4}}\end{textblock}}

% Title and date

\title{Sparse Multiple Index Models for High-dimensional Nonparametric
Forecasting}
\date{15 June 2024}

\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

% Working paper number and JEL codes

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\makeatother

\wp{no/yr}
\jel{C10,C14,C22}

% Title page

\makeatletter
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}
        \placefig{2}{1.5}{width=5cm}{monash2}
        \placefig{16.9}{1.5}{width=2.1cm}{MBSportrait}
        \begin{textblock}{4}(16.9,4)ISSN 1440-771X\end{textblock}
        \begin{textblock}{7}(12.7,27.9)\hfill
        \includegraphics[height=0.7cm]{AACSB}~~~
        \includegraphics[height=0.7cm]{EQUIS}~~~
        \includegraphics[height=0.7cm]{AMBA}
        \end{textblock}
        \vspace*{2.5cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \footnotesize http://monash.edu/business/ebs/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE
                \expandafter{\@author}
                \end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}\vspace*{2cm}}}
        \def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
        \def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

% Authors


  \author{Nuwani K Palihawadana, Rob J Hyndman, Xiaoqian~Wang}
  \addresses{%
    %
      \textbf{Nuwani K Palihawadana}\\%
      %
        Department of Econometrics \& Business Statistics\\%
        Monash University\\%
        Clayton VIC 3800\\%
        Australia\\%
      %
      {Email: nuwani.kodikarapalihawadana@monash.edu}\\%
      \textit{Corresponding author}\\[0.5cm]%
   %
      \textbf{Rob J Hyndman}\\%
      %
        Department of Econometrics \& Business Statistics\\%
        Monash University\\%
        Clayton VIC 3800\\%
        Australia\\%
      %
      {Email: rob.hyndman@monash.edu}\\%
      \\[0.5cm]%
   %
      \textbf{Xiaoqian~Wang}\\%
      %
        Department of Econometrics \& Business Statistics\\%
        Monash University\\%
        Clayton VIC 3800\\%
        Australia\\%
      %
      {Email: xiaoqian.wang@monash.edu}\\%
      \\[0.5cm]%
   %
   }%
   \lfoot{\sf Palihawadana, Hyndman, Wang: 15 June 2024}

% Keywords

\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

% Abstract
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline\setstretch{1.5}}
  {\end{minipage}}
\begin{document}
\maketitle

\begin{abstract}
Forecasting often involves high-dimensional predictors, which have
nonlinear relationships with the outcome of interest. Nonparametric
additive index models can capture these relationships, while addressing
the curse of dimensionality. This paper introduces a new algorithm,
\textbf{\emph{S}}parse \textbf{\emph{M}}ultiple \textbf{\emph{I}}ndex
(\textbf{\emph{SMI}}) \textbf{\emph{Modelling}}, tailored for estimating
high-dimensional nonparametric/semi-parametric additive index models,
while limiting the number of parameters to estimate, by optimising
predictor selection and predictor grouping. The SMI Modelling algorithm
uses an iterative approach based on mixed integer programming to solve
an \(\ell_{0}\)-regularised nonlinear least squares optimisation problem
with linear constraints. We demonstrate the performance of the proposed
algorithm through a simulation study, along with two empirical
applications to forecast heat-related daily mortality and daily solar
intensity.
\end{abstract}

\begin{keywords}
  Additive index models; Variable selection; Dimension
reduction; Predictor grouping; 
  Mixed integer programming.
\end{keywords}

\setstretch{1.5}
\section{Introduction}\label{sec-introduction}

Forecasts are often contingent on a very long history of predictors,
which are nonlinearly related to the variable of interest. For example,
when forecasting half-hourly electricity demand, it is common to use at
least a week of historical half-hourly temperatures and other weather
observations \autocite{HF2010}. The relationships between the lagged
temperatures and electricity demand are nonlinear (due to both heating
and cooling effects), and involve complex interactions due to thermal
inertia in buildings \autocite{FH2012}. Similarly, when forecasting bore
levels, rainfall data from up to thousand days earlier can impact the
result \autocite{Peterson2014,Bakker2019,Rajaee2019} due to the complex
nonlinear flow dynamics of rainfall into aquifers.

These examples suggest a possible nonlinear ``\emph{transfer function}''
model of the form \begin{equation}\phantomsection\label{eq-transferfun}{
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}, y_{t-1},\dots,y_{t-k}) + \varepsilon_{t},
}\end{equation} where \(y_{t}\) is the observation of the response
variable at time \(t\), \(\bm{x}_{t}\) is a vector of predictors at time
\(t\), and \(\varepsilon_{t}\) is an i.i.d. random error. By including
lagged values of \(y_{t}\) along with the lagged predictors, we allow
for any serial correlation in the data.

The form of \(f\) is typically nonlinear, and involves complicated
interactions with a high value of \(p\) (and possibly also large \(k\)).
It is infeasible to estimate \(f\) in high-dimensional settings (where
\(p\) is large) due to the curse of dimensionality
\autocite{Bellman57,Stone82}. Instead, we normally impose some form of
additivity constraint, and ignore interactions of more than two or three
variables. There are also numerous ad hoc model choices in selecting the
appropriate predictors to include.

For example, \textcite{FH2012} proposed a \textbf{\emph{semi-parametric
additive model}} to obtain short-term forecasts of the half-hourly
electricity demand for power systems in the Australian National
Electricity Market. In this model, \(f\) is assumed to be fully
additive, and is used to capture the effects of recent predictor values
on the demand. The main objective is to allow nonparametric components
in a regression-based modelling framework, particularly to address
serially correlated errors. The model fitted for each half-hourly period
(\(q\)) can be written as \begin{equation}\phantomsection\label{eq-FH}{
 \log(y_{t,q}) = h_{q}(t) + f_{q}(\bm{w}_{1,t},\bm{w}_{2,t}) + \sum_{j=1}^k a_{q,j}(y_{t-j}) + \varepsilon_{t}.
}\end{equation} The term \(h_{q}(t)\) models several calendar effects as
either linear or smooth terms. Temperature effects are modelled using
the nonparametric component \(f_{q}(\bm{w}_{1,t},\bm{w}_{2,t})\), where
\(\bm{w}_{i,t} = [w_{i,t},\dots,w_{i,t-p}]'\) is a vector of lagged
temperatures at site \(i\). The terms \(a_{q,j}(y_{t-j})\) capture the
lagged effects of the response. Note that the error term
\(\varepsilon_{t}\) is serially uncorrelated within each half-hourly
model, because the serial correlation is eliminated by the inclusion of
lagged responses in the model. However, some correlation may still exist
between residuals from various half-hourly models \autocite{FH2012}.

Similarly, a \textbf{\emph{distributed lag model}} was proposed by
\textcite{Wood2017} to forecast daily death rates in Chicago using
measurements of several air pollutants. The response variable is
modelled via a sum of smooth functions of lagged predictor variables,
which is quite similar to the semi-parametric additive model used by
\textcite{FH2012}. However, unlike in \textcite{FH2012},
\textcite{Wood2017} suggested allowing the smooth functions for lags of
the same covariate to vary smoothly over lags, preventing large
differences in estimated effects between adjacent lags. Thus, the model
is of the form \[
 \log(y_{t}) = f_{1}(t) + \sum_{k=0}^{K} f_{2}(p_{t-k}, k) + \sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k),
\] where \(y_{t}\) is the death rate at day \(t\), \(f_{1}\) is a
nonparametric term to capture the time effect, and \(p_t\), \(o_t\), and
\(w_t\) are various predictor variables. The model incorporates the
current value (\(k = 0\)) and several lagged values
(\(k = 1, \dots, K\)) of the predictors, where the distributed lag
effect of a single predictor variable, and of the interaction of two
predictor variables are captured by the sums
\(\sum_{k=0}^{K} f_{2}(p_{t-k}, k)\) and
\(\sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k)\) respectively.

Further examples include \textcite{Ho2020} and \textcite{Du2012}, who
used semi-parametric additive models to estimate ground-level
\(\text{PM}_{2.5}\) concentrations in Taiwan, and logarithm of total
distance driven by each household as a proxy for household gasoline
consumption in Canada, respectively. Moreover, median housing prices in
Boston were analysed by \textcite{Boente2023} using semi-parametric
additive models, while nonparametric additive models were utilised by
\textcite{Ibrahim2023} for predicting census survey response rates.
\textcite{Ravindra2019} provided a comprehensive review of the
applications of additive models in environmental data, with a special
focus on air pollution, climate change, and human health related
studies.

While nonparametric/semi-parametric additive models have been used to
address problems from diverse application areas, there are several
unresolved issues in their implementation. First, the estimation of the
model is challenging in high-dimensional settings due to the large
number of nonparametric components to be estimated. Second, there is a
noticeable subjectivity in selecting predictor variables (from the
available predictors) for the model, and in identifying which terms
should be grouped together to model interactions. In most of the
applications discussed above, the choices are based on empirical
explorations or domain expertise.

In this paper, we are interested in high-dimensional applications that
exhibit complicated interactions among predictors, particularly in the
presence of a large number of lagged variables and correlated errors. In
such situations, \emph{index models} prove beneficial in improving the
flexibility of the broader class of nonparametric additive models
\autocite{Radchenko2015}, while mitigating the difficulty of estimating
a nonparametric component for each individual predictor.

Therefore, we propose to address the above mentioned issues using a
\textbf{\emph{Sparse Multiple Index}} (SMI) model with automatic
variable selection and grouping. This semi-parametric model can be
written as \begin{equation}\phantomsection\label{eq-semipara}{
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
}\end{equation} where \(y_{i}\) is the univariate response,
\(\beta_{0}\) is the model intercept,
\(\bm{x}_{ij} \in \mathbb{R}^{l_{j}}\), \(j = 1, \dots, p\) are \(p\)
subsets of all the predictors entering indices, \(\bm{\alpha}_{j}\) is a
vector of index coefficients corresponding to the index
\(h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}\), \(g_{j}\) is a smooth
nonlinear function (possibly estimated by a spline). Note that we also
allow for the inclusion of predictors that do not enter any of the
indices, including covariates \(w_{ik}\) that relate to the response
through the nonlinear functions \(f_{k}\), \(k = 1, \dots, d\), and
linear covariates denoted by \(\bm{u}_{i}\). Although our interest is in
forecasting time series data, the model can be used more widely, and so
we have not included any notation specific to time series in the model
formulation.

The SMI model subsumes the models discussed above, incorporating fully
additive models \autocite{Wood2011,Wood2017}, where each predictor has
its own index, and single index models
\autocite{Stoker1986,Hardle1993,Radchenko2015}, where all predictors are
included in a single index. The greater generality allows us to address
the two issues mentioned earlier. First, the number of parameters to
estimate is reduced by combining variables using linear indices, and by
grouping predictors into indices to constrain the order and form of
interactions. In our model formulation, both the number of indices \(p\)
and the predictor grouping among indices are unknown. We propose
algorithmic selection of the predictors to include in each index,
thereby reducing the subjectivity in model formulation. We assume that
no predictor enters more than one index, i.e., overlapping of predictors
among indices is not allowed.

To our knowledge, no previous research has been done to explore how
predictor choices can be made more objective and principled in
nonparametric additive index models. Hence, our goal is to develop a
methodology for optimal predictor selection and grouping in the context
of high-dimensional nonparametric additive index models. Moreover, due
to computational advancements in the field, the use of mathematical
optimisation in solving statistical problems has gained a lot of recent
interest \autocite{Theusl2020}. This motivated us to develop a variable
selection algorithm based on mathematical optimisation techniques.

It is crucial to point out that any variable selection methodology
naturally renders inferential statistics invalid, since we do not assume
that the resulting model obtained through the variable selection
procedure represents the true data generating process. Hence, our focus
is only on improving forecasts, but not on making inferences on the
resulting parameter estimates.

The rest of this paper is organised as follows. Section~\ref{sec-SMI}
presents our proposed \emph{Sparse Multiple Index Model} and describes
the algorithm for variable selection and grouping, and the estimation
procedure. In Section~\ref{sec-simulation}, we demonstrate the
functionality and the characteristics of the proposed algorithm through
a simulation experiment. Section~\ref{sec-application} illustrates two
empirical applications of the proposed estimation and variable selection
methodology, related to forecasting heat exposure related daily
mortality and daily solar intensity. Some benchmark comparison methods
are briefly introduced in Section~\ref{sec-benchmark}. Concluding
remarks are given in Section~\ref{sec-conclusion}.

\section{Sparse Multiple Index Model}\label{sec-SMI}

\subsection{Optimisation Problem
Formulation}\label{optimisation-problem-formulation}

We implement variable selection for the proposed \textbf{\emph{Sparse
Multiple Index}} (SMI) model (Equation~\ref{eq-semipara}) by allowing
for zero index coefficients for predictors. Suppose we observe
\(y_1,\dots,y_n\), along with a set of potential predictors,
\(\bm{x}_1,\dots,\bm{x}_n\), with each vector \(\bm{x}_i\) containing
\(q\) predictors. The optimisation problem we seek to address is of the
form below, where the sum of the squared error of the model
(Equation~\ref{eq-semipara}) is minimised together with an \(\ell_{0}\)
penalty term and an \(\ell_{2}\) (ridge) penalty term: \begin{align}
  & \min_{\beta_{0}, p, \bm{\alpha}, \bm{g}, \bm{f}, \bm{\theta}} \quad \sum_{i = 1}^{n}\Bigg [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i}) - \sum_{k = 1}^{d}f_{k}(w_{ik}) - \bm{\theta}^{T}\bm{u}_{i}\Bigg]^{2} \label{eq-smi}\\
  & \hspace*{8cm} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}\mathbb{1}(\alpha_{jm} \neq 0) + \lambda_{2}\sum_{j = 1}^{p}\|\bm{\alpha}_{j}\|_2^2 \nonumber \\
  & \text{such that}\quad \sum_{j=1}^p \mathbb{1}(\alpha_{jm} \neq 0) \in \{0,1\} \quad \forall m ,\nonumber
\end{align} where
\(\bm{\alpha} = [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} ]^{T}\),
\(\bm{g} = \{g_{1}, g_{2}, \dots, g_{p}\}\),
\(\bm{f} = \{f_{1}, f_{2}, \dots, f_{d}\}\), \(\mathbb{1}(\cdot)\) is
the indicator function, \(\lambda_{0} > 0\) is a tuning parameter that
controls the number of selected predictors entering indices, and
\(\lambda_{2} \ge 0\) is another tuning parameter that controls the
strength of the additional shrinkage imposed on the estimated index
coefficients. The constraint ensures that every predictor can only have
non-zero coefficient in at most one index.

Applying an \(\ell_{2}\)-penalty in addition to the \(\ell_{0}\)-penalty
is motivated by related literature
\autocite{Hazimeh2020,Mazumder2023,Hazimeh2023}, where it is suggested
that the prediction performance of best-subset selection is enhanced by
the inclusion of an additional ridge penalty, especially when there is a
low signal-to-noise ratio (SNR).

To solve the optimisation problem in Equation \ref{eq-smi}, we present a
big-\(M\) based \textbf{\emph{Mixed Integer Quadratic Programming}}
(MIQP) formulation: \begin{equation}\phantomsection\label{eq-smi-mip}{
\begin{aligned}
  \min_{\beta_{0}, p, \bm{\alpha}, \bm{g}, \bm{f}, \bm{\theta}, \bm{z}} \quad & \sum_{i = 1}^{n}\Bigg [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}{g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} - \sum_{k = 1}^{d} {f_{k}(w_{ik})} - \bm{\theta}^{T}\bm{u}_{i}\Bigg ]^{2} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q} \alpha_{jm}^{2} \\
  \text{s.t.} \quad & |\alpha_{jm}| \le Mz_{jm} \quad \forall j, \forall m, \\
  & \sum_{j = 1}^{p}z_{jm} \le 1 \quad \forall m, \\
  & z_{jm} \in \{0, 1\},
\end{aligned}
}\end{equation} where \(j = 1, \dots, p\), and \(m = 1, \dots, q\). We
have introduced here binary variables
\(z_{jm} = \mathbb{1}(\alpha_{jm} \neq 0)\) to indicate in which index
(if any) each predictor enters. The pre-specified \emph{big-\(M\)
parameter} is denoted by \(M < \infty\), and it should be sufficiently
large. If \(\bm{\alpha^{*}}\) is the optimal solution to the problem
given in Equation~\ref{eq-smi-mip}, then the big-\(M\) parameter should
satisfy
\(\max \big ( \{|\alpha_{jm}^{*} |\}_{j\in[p],m\in[q]} \big) \le M\).
The big-\(M\) constraint ensures that \(\alpha_{jm}\) is zero if and
only if \(z_{jm}\) is zero, and if \(z_{jm} = 1\), then
\(|\alpha_{jm}| \le M\). At the same time, the \(\ell_{0}\)-penalty term
influences some of the binary variables \(z_{jm}\) to be zero, while the
\(\ell_{2}\)-penalty term enforces additional shrinkage on the estimated
coefficients. Together, these components achieve variable selection.

\subsection{Estimation Algorithm}\label{estimation-algorithm}

We now show how to efficiently find a minimiser for the problem given in
Equation~\ref{eq-smi-mip}. Since the number of indices \(p\), the vector
of index coefficients \(\bm{\alpha}\), and the set of nonparametric
functions \(\bm{g}\) are all unknown, it is impossible to solve the
above MIQP given in Equation~\ref{eq-smi-mip} directly. Hence, we
propose an iterative algorithm to solve the problem.

\subsubsection{Initialising the Index Structure and Index
Coefficients}\label{sec-step1}

We first need to provide a feasible initialisation for the index
structure (i.e.~the number of indices \(p\) and the grouping of
predictors among indices) as well as for the index coefficients
(\(\bm{\alpha}\)) of the model.

Based on several experiments, we propose three alternative methods for
initialising the SMI model as follows.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{PPR: Projection Pursuit Regression Based Initialisation}

  A Projection Pursuit Regression model \autocite{Friedman1981} is a
  multiple index model, where each index includes all the available
  predictors. Since the proposed SMI model requires that there are no
  overlapping indices, it is impossible to use an estimated PPR model
  directly as a starting model for the algorithm. Thus, we follow the
  steps presented below to come up with a feasible initialisation for
  the index structure and the index coefficients.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Scale all the variables of the data set by dividing each variable by
    its standard deviation (so that it is possible to compare the
    estimated coefficients among predictors).
  \item
    Fit a PPR model and obtain estimated index coefficients. (The user
    can decide the number of initial indices \(p^*\) to be estimated; we
    use \(p^* = 5\) in our simulations and applications.)
  \item
    Calculate a threshold
    \(\tau = 0.1 \times \max(\mathrm{abs}(\text{PPR index coefficients}))\).
  \item
    Set to zero all coefficients whose absolute values fall below the
    calculated threshold.
  \item
    For predictors appearing in multiple indices, assign them to the
    index with the maximum absolute coefficient and zero out their
    coefficients in other indices.
  \item
    After performing the above steps a-e, if any originally estimated
    index has all zero coefficients, it will be excluded from the model.
  \end{enumerate}

  Now, the index structure and the index coefficients obtained through
  the above steps are considered to be a feasible initialisation for the
  proposed algorithm. Once the optimal SMI model is obtained through the
  algorithm, each index coefficient will be back-transformed to the
  original scale of the respective predictor variable, reversing the
  scaling effect applied at the beginning.
\item
  \textbf{Additive: Nonparametric Additive Model Based Initialisation}

  As a fully additive model is a special case of the SMI model, we can
  set \(p=q\) and assign each predictor to its own index.
\item
  \textbf{Linear: Linear Regression Based Initialisation}

  We first regress the response variable on the predictors using a
  multiple linear regression. Then, we construct a single index
  (i.e.~\(p = 1\)) using the estimated regression coefficients as the
  initial index coefficients of the predictors.
\item
  \textbf{Multiple: Picking One From Multiple Initialisations}

  The final optimised SMI model may change depending on the
  initialisation provided to the algorithm. Hence, we also consider
  using several different models as initialisations, optimise the SMI
  model for each of them, and pick the initial model that results in the
  lowest loss for the MIQP problem.
\end{enumerate}

Of course, it is also possible for a user to specify an initialisation,
based on their own domain expertise or prior knowledge in initialising
the algorithm. In each of the above initialisation options, once the
estimate for \(\bm{\alpha}\) is obtained, the estimated initial index
coefficients for each index are scaled to have unit norm to ensure
identifiability.

\subsubsection{Estimating Nonlinear Functions}\label{sec-step2}

Once we have an estimate for \(\bm{\alpha}\), estimating the SMI model
is equivalent to estimating a Generalized Additive Model (GAM) as \[
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\hat{h}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(y_{i}\) is the response, and
\(\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{i}\) is the estimated
index. The R packages \textbf{\emph{mgcv}} \autocite{Wood2011} and
\textbf{\emph{gam}} \autocite{Hastie2023}, for example, can be used to
fit GAMs.

\subsubsection{Updating the Index Structure and Index
Coefficients}\label{sec-step3}

We obtain the updated index coefficients \(\bm{\alpha}^{\text{new}}\)
through a MIQP: \begin{equation}\phantomsection\label{eq-smi-update}{
\begin{aligned}
  \min_{\bm{\alpha}^{\text{new}}, \bm{z}^{\text{new}}} & (\bm{\alpha}^{\text{new}} - \bm{\alpha}^{\text{old}})^{T}\bm{V}^{T}\bm{V}(\bm{\alpha}^{\text{new}} - \bm{\alpha}^{\text{old}}) - 2(\bm{\alpha}^{\text{new}} - \bm{\alpha}^{\text{old}})^{T}\bm{V}^{T}\bm{r} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm}^{\text{new}} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q}(\alpha_{jm}^{new})^2 \\
  \text{s.t. } & |\alpha_{jm}^{\text{new}}| \le Mz_{jm}^{\text{new}} \quad \forall j, \forall m,\\
  & z_{jm}^{\text{new}} \in \{0, 1\}, \\
  & \sum_{j = 1}^{p}z_{jm}^{\text{new}} \le 1 \quad \forall m,
\end{aligned}
}\end{equation} where \(j = 1, \dots, p\), \(m = 1, \dots, q\),
\(\bm{\alpha}^{\text{old}}\) is the current value of \(\bm{\alpha}\),
\(z_{jm}^{\text{new}}\) are the updated set of binary variables to be
estimated, and \(\bm{V}\) is the matrix of partial derivatives of the
right hand side of Equation~\ref{eq-semipara} with respect to
\(\bm{\alpha}_{j}\). The \(i^{th}\) line of \(\bm{V}\) contains
\([ \bm{v}_{i1}, \dots, \bm{v}_{ip}]\), where
\(\bm{v}_{ij} = \bm{x}_{i}g_{j}'(h_{ij})\). The current residual vector,
which contains
\(r_{i} = y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}\big((\bm{\alpha}_{j}^{\text{old}})^T\bm{x}_{i}\big)\),
is denoted by \(\bm{r}\). It is important to note that the additional
covariates \(w_{ik}\) and \(\bm{u}_{i}\) are not required to update
\(\bm{\alpha}_{j}\), because they are constants with respect to
\(\bm{\alpha}_{j}\), and thus they disappear from \(\bm{V}\).

Similar to the explanation given by \textcite{Masselot2022}, the MIQP
objective function in Equation~\ref{eq-smi-update} ignores the Hessian
(the matrix of second derivatives of Equation~\ref{eq-semipara}, with
respect to \(\bm{\alpha}_{j}\)), and considers only the matrix of first
derivatives, which is a quasi-Newton step \autocite{Peng2022}.
Therefore, the \(\bm{\alpha}\) updating step given in
Equation~\ref{eq-smi-update} is assured to be in a descent direction.

After obtaining \(\bm{\alpha}^{\text{new}}\), if any of the estimated
individual index coefficient vectors \(\bm{\alpha}_{j}^{\text{new}}\)
contains all zeros, they will be dropped from the model. Then we scale
each estimated index coefficient vector
\(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j}^{\text{new}}\) to have a unit
norm.

Next, the algorithm alternates between updating the index coefficients
\(\bm{\alpha}\) and estimating the nonlinear functions \(\bm{g}\) until
it meets one of the three criteria: (i) the reduction ratio of the
objective (loss) function value in Equation~\ref{eq-smi-mip}, calculated
between consecutive iterations, reaches a pre-specified convergence
tolerance; (ii) the loss increases consecutively for three iterations;
or (iii) the maximum number of iterations is reached. The selection of
convergence tolerance and maximum iterations depends on the specific
problem or data.

Finally, we consider adjusting the index structure of the model to
exploit any potential benefits in terms of further minimising the loss
function in Equation~\ref{eq-smi-mip}. As indices can be automatically
reduced by dropping zero indices in each optimisation iteration, this
step focuses on potential index additions to the current model.
Specifically, we consider adding a new index to the current model by
identifying dropped predictors. If applicable, a new index is
constructed with all these dropped predictors, and the alternating
updating process from the previous step is repeated. This step continues
until one of these termination criteria is met: (i) the number of
indices reaches \(q\), selecting the final model as output; (ii) loss
increases after the increment, selecting the previous iteration model as
the final SMI model; or (iii) the solution maintains the same number of
indices as the previous iteration, and the absolute difference of index
coefficients between two successive iterations is not larger than a
pre-specified tolerance, choosing the model with the smaller loss as the
final SMI model in this case.

To obtain an estimated model with the best possible forecasting
accuracy, it is important to select appropriate values for the
non-negative penalty parameters \(\lambda_{0}\) and \(\lambda_{2}\). One
possible way to do this is to estimate the model over a grid of possible
values for \(\lambda_{0}\) and \(\lambda_{2}\), and then select the
combination that yields the lowest loss function value. Moreover, it is
also crucial to choose a suitable value for the big-\(M\) parameter, as
the strength of the MIP formulation depends on the choice of a good
upper bound \autocite{Bertsimas2016}. According to
\textcite{Hazimeh2023}, several methods have been used to select \(M\)
in practice. For more details on estimating \(M\) in a linear regression
setting, refer to \textcite{Bertsimas2016}.

The following algorithm summarises the key steps of the SMI Modelling
algorithm. \newline

\subsubsection*{Algorithm 1: SMI Modelling
Algorithm}\label{algorithm-1-smi-modelling-algorithm}
\addcontentsline{toc}{subsubsection}{Algorithm 1: SMI Modelling
Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialise \(p\), the predictor grouping among indices, and obtain the
  initial estimate of \(\bm{\alpha}\) using one of the options in
  Section~\ref{sec-step1}. Then scale each \(\hat{\bm{\alpha}}_{j}\),
  \(j=1,\dots,p\), to have a unit norm.
\item
  Estimate \(g_{j}\), \(j=1,\dots,p\), using a GAM taking \(y_{i}\) as
  the response and \(\hat{\bm{\alpha}}_{j}^{T}\bm{x}_{i}\) as
  predictors.
\item
  Update \(\bm{\alpha}\) through the MIQP in
  Equation~\ref{eq-smi-update}, and scale each \(\hat{\bm{\alpha}}_{j}\)
  to have a unit norm.
\item
  Iterate steps 2 and 3 until convergence, loss increase for three
  consecutive iterations, or reaching the maximum iterations.
\item
  If there are no dropped predictors, stop. Otherwise include a new
  index consisting of dropped predictors, and repeat step 4.
\item
  Increase \(p\) by one in each iteration of step 5 until meeting one of
  the termination criteria below.

  \begin{itemize}
  \tightlist
  \item
    The number of indices in the iteration reaches \(q\); select the
    final fitted model as output.
  \item
    Loss increases after the increment; select previous iteration model
    as the final SMI model.
  \item
    The solution maintains the same number of indices as the previous
    iteration, and the absolute difference of index coefficients between
    two successive iterations is not larger than a pre-specified
    tolerance; select the model with smaller loss as the final SMI
    model.
  \end{itemize}
\end{enumerate}

Throughout the experiments in the paper, we use \(M = 10\), a
convergence tolerance of \(0.001\), and a maximum of \(50\) iterations
in step 4 of Algorithm 1, and a convergence tolerance of \(0.001\) for
coefficients in step 6 of Algorithm 1.

\section{Simulation Experiment}\label{sec-simulation}

This section presents the results of a simulation experiment designed to
demonstrate the performance and characteristics of the proposed SMI
Modelling algorithm. In particular, we investigate how the estimated SMI
model varies depending on the initialisation used.

\subsection{Data Generation}\label{sec-datagen}

\textbf{Generating predictor variables:}

First, we generate two time series, each of length 1205: \(x_{0}\) from
a uniform distribution on the interval \([0, 1]\), and \(z_{0}\) from a
normal distribution \(N(5, 4)\). Next, we construct lagged series of
both \(x_{0}\) and \(z_{0}\), where \(x_i\) denotes the \(i^{th}\) lag
of \(x_{0}\), and \(z_i\) denotes the \(i^{th}\) lag of \(z_{0}\). Then
\(\bm{x} = \{x_{0}, x_{1}, \dots, x_{5}\}\), and
\(\bm{z} = \{z_{0}, z_{1}, \dots, z_{5}\}\)) are taken as predictors in
the simulation experiment.

\textbf{Generating response variables:}

We generate two response variables \(y_{1}\) and \(y_{2}\), using two
different index structures and a normally distributed white noise
component with two different variances as follows:

\begin{itemize}
\item
  Low noise level - \(N(\mu = 0, \sigma^2 = 0.01)\):\\
  \(y_{1} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + \epsilon, \quad \epsilon\sim N(0, 0.01)\)\\
  \(y_{2} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + (0.35*x_{2} + 0.7*x_{5})^2 + \epsilon, \quad \epsilon\sim N(0, 0.01)\)
\item
  High noise level - \(N(\mu = 0, \sigma^2 = 0.25)\):\\
  \(y_{1} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + \epsilon, \quad \epsilon\sim N(0, 0.25)\)\\
  \(y_{2} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + (0.35*x_{2} + 0.7*x_{5})^2 + \epsilon, \quad \epsilon\sim N(0, 0.25)\)
\end{itemize}

Hence, the response \(y_{1}\) is constructed using a single index
consisting of the predictor variables \(x_{0}, x_{1}\), and \(x_{3}\),
whereas the other response \(y_{2}\) is constructed using two indices,
where the first index consists of the predictors \(x_{0}, x_{1}\), and
\(x_{3}\), and the second index consists of \(x_{2}\) and \(x_{5}\).
Neither the variable \(x_{4}\) nor any of the \(z\) variables are used
in generating \(y_{1}\) and \(y_{2}\).

Once the data set is generated, the first five observations are
discarded due to the missing values introduced by lagged variables,
leaving a data set with 1200 observations. We use the first 1000
observations as the training set, while the remaining 200 observations
are kept aside as the test set for evaluating the estimated models.

\subsection{Experiment Setup}\label{sec-exp}

We estimate SMI models through the proposed algorithm for each of the
two response variables, using three different sets of predictors as
inputs. Our aim is to assess the algorithm's capability to correctly
pick the relevant predictor variables (and drop the irrelevant
predictors), and to estimate the index structure of the true model.

The three different sets of predictors considered are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All \(\bm{x}\) variables (denoted as ``all \(\bm{x}\)'');
\item
  All \(\bm{x}\) variables and all \(\bm{z}\) variables (denoted as
  ``all \(\bm{x}\) + all \(\bm{z}\)'');
\item
  The first three \(\bm{x}\) variables (i.e.~\(x_{0}, x_{1}\) and
  \(x_{2}\)) and all \(\bm{z}\) variables (denoted as ``some \(\bm{x}\)
  + all \(\bm{z}\)'').
\end{enumerate}

We apply the proposed SMI Modelling algorithm with each of the above
predictor combinations, for both variations of the responses concerning
the noise level. Moreover, we consider every initialisation options
discussed in Section~\ref{sec-step1}, for each of the two responses.

\subsection{Results}\label{sec-sim-results}

We summarise the results of the simulation experiment in
Table~\ref{tbl-simulation}. In the columns, we indicate the index
structure (i.e.~the predictor grouping among indices) estimated by the
proposed algorithm under each of the initialisation options, i.e.,
``PPR'', ``Additive'', ``Linear'', and ``Multiple''. This is detailed
for each combination explored, considering response, input predictors,
and noise levels.

In the simulation experiment, we did not perform any tuning for the
penalty parameters \(\lambda_{0}\) and \(\lambda_{2}\). Our experiments
indicated that, for this simple example, different values of penalty
parameters have a negligible impact on the estimated models. The default
values \(\lambda_{0} = 1\) and \(\lambda_{2} = 1\) were used in
estimating all the models presented in Table~\ref{tbl-simulation}.

\begin{table}[!h]

\caption{\label{tbl-simulation}Simulation experiment results.}

\centering{

\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}{llllll}
\toprule
\textbf{True Model} & \textbf{Predictors} & \textbf{PPR} & \textbf{Additive} & \textbf{Linear} & \textbf{Multiple}\\
\midrule
\addlinespace[0.6em]
\multicolumn{6}{l}{\textbf{Low noise level}}\\
\hspace{1em}$y_{1}$ & all $\bm{x}$ & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$)\\
\hspace{1em}$y_{1}$ & all $\bm{x}$ + all $\bm{z}$ & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$)\\
\hspace{1em}$y_{1}$ & some $\bm{x}$ + all $\bm{z}$ & ($x_{0}, x_{1}, z_{2}, z_{4}$) & ($x_{0}, x_{1}$) ($z_{4}$) ($z_{1}$) & ($x_{0}, x_{1}, z_{2}, z_{4}$) & ($x_{0}, x_{1}, z_{2}, z_{4}$)\\
[0.4em]
\hspace{1em}$y_{2}$ & all $\bm{x}$ & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$)\\
\hspace{1em}$y_{2}$ & all $\bm{x}$ + all $\bm{z}$ & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$)\\
\hspace{1em}$y_{2}$ & some $\bm{x}$ + all $\bm{z}$ & ($x_{0}, x_{1}$) ($x_{2}$) ($z_{4}$) & ($x_{0}, x_{1}, z_{4}$) ($x_{2}$) & ($x_{0}, x_{1}, x_{2}, z_{2}$) & ($x_{0}, x_{1}$) ($x_{2}, z_{2}, z_{3}$)\\
\addlinespace[0.6em]
\multicolumn{6}{l}{\textbf{High noise level}}\\
\hspace{1em}$y_{1}$ & all $\bm{x}$ & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{4}, x_{5}$) & ($x_{0}, x_{1}$) ($x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$)\\
\hspace{1em}$y_{1}$ & all $\bm{x}$ + all $\bm{z}$ & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) ($z_{0}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) ($z_{0}$)\\
\hspace{1em}$y_{1}$ & some $\bm{x}$ + all $\bm{z}$ & ($x_{0}, x_{1}$) ($z_{1}$) ($z_{4}$) & ($x_{0}, x_{1}$) ($z_{1}$) ($z_{4}$) & ($x_{0}, x_{1}, z_{2}, z_{4}$) & ($x_{0}, x_{1}$) ($z_{0}, z_{4}$) ($z_{1}$)\\
[0.4em]
\hspace{1em}$y_{2}$ & all $\bm{x}$ & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) ($x_{4}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) ($x_{4}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}$) ($x_{4}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) ($x_{4}$)\\
\hspace{1em}$y_{2}$ & all $\bm{x}$ + all $\bm{z}$ & ($x_{0}, x_{1}, x_{3}$) ($x_{5}, z_{1}$) ($x_{2}, z_{0}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}, z_{1}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}, z_{0}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$)\\
\hspace{1em}$y_{2}$ & some $\bm{x}$ + all $\bm{z}$ & ($x_{0}, x_{1}, z_{0}, z_{3}, z_{4}$) ($x_{2}$) & ($x_{0}, x_{1}, z_{0}, z_{1}, z_{3}, z_{4}$) ($x_{2}$) & ($x_{0}, x_{1}, x_{2}, z_{0}, z_{3}, z_{4}$) & ($x_{0}, x_{1}, z_{0}, z_{1}, z_{3}, z_{4}$) ($x_{2}$)\\
\bottomrule
\end{tabular}}

}

\end{table}%

At a low noise level, in both cases of ``all \(\bm{x}\)'' and ``all
\(\bm{x}\) + all \(\bm{z}\)'', all initialisations enable the algorithm
to estimate the correct index structure for both \(y_{1}\) and
\(y_{2}\), with an exception in the ``Linear'' option for \(y_{2}\). The
``Linear'' option for \(y_{2}\) selects the correct variables, but fails
to identify the two-index structure. This suggests that initialising the
algorithm with a higher number of indices might be more effective than
with a lower number. In the case of ``some \(\bm{x}\) + all
\(\bm{z}\)'', for both \(y_{1}\) and \(y_{2}\), the models estimated
under all initialisations include some noise variables. This indicates
that when the available predictors are insufficient to capture the data
signal, the algorithm might select irrelevant variables to make up for
the missing signal.

When the fitted models are evaluated for \(y_{1}\), in both cases of
``all \(\bm{x}\)'' and ``all \(\bm{x}\) + all \(\bm{z}\)'', all
initialisations result in a test set Mean Squared Error (MSE) of
\(\approx 0.01\), which corresponds to the variance of the true model
error. This confirms the accuracy with which the SMI Modelling algorithm
estimates the index structure for \(y_{1}\). For \(y_{2}\), all the
estimated models result in a test set MSE of \(\approx 0.16\). This is
an interesting result as the test set MSE of an estimated model with
incorrect index structure, but with correct predictors (``Linear'') is
similar to the models with correct index structure (``PPR'',
``Additive'', and ``Multiple''). This suggests that the selection of the
predictor variables is more important than determining the index
structure of the model in this case. For both \(y_{1}\) and \(y_{2}\),
in the case of ``some \(\bm{x}\) + all \(\bm{z}\)'', MSE on the test set
increases in comparison to the above cases, probably due to the
inclusion of the noise variables.

Moreover, in contrast to \(y_{1}\), the test set MSE values for
\(y_{2}\) are higher than the variance of the true model error.
Intuitively, the complexity of the model \(y_{2}\) is higher than
\(y_{1}\), where the total estimation error of two nonlinear functions
(corresponding to the two indices) for \(y_{2}\), might be higher than
the error of estimating a single nonlinear function for \(y_{1}\).

As expected, the accuracy with which the SMI Modelling algorithm
estimates the index structure is in general lower with the high noise
level than with the low noise level. For both \(y_{1}\) and \(y_{2}\),
most of the estimated models have selected irrelevant variables. In both
the cases of ``all \(\bm{x}\)'' and ``all \(\bm{x}\) + all \(\bm{z}\)'',
all the models estimated for \(y_{1}\) (except for the ``Additive''
option in the ``all \(\bm{x}\)'' case) result in a test set MSE of
\(\approx 0.23\) (which is slightly lower than the variance of the true
model error) irrespective of the fact that in the ``all \(\bm{x}\)''
case, ``PPR'' option, and in the ``all \(\bm{x}\) + all \(\bm{z}\)''
case, ``Additive'' and ``Multiple'' options include irrelevant
variables. This suggests over-fitting when there is low signal-to-noise
ratio in the data. The result is the same for \(y_{2}\), where
irrespective of the different index structures and predictor choices,
the estimated models in the above two predictor combinations produce
similar MSE values on the test set.

Similar to the previous case of low noise level, when only a part of
\(\bm{x}\) variables are provided, the test set MSE values increase for
both \(y_{1}\) and \(y_{2}\), where the estimated models for \(y_{2}\)
produce higher test MSE values in comparison to the models for
\(y_{1}\).

It is worth mentioning here that in real-world forecasting problems, the
true data generating process (DGP) is unknown, and we do not expect an
estimated model to precisely capture the true DGP. Therefore, as long as
the estimated model demonstrates good forecasting accuracy, the index
structure of the estimated model is less important.

Finally, the simulation study indicates that the choice of the
initialisation depends on the data and application. Thus, users are
encouraged to follow a trial-and-error procedure to determine the most
suitable initial model for a given application.

\section{Empirical Applications}\label{sec-application}

\subsection{Forecasting Daily Mortality}\label{sec-mortality}

We apply the SMI Modelling algorithm to a data set used in
\textcite{Masselot2022}, to forecast daily mortality based on heat
exposure. Studying the effects of various environmental exposures such
as weather related variables, pollutants and man-made environmental
conditions on human health, is of significant importance in
environmental epidemiology.

\subsubsection{Description of the Data}\label{description-of-the-data}

For this analysis, we consider daily mortality and heat exposure data
for the Metropolitan Area of Montreal, Qubec, Canada, from 1990 to
2014, for the months June, July, and August (i.e.~summer). The daily
all-cause mortality data were obtained from the National Institute of
Public Health, Qubec, while \emph{DayMet}, a \(1km \times 1km\) grid
data set \autocite{Thornton2021}, was used to extract daily temperature
and humidity data.

Figure~\ref{fig-deaths} shows the time plots of daily deaths during the
summer for the years from 1990 to 1993. The series for each of the four
years are presented separately in a faceted grid for visual clarity.

\begin{figure}

\centering{

\includegraphics{smimodel_files/figure-pdf/fig-deaths-1.pdf}

}

\caption{\label{fig-deaths}Daily mortality in summer in Montreal, Canada
from 1990 to 1993.}

\end{figure}%

Maximum temperature, minimum temperature, and vapour pressure (to
represent the level of humidity) are considered as predictors in this
empirical study. The number of daily deaths is plotted against each of
these predictors in Figures \ref{fig-Tmax}--\ref{fig-Vp}, respectively,
where we can observe that the relationships between these predictors and
the response are slightly nonlinear.

\begin{figure}

\centering{

\includegraphics{smimodel_files/figure-pdf/fig-Tmax-1.pdf}

}

\caption{\label{fig-Tmax}Daily mortality in summer (from 1990 to 2014)
plotted against maximum temperature.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{smimodel_files/figure-pdf/fig-Tmin-1.pdf}

}

\caption{\label{fig-Tmin}Daily mortality in summer (from 1990 to 2014)
plotted against minimum temperature.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{smimodel_files/figure-pdf/fig-Vp-1.pdf}

}

\caption{\label{fig-Vp}Daily mortality in summer (from 1990 to 2014)
plotted against vapour pressure.}

\end{figure}%

\pagebreak[3]

\subsubsection{Predictors Considered}\label{predictors-considered}

\paragraph{1) Daily deaths lags:}\label{daily-deaths-lags}

Fourteen lags of the daily deaths itself are used as predictors to
incorporate the serial correlations presented in the data into the
modelling process.

\paragraph{2) Current maximum/minimum temperatures and
lags:}\label{current-maximumminimum-temperatures-and-lags}

In addition to current maximum and minimum temperatures, the temperature
measurements up to 14 days prior are considered as predictors in the
forecasting model. This accounts for the cumulative impact of both
current and recent past temperatures on a person's heat exposure.

\paragraph{3) Current vapour pressure and
lags:}\label{current-vapour-pressure-and-lags}

Similarly, the current value and 14 lags of vapour pressure are
considered as predictors, as a proxy for the level of humidity.

\paragraph{4) Calendar effects:}\label{calendar-effects}

Finally, a couple of calendar variables (\emph{day of the season (DOS)}
and \emph{Year}) are incorporated into the model to capture annual trend
and seasonality, which is a common practice in environmental
epidemiology.

\subsubsection{Modelling Framework}\label{modelling-framework}

Death lags, maximum temperature lags, minimum temperature lags, and
vapour pressure lags are predictors that may enter indices. The two
calendar variables, \emph{DOS} and \emph{Year}, are included in the
model as separate nonparametric components that do not enter any of the
indices. Hence, the relevant SMI model can be written as
\begin{equation}\phantomsection\label{eq-heat}{
  \textbf{Deaths} = \beta_{0} + \sum_{j = 1}^{p}{g_{j}(\bm{X}\bm{\alpha}_{j})} + f_{1}(\textbf{DOS}) + f_{2}(\textbf{Year})+ \bm{\varepsilon},
}\end{equation} where

\begin{itemize}
\tightlist
\item
  \(\textbf{Deaths}\) is the vector containing daily deaths
  observations;
\item
  \(\beta_{0}\) is the model intercept;
\item
  \(p\) is the unknown number of indices to be estimated via the
  proposed algorithm;
\item
  \(\bm{X}\) is a matrix containing the \(q=59\) predictor variables
  that may enter indices (i.e.~death lags, maximum temperature lags,
  minimum temperature lags, and vapour pressure lags);
\item
  \(\bm{\alpha}_{j}\), \(j = 1, \dots, p\) are the index coefficient
  vectors, each of length \(q\);
\item
  \(g_{1}, \dots,g_p\), \(f_{1}\), and \(f_{2}\) are unknown
  nonparametric functions; and
\item
  \(\bm{\varepsilon}\) is the error term.
\end{itemize}

The data from 1990 to 2012 are used as the training set to estimate the
model, while the data from the three summer months of year 2013 are kept
aside as a validation set. This validation set is used to perform the
penalty parameter tuning of the SMI models. Here, once the tuning for
the two penalty parameters is completed, the models are re-fitted for
the entire data set combining training and validation sets. The data of
year 2014 are used as the test set for evaluating forecasting
performance.

The forecasting accuracy on the test set is evaluated using MSE and Mean
Absolute Error (MAE). We assume that the future values of the
maximum/minimum temperatures and vapour pressure are known to use in the
forecasting model; thus it is a post hoc analysis.

\subsubsection{Benchmark Methods}\label{sec-benchmark}

In addition to our proposed SMI model, we also evaluate three benchmark
models for comparison purposes.

The first benchmark is a nonparametric additive model formulated through
\textbf{\emph{Backward Elimination}}, as proposed by \textcite{FH2012}.
The process starts with all variables included in an additive model, and
variables are progressively omitted until the optimal model is obtained
based on the validation set. Once the optimal model is obtained, the
final model is re-fitted for the entire data set combining training and
validation sets.

The second benchmark is a \textbf{\emph{Group-wise Additive Index Model
(GAIM)}}, which can be written in the form \[
  y_{i} = \sum_{j = 1}^{p} g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \varepsilon_{i}, 
\] where \(y_{i}\) is the univariate response,
\(\bm{x}_{ij} \in \mathbb{R}^{l{j}}\), \(j = 1, \dots, p\) are
pre-specified non-overlapping subsets of \(\bm{x}_{i}\), and
\(\bm{\alpha}_j\) are the corresponding index coefficients, \(g_{j}\) is
an unknown (possibly nonlinear) component function, and
\(\varepsilon_{i}\) is the random error, which is independent of
\(\bm{x}_{i}\) \autocite{Wang2015,Masselot2022}. For this model, death
lags, maximum temperature lags, minimum temperature lags, and vapour
pressure lags are intuitively categorised into four separate groups,
with an index estimated for each group.

The third benchmark is a \textbf{\emph{Projection Pursuit Regression
(PPR)}} model \autocite{Friedman1981} given by \[
  y_{i} = \sum_{j=1}^{p} {g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} + \varepsilon_{i},
\] where \(y_{i}\) is the response, \(\bm{x}_{i}\) is the
\(q\)-dimensional predictor vector,
\(\bm{\alpha}_{j} = ( \alpha_{j1}, \dots, \alpha_{jp} )^{T}\),
\(j = 1, \dots, p\) are \(q\)-dimensional projection vectors (or vectors
of ``index coefficients''), \(g_{j}\)'s are unknown nonlinear functions,
and \(\varepsilon_{i}\) is the random error. The number of indices in
the PPR model was taken as four, matching the number of indices
estimated by the GAIM.

Since a validation set is not used in estimating the two benchmark
models \emph{GAIM} and \emph{PPR}, these two models are estimated on the
entire data set combining training and validation sets.

\subsubsection{Results}\label{results}

We estimated SMI models for the mortality data using three different
initialisation options: ``PPR'', ``Additive'' and ``Linear'', for
comparison purposes. We tuned the penalty parameters \(\lambda_{0}\) and
\(\lambda_{2}\) over ranges of integers from 1 to 12, and 0 to 12
respectively, based on validation set MSE. Here, a greedy search is used
instead of a grid search to reduce computational time.

The penalty parameter combination
\((\lambda_{0} = 5, \lambda_{2} = 12)\) was selected for the model
fitted with ``PPR'' initialisation. The estimated model,
\textbf{\emph{SMI Model (5, 12) - PPR}}, resulted in seven indices
without dropping any of the index variables. The optimal penalty
parameter combination for the model initiated with ``Additive'' was
\((\lambda_{0} = 1, \lambda_{2} = 0)\), resulting in the
\textbf{\emph{SMI Model (1, 0) - Additive}}, equivalent to a
nonparametric additive model (no index variables or indices were
dropped). The model estimated with ``Linear'' initialisation selected
\((\lambda_{0} = 6, \lambda_{2} = 11)\) (\textbf{\emph{SMI Model (6, 11)
- Linear}}), and resulted in two indices, without dropping any of the
index variables.

We evaluated forecasting errors of the estimated models using two
subsets of the original test set:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{\emph{Test Set 1:}} original test set spanning three months
  (June, July and August 2014); and
\item
  \textbf{\emph{Test Set 2:}} a test set covering one month (June 2014).
\end{enumerate}

The MSE and MAE values on the two different test sets for the estimated
SMI models along with all benchmark methods considered are presented in
Table~\ref{tbl-heat}. We observe that the SMI model estimated with
``PPR'' initialisation, \textbf{\emph{SMI Model (5, 12) - PPR}}, shows
the best forecasting performance on both test sets, compared to the
other two estimated SMI models.

\begin{table}[!hb]

\caption{\label{tbl-heat}Daily mortality forecasting - Out-of-sample
point forecast results.}

\centering{

\centering
\begin{tabular}{lrr>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Test Set 1} & \multicolumn{2}{c}{Test Set 2} \\
\cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7}
\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Predictors} & \multicolumn{1}{c}{Indices} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE}\\
\midrule
SMI Model (5, 12) - PPR & 61 & 7 & \textbf{85.233} & \textbf{7.140} & \textbf{97.353} & \textbf{7.772}\\
SMI Model (1, 0) - Additive & 61 & 59 & 96.398 & 7.481 & 112.199 & 8.156\\
SMI Model (6, 11) - Linear & 61 & 2 & 100.231 & 7.719 & 120.542 & 8.598\\
Backward Elimination & 40 &  & 136.204 & 9.319 & 140.867 & 9.385\\
GAIM & 61 & 4 & 90.763 & 7.247 & 106.251 & 7.928\\
PPR & 61 & 4 & 90.698 & 7.343 & 110.497 & 8.057\\
\bottomrule
\end{tabular}

}

\end{table}%

Table~\ref{tbl-heat} also shows that \textbf{\emph{SMI Model (5, 12) -
PPR}} outperforms all three benchmark models in terms of forecasting
accuracy, for both \emph{Test Set 1} and \emph{Test Set 2}. However, the
SMI models estimated using ``Additive'' or ``Linear'' initialisations
have inferior forecasting performance compared to GAIM and PPR models.
The actual number of deaths and the predicted values from the
\textbf{\emph{SMI Model (5, 12) - PPR}} and benchmark models on
\emph{Test Set 2} are plotted in Figure~\ref{fig-heatPred} for further
comparison.

\begin{figure}

\centering{

\includegraphics{smimodel_files/figure-pdf/fig-heatPred-1.pdf}

}

\caption{\label{fig-heatPred}Actual number of deaths vs.~predicted
number of deaths from ``SMI Model (5, 12) - PPR'' and benchmark models
for Test Set 2.}

\end{figure}%

\subsection{Forecasting Daily Solar Intensity}\label{sec-solar}

Next, we utilise the SMI Modelling algorithm to forecast daily solar
intensity, using other weather conditions.

\subsubsection{Description of the Data}\label{description-of-the-data-1}

We use solar intensity and other weather variables measured at the Davis
weather station in Amherst, Massachusetts, obtained from the \emph{UMass
Trace Repository} \autocite{Umass2023}. The data was recorded at every
five minutes, from 24 February 2006 to 27 February 2013, using sensors
for measuring temperature, wind chill, humidity, dew point, wind speed,
wind direction, rain, pressure, solar intensity, and UV. We converted
the five minutes data to daily data by averaging each variable.

Figure~\ref{fig-solar} shows the time plot of daily solar intensity for
the entire period, which clearly depicts the annual seasonality in the
data. The gaps show days for which the observations were missing; these
were excluded from the analysis.

\begin{figure}

\centering{

\includegraphics{smimodel_files/figure-pdf/fig-solar-1.pdf}

}

\caption{\label{fig-solar}Daily solar intensity in Amherst,
Massachusetts, from February 2006 to February 2013.}

\end{figure}%

In Figure~\ref{fig-preds}, the daily solar intensity is plotted against
each of the predictors: temperature, dew point, wind, rain and humidity.
We can observe that the relationships between these predictors and the
response are nonlinear.

\begin{figure}

\centering{

\includegraphics{smimodel_files/figure-pdf/fig-preds-1.pdf}

}

\caption{\label{fig-preds}Daily solar intensity against other weather
variables.}

\end{figure}%

\subsubsection{Predictors Considered}\label{predictors-considered-1}

\paragraph{1) Solar intensity lags:}\label{solar-intensity-lags}

Three lags of the daily solar intensity itself are used as predictors to
incorporate the serial correlations presented in the data into the
modelling process.

\paragraph{2) Current weather variables and
lags:}\label{current-weather-variables-and-lags}

In addition to current temperature, dew point, wind speed, rain and
humidity, the measurements of the three previous days for each of the
these weather variables are also included as predictors in the
forecasting model.

\paragraph{3) Calendar effects:}\label{calendar-effects-1}

Finally, a calendar variable, \emph{day of the year (DOY)} is
incorporated into the model as a smooth function through Fourier terms
to capture annual seasonality, and also to control the autocorrelation
in residuals.

\subsubsection{Modelling Framework}\label{modelling-framework-1}

The lags of solar intensity, and the lags of weather variables are
considered as predictors that may enter indices. The Fourier terms
capturing the day of the year effect are included into the model as
linear variables (so that they are not part of the indices).

We considered the number of pairs of Fourier terms (\(K\)) from
\(K = 1\) to \(K = 10\), and used the SMI model with ``Linear''
initialisation to determine the number of pairs that best captures the
day of the year effect, based on validation set MSE. (i.e.~Multiple SMI
models were fitted using different numbers of pairs Fourier terms, and
the number of pairs of Fourier terms corresponding to the model that
resulted in the lowest validation set MSE was selected.) The SMI model
(with ``Linear'' initialisation) estimated with \(K = 8\) resulted in
the lowest validation set MSE, and hence \(K = 8\) was taken as the
number of pairs of Fourier terms in estimating all the SMI models and
benchmark models.

Hence, the relevant SMI model can be written as
\begin{equation}\phantomsection\label{eq-solar}{
  \textbf{Solar} = \beta_{0} + \sum_{j = 1}^{p}{g_{j}(\bm{X}\bm{\alpha}_{j})} + \sum_{k = 1}^{8}(\theta_{k}\textbf{DOY}\_\textbf{S}_{\bm{k}} + \delta_{k}\textbf{DOY}\_\textbf{C}_{\bm{k}}) + \bm{\varepsilon},
}\end{equation} where

\begin{itemize}
\tightlist
\item
  \(\textbf{Solar}\) is the vector containing daily observations of
  solar intensity;
\item
  \(\beta_{0}\) is the model intercept;
\item
  \(p\) is the unknown number of indices to be estimated via the
  algorithm;
\item
  \(\bm{X}\) is a matrix containing the \(q=23\) predictor variables
  that may enter indices (i.e.~solar intensity, temperature, dew point,
  wind speed, rain and humidity lags);
\item
  \(\bm{\alpha}_{j}\), \(j = 1, \dots, p\) are the index coefficient
  vectors, each of length \(q\);
\item
  \(g_{1}, \dots, g_{p}\) are unknown nonparametric functions;
\item
  \(\textbf{DOY}\_\textbf{S}_{\bm{k}}\) and
  \(\textbf{DOY}\_\textbf{C}_{\bm{k}}\), \(k = 1, \dots, 8\) are the
  sine and cosine terms of the first eight pairs of Fourier terms
  corresponding to the variable \(\textbf{DOY}\) respectively (seasonal
  period = 365.25);
\item
  \(\theta_{k}\) and \(\delta_{k}\), \(k = 1, \dots, 8\) are the
  coefficients of \(\textbf{DOY}\_\textbf{S}_{\bm{k}}\) and
  \(\textbf{DOY}\_\textbf{C}_{\bm{k}}\) respectively; and
\item
  \(\bm{\varepsilon}\) is the error term.
\end{itemize}

The data from February 2006 to October 2012 are used as the training set
to estimate the model, while the data from November and December 2012
are kept aside as a validation set. This validation set is used to
perform the penalty parameter tuning of the SMI models as well as
selecting the best number of pairs of Fourier terms to be used for the
variable \emph{DOY} in this application. Once the tuning for the two
penalty parameters and the selection of the Fourier terms are completed,
the models are re-fitted for the entire data set combining training and
validation sets. The data of January and February 2013 comprise the test
set to evaluate the forecasting performance of the estimated models.

The forecasting accuracy of the estimated models on the test set is
evaluated using MSE and MAE. We assumed that the future values of the
weather variables are known; thus it is a post hoc analysis.

\subsubsection{Results}\label{results-1}

We estimated SMI models for the solar intensity data using three
different initialisation options: ``PPR'', ``Additive'' and ``Linear''.
We also tuned the penalty parameters \(\lambda_{0}\) and
\(\lambda_{2}\), over ranges of integers from 1 to 12, and 0 to 12
respectively.

The penalty parameter combination \((\lambda_{0} = 1, \lambda_{2} = 0)\)
was selected for the model fitted with ``PPR'' initialisation. The
estimated model, \textbf{\emph{SMI Model (1, 0) - PPR}}, resulted in
four indices without dropping any of the index variables. The optimal
penalty parameter combination for the model estimated taking
``Additive'' model as the starting point was
\((\lambda_{0} = 6, \lambda_{2} = 0)\). The estimated SMI model did not
drop any index variables or indices, and thus the final model,
\textbf{\emph{SMI Model (6, 0) - Additive}}, is equivalent to a
semi-parametric additive model. The model estimated with ``Linear''
initialisation also selected \((\lambda_{0} = 1, \lambda_{2} = 0)\).
Unlike the above models, this SMI Model dropped all index variables and
resulted in null indices, and hence, the final model, \textbf{\emph{SMI
Model (1, 0) - Linear}}, is just a linear model with the sixteen linear
variables \(\textbf{DOY}\_\textbf{S}_{\bm{k}}\) and
\(\textbf{DOY}\_\textbf{C}_{\bm{k}}\), \(k = 1, \dots, 8\). Notice that
the all three SMI models have \(\lambda_{2} = 0\), indicating that the
models have omitted the \(\ell_{2}\)-penalty in the estimation process.

Table~\ref{tbl-solar} presents the MSE and MAE values for the estimated
SMI models on the test set. The results indicate that the SMI model
estimated with ``PPR'' initialisation, \textbf{\emph{SMI Model (1, 0) -
PPR}}, shows the best forecasting performance among the three estimated
SMI models.

We also present forecasting errors of the three benchmark models in
Table~\ref{tbl-solar}, to compare with the estimated SMI models. Here,
the GAIM is fitted by grouping the lags of each weather variable into
distinct group, resulting in six indices. The number of indices of the
PPR model was taken as six, matching the number of indices estimated by
the GAIM. Note that here, the calendar variable \emph{DOY} was excluded
when estimating the PPR model because including separate linear terms is
not feasible in a PPR model.

\begin{table}

\caption{\label{tbl-solar}Daily solar intensity forecasting -
Out-of-sample point forecast results.}

\centering{

\centering
\begin{tabular}{lrr>{}r>{}r}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Test Set} \\
\cmidrule(l{3pt}r{3pt}){4-5}
\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Predictors} & \multicolumn{1}{c}{Indices} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE}\\
\midrule
SMI Model (1, 0) - PPR & 39 & 4 & 764.087 & 21.916\\
SMI Model (6, 0) - Additive & 39 & 23 & 981.033 & 24.617\\
SMI Model (1, 0) - Linear & 16 & 0 & 2065.762 & 34.105\\
Backward Elimination & 36 &  & 819.429 & 22.998\\
GAIM & 39 & 6 & 1972.777 & 37.130\\
PPR & 23 & 6 & \textbf{723.067} & \textbf{21.731}\\
\bottomrule
\end{tabular}

}

\end{table}%

Table~\ref{tbl-solar} shows that the forecasting errors of
\textbf{\emph{SMI Model (1, 0) - PPR}} is lower than the backward
elimination and GAIM models. However, the \textbf{\emph{SMI Model (1, 0)
- PPR}} is unable to outperform the PPR model, which has resulted in the
best forecasting accuracy in this case.

Here, it is worth considering the differences between the SMI model and
the PPR model that shows superior forecasting performance. When
estimating a PPR model, both the number of indices and the predictors
within each index (each index includes all provided predictors that are
entering indices) are pre-determined. In contrast, the SMI model takes a
more general and objective approach, where the number of indices as well
as predictors within each index are automatically determined through the
proposed algorithm. Thus, the SMI model faces a more challenging
estimation task due to the limited prior information provided regarding
the model structure.

The actual solar intensity and the predicted values from the
\textbf{\emph{SMI Model (1, 0) - PPR}} and benchmark models are plotted
in Figure~\ref{fig-solarPred} for further comparison.

\begin{figure}

\centering{

\includegraphics{smimodel_files/figure-pdf/fig-solarPred-1.pdf}

}

\caption{\label{fig-solarPred}Actual solar intensity vs.~predicted solar
intensity from ``SMI Model (1, 0) - PPR'' and benchmark models.}

\end{figure}%

\subsection{Software}\label{software}

These two empirical applications were performed using the R statistical
software \autocite{Rcore}. We used the commercial MIP solver
\textbf{\emph{Gurobi}} \autocite{gurobi2023} to solve the MIQPs related
to the proposed SMI Modelling algorithm, through the
\textbf{\emph{Gurobi plug-in}}
\autocite[ROI.plugin.gurobi,][]{Schwendinger2023} available from the
\textbf{\emph{R Optimization Infrastructure}}
\autocites[ROI,][]{Hornik2023,Theusl2020} package. Furthermore, the GAMs
were fitted using the R package \textbf{\emph{mgcv}}
\autocite{mgcv,Wood2011}.

\section{Conclusions and Further Research}\label{sec-conclusion}

In this paper, we presented a novel algorithm for estimating a
nonparametric/semi-parametric additive index model with optimal
predictor selection and predictor grouping, which we refer to as Sparse
Multiple Index (SMI) Model. The SMI Modelling algorithm is an iterative
procedure that is developed based on mixed integer programming to solve
an \(\ell_{0}\)-regularised nonlinear least squares optimisation problem
with linear constraints.

The proposed SMI Modelling algorithm has a number of key features: 1) It
performs automatic selection of both the number of indices and the
predictor grouping when estimating the nonparametric additive index
model. Users need to input the set of predictors entering indices and a
starting model (index structure and a set of index coefficients) to
initiate the algorithm. 2) It performs automatic variable selection,
which is particularly beneficial in high-dimensional settings. This
feature contributes to an objective and principled estimation, reducing
subjectivity across different users. 3) It is capable of estimating a
wide spectrum of models, from single index models (one index) to
additive models (number of indices equals the number of predictors
entering indices). Hence, the SMI Modelling algorithm is a more general
estimation tool for nonparametric additive models. 4) It provides the
flexibility to include separate nonlinear and linear predictors in the
model that are not entering any indices, allowing the estimation of
semi-parametric additive models.

We demonstrated the performance of the proposed algorithm through a
simulation study and two empirical applications. Due to the limited
input information provided to the algorithm, the estimation of a SMI
model is a challenging problem.

In both of the empirical applications presented above, the SMI model
estimated with ``PPR'' initialisation resulted in best forecasting
performance out of the three different SMI models estimated, indicating
that ``PPR'' initialisation might be a more viable option. However, just
these two examples are not sufficient to establish a universally
applicable initialisation option for the SMI model across various
scenarios. Hence, as mentioned in Section~\ref{sec-simulation}, we
encourage users to follow a trial-and-error procedure to identify the
most effective initialisation option for their specific application.

Since the difficulty of specifying an initialisation that works in
general is a limitation of the proposed algorithm, an interesting future
research problem would be to explore the potential for determining a
generalised initialisation for the SMI Modelling algorithm that will
work well across various applications.

This study should be viewed as a first attempt to develop a more
objective methodology for variable selection and model estimation in the
broader class of nonparametric additive models for forecasting. An
important future research problem is therefore, to assess the
performance of the proposed SMI Modelling algorithm across various data
sets with diverse properties, identifying scenarios where it outperforms
other benchmark methods.

Furthermore, the MIQP in the algorithm is somewhat analogous to the
\emph{best subset selection} method frequently used in least squares
problems. Thus, another limitation of the proposed algorithm is the
increase in computational time as the number of predictors and the
number of indices increase. Therefore, it would be an interesting
research to obtain further insights regarding the algorithm to see what
improvements can be made to the algorithm design to reduce the
computational cost in a high-dimensional context.

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

We thank Professor Louise Ryan for useful discussions during the initial
stage of the project, and for her valuable comments and feedback on this
research work.

Furthermore, this research is partially supported by the Monash
eResearch Centre through the use of the MonARCH (Monash Advanced
Research Computing Hybrid) HPC Cluster, and by the Australian Research
Council Industrial Transformation Training Centre in Optimisation
Technologies, Integrated Methodologies, and Applications (OPTIMA),
Project ID IC200100009.


\printbibliography


\end{document}
