\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{multirow}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{
            pdftitle={Sparse Multiple Index Models for High-dimensional Nonparametric Forecasting},
            pdfkeywords={Additive index models, Variable selection,
Dimension reduction, Mixed integer programming},
            colorlinks=true,
            linkcolor=blue,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Sparse Multiple Index Models for High-dimensional Nonparametric
Forecasting}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\usepackage[tabular,lf]{sourcesanspro}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}
        \placefig{2}{1.5}{width=5cm}{_extensions/numbats/wp/monash2}
        \placefig{16.9}{1.5}{width=2.1cm}{_extensions/numbats/wp/MBSportrait}
        \begin{textblock}{4}(16.9,4)ISSN 1440-771X\end{textblock}
        \begin{textblock}{7}(12.7,27.9)\hfill
        \includegraphics[height=0.7cm]{_extensions/numbats/wp/AACSB}~~~
        \includegraphics[height=0.7cm]{_extensions/numbats/wp/EQUIS}~~~
        \includegraphics[height=0.7cm]{_extensions/numbats/wp/AMBA}
        \end{textblock}
        \vspace*{2cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \footnotesize http://monash.edu/business/ebs/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}\vspace*{2cm}}}
\def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false, date=year}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}

\wp{no/yr}
\jel{C10,C14,C22}

\RequirePackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}
\def\placefig#1#2#3#4{\begin{textblock}{.1}(#1,#2)\rlap{\includegraphics[#3]{#4}}\end{textblock}}




\author{Nuwani~Palihawadana, Rob J~Hyndman, Xiaoqian~Wang}
\addresses{\textbf{Nuwani Palihawadana}\newline
Department of Econometrics \& Business Statistics\newline
Clayton VIC 3800\newline
Australia\newline
{Email: nuwani.kodikarapalihawadana@monash.edu}\newline Corresponding author\newline\\[0.5cm]
\textbf{Rob J Hyndman}\newline
Department of Econometrics \& Business Statistics\newline
Clayton VIC 3800\newline
Australia\newline
{Email: rob.hyndman@monash.edu}\\[0.5cm]
\textbf{Xiaoqian Wang}\newline
Department of Econometrics \& Business Statistics\newline
Clayton VIC 3800\newline
Australia\newline
{Email: xiaoqian.wang@monash.edu}\\[0.5cm]
}

\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf Palihawadana, Hyndman, Wang: \@date}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother

% Adjust headwidth in case user has changed geometry in header-includes
\renewcommand{\headwidth}{\textwidth}

\begin{document}
\maketitle
\begin{abstract}
High-dimensionality is a common phenomenon in real-world forecasting
problems. Oftentimes, forecasts are contingent on a long history of
predictors, while the relationships between some predictors and the
response of interest exhibit complex nonlinear patterns. In such a
situation, a nonlinear ``transfer function'' model, with additivity
constraints to mitigate the issue of \emph{curse of dimensionality}, is
a conspicuous choice. Particularly, nonparametric \emph{additive index
models} greatly reduce the number of parameters to be estimated in
comparison to a general additive model. In this paper, we present a
novel algorithm for estimating high-dimensional nonparametric additive
index models, with simultaneous variable selection, which we call
\textbf{\emph{SMI}} (\textbf{\emph{S}}parse \textbf{\emph{M}}ultiple
\textbf{\emph{I}}ndex) \textbf{\emph{Model}}. The SMI Model algorithm is
based on an iterative procedure that applies mixed integer programming
to solve an \(\ell_{0}\)-regularised nonlinear least squares problem. We
demonstrate the functionality and the characteristics of the proposed
algorithm through a simple simulation exercise. We also illustrate the
use of the SMI Model algorithm in two empirical applications related to
forecasting heat exposure related daily mortality and daily solar
intensity.
\end{abstract}
\begin{keywords}
Additive index models, Variable selection, Dimension reduction, Mixed
integer programming
\end{keywords}

\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, frame hidden, interior hidden, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, sharp corners, breakable]}{\end{tcolorbox}}\fi

\hypertarget{sec-introduction}{%
\section{Introduction}\label{sec-introduction}}

Forecasts are often contingent on a very long history of predictors. For
example, when forecasting half-hourly electricity demand, it is common
to use at least a week of historical half-hourly temperatures and other
weather observations \autocite{HF2010}. Similarly, when forecasting bore
levels, rainfall data from up to thousand days earlier can impact the
result \autocite{Bakker2019} due to the complex flow dynamics of
rainfall into aquifers.

On the other hand, in most of these applications, the relationships
between the predictors and the response variable exhibit complex
nonlinear patterns. For instance, the relationship between electricity
demand and temperature is often nonlinear \autocite{HF2010,FH2012}.

These examples suggest a possible nonlinear ``\emph{transfer function}''
model of the form \[
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}, y_{t-1},\dots,y_{t-k}) + \varepsilon_{t},
\] where \(y_{t}\) is the observation of the response variable at time
\(t\), \(\bm{x}_{t}\) is a vector of predictors at time \(t\), and
\(\varepsilon_{t}\) is the random error. By including lagged values of
\(y_{t}\) along with the lagged predictors, we allow for any serial
correlation in the data. However, it makes the resulting function
difficult to interpret. An alternative formulation is \[
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}) + g(\varepsilon_{t}, \varepsilon_{t-1},\dots,\varepsilon_{t-k}),
\] which is more difficult to estimate, but makes it simpler to
interpret the effect of the predictors on the response variable.

When applying the transfer function model to forecast lengthy time
series with complex patterns, the form of \(f\) is nonlinear, involving
complicated interactions, and with a high value of \(p\).

Typically, the form of \(f\) involves many ad hoc model choices. It is
essentially impossible to estimate a \(p\)-dimensional function for
large \(p\) due to the curse of dimensionality
\autocite{Bellman57,Stone82}. Instead, we normally impose some form of
additivity, along with some low-order interactions.

For example, \textcite{FH2012} proposed a \textbf{\emph{semi-parametric
additive model}} to obtain short-term forecasts of the half-hourly
electricity demand for power systems in the Australian National
Electricity Market. In this model, \(f\) is assumed to be fully
additive, and is used to capture the effects of recent predictor values
on the demand. The main objective behind the use of this proposed
semi-parametric model is to allow nonparametric components in a
regression-based modelling framework with serially correlated errors
\autocite{FH2012}. The model fitted for each half-hourly period (\(q\))
can be written as \[
 \log(y_{t,q}) = h_{q}(t) + f_{q}(w_{1,t},w_{2,t}) + a_{q}(y_{t-,p}) + \varepsilon_{t},
\] where the response variable is the logarithm of electricity demand at
time \(t\) (measured in the half-hourly intervals) during period \(q\).
The term \(h_{q}(t)\) models several calendar effects that are included
as linear terms. The temperature effects are modelled using the
nonparametric component \(f_{q}(w_{1,t},w_{2,t})\), while the
nonparametric term \(a_{q}(y_{t-,p})\) captures the lagged effects of
the response. It is important to notice here that the error term
\(\varepsilon_{t}\) is serially uncorrelated in each half-hourly model,
because the serial correlation is eliminated by the inclusion of the
lagged responses in the model. However, there will still be some
correlation between the residuals from the various half-hourly models
\autocite{FH2012}.

Similarly, a \textbf{\emph{distributed lag model}} was proposed by
\textcite{Wood2017} to forecast daily death rate in Chicago using
measurements of several air pollutants. In this model, the response
variable is modelled via a sum of smooth functions of lagged predictor
variables, which is quite similar in nature to the semi-parametric
additive model used by \textcite{FH2012}. However, unlike in
\textcite{FH2012}, \textcite{Wood2017} suggested to allow the smooth
functions for lags of the same covariate to vary smoothly over lags,
preventing large differences in estimated effects between adjacent lags.
Thus, the model is of the form \[
 \log(y_{t}) = f_{1}(t) + \sum_{k=0}^{K} f_{2}(p_{t-k}, k) + \sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k),
\] where \(y_{t}\) is the death rate at day \(t\), and \(f_{1}\) is a
nonparametric term to capture the \emph{time} effect. The model
incorporates the current value (\(k = 0\)) and several lagged values
(\(k = 1, \dots, K\)) of the predictors, where the \emph{distributed lag
effect} of a single predictor variable, and of an interaction of two
predictor variables are captured by the sum of nonparametric terms
\(\sum_{k=0}^{K} f_{2}(p_{t-k}, k)\) and
\(\sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k)\) respectively. The smooth
functions \(f_{2}\) and \(f_{3}\) are proposed to be estimated using
\emph{tensor product smooths}.

For more examples, \textcite{Ho2020} used semi-paramteric additive
models to estimate ground-level \(PM_{2.5}\) concentrations in Taiwan,
while nonparametric additive models were utilised by
\textcite{Ibrahim2021} for predicting census survey response rates.
Furthermore, \textcite{Ravindra2019} provided a comprehensive review of
the applications of additive models for environmental data, with a
special focus on air pollution, climate change, and human health related
studies.

While such models have been used to address problems including
electricity demand, air quality related mortality rate and groundwater
level forecasting etc.
\autocite{FH2012,HF2010,Wood2017,Peterson2014,Rajaee2019}, there are
still a number of unresolved issues in their applications. In this
paper, we attempt to address two of those issues. Firstly, even though
nonparametric additive models act as a remedy to the curse of
dimensionality as we discussed earlier, the estimation of the model is
still challenging in a high-dimensional setting due to the large number
of nonparametric components to be estimated. Secondly, there is a
noticeable subjectivity in the selection of predictor variables (from
the available predictors) for the model, where in most of the
applications of interest we discussed above, the predictor choices in
the final model are mainly based on empirical explorations or domain
expertise.

There are a number of previous studies that have attempted to address
the issue of variable selection in nonparametric/semi-parametric
additive models to some extend, using various techniques. For example,
\textcite{Huang2010} used a \emph{Least Absolute Shrinkage and Selection
Operator (LASSO)} \autocite{Tibshirani1996} based procedure for variable
selection in nonparametric additive models, whereas \textcite{FH2012}
used a straightforward backward elimination technique to achieve
selection. Moreover, \textcite{Ibrahim2021} and \textcite{Hazimeh2023}
used Mixed Integer Programming based methodologies to provide a solution
to the \emph{best subset selection} problem in nonparametric additive
models. More details of these methods are discussed later in
Section~\ref{sec-background}.

In this paper, however, we are interested in high-dimensional
applications that exhibit complicated interactions among predictors
(specially in the presence of large number of lagged variables), as well
as correlated errors. In such a situation, \emph{``index models''}
(refer Section~\ref{sec-Index}) seem to be useful for improving the
flexibility of the broader class of nonparametric additive models
\autocite{Radchenko2015}, while mitigating the difficulty of estimating
a nonparametric component for each individual predictor.

To our knowledge, no previous research has been done to look at how the
predictor choices can be made more objective and principled in
nonparametric additive index models. Hence, our goal was to develop a
methodology for optimal predictor selection in the context of
high-dimensional nonparametric additive index models. Moreover, due to
computational advancements in the field, the use of \emph{Mathematical
Optimisation} concepts in solving statistical problems has gained a lot
of interest in the recent past \autocite{Theusl2020}. This motivated us
to develop a variable selection algorithm based on mathematical
optimisation techniques.

Additionally, it is crucial to point out that any such variable
selection methodology naturally renders inferential statistics invalid,
since we do not assume the resulting model obtained through the variable
selection procedure to represent the true data generating process.
Hence, our focus in this paper is only on improving forecasts, but not
on making inferences on the resulting parameter estimates.

The rest of this paper is organised as follows. In
Section~\ref{sec-background}, we provide a concise exposition of related
ideas and previous work, while establishing the foundation for this
paper. Section~\ref{sec-SMI} presents our proposed model, \emph{Sparse
Multiple Index Model} (SMI Model), and describes the variable selection
algorithm and estimation procedure. In Section~\ref{sec-simulation}, we
demonstrate the functionality and the characteristics of the proposed
algorithm through a simulation example. Section~\ref{sec-application}
illustrates two empirical applications of the proposed estimation and
variable selection methodology, related to forecasting heat exposure
related daily mortality and daily solar intensity. Concluding remarks
are given in Section~\ref{sec-conclusion}.

\hypertarget{sec-background}{%
\section{Background}\label{sec-background}}

\hypertarget{variable-selection-in-nonparametric-additive-models}{%
\subsection{Variable Selection in Nonparametric Additive
Models}\label{variable-selection-in-nonparametric-additive-models}}

As discussed in Section~\ref{sec-introduction}, the estimation of
nonparametric function \(f\) becomes infeasible in high-dimensional
settings (i.e.~number of predictors is very large) due to curse of
dimensionality. As a result, \emph{nonparametric additive models} have
been employed with growing popularity. Let
\((y_{i}, \bm{x}_{i}), i = 1, \dots, n\), be independent and identically
distributed (i.i.d) observations, and
\(\bm{x}_{i} = (x_{i1}, \dots, x_{ip})^{T}\) be a \(p\)-dimensional
vector of predictor values. Then a nonparametric additive model can be
written as \begin{equation}\protect\hypertarget{eq-add}{}{
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \varepsilon_{i}, \quad i = 1, \dots, n,
}\label{eq-add}\end{equation} where \(f_{j}\)'s are unknown functions
(probably non-linear and smooth), and \(\varepsilon_{i}\) is the random
error \autocite{Lian2012}. Even such an additivity condition is imposed,
estimating the optimal predictive model will still be troublesome when
\(p\) is very large (probably even larger than the sample size \(n\))
due to over-fitting \autocite{Lian2012}. Thus, it is natural to bring in
the sparsity assumption, and assume that some of \(f_{j}\)'s are zero,
which gives rise to the need of a variable selection method to
differentiate between zero and non-zero components, while estimating the
non-zero components \autocite{Huang2010}.

\hypertarget{backward-elimination}{%
\subsubsection{Backward Elimination}\label{backward-elimination}}

In the problem of forecasting long-term peak electricity demand,
\textcite{HF2010} used a stepwise procedure for variable selection
through cross-validation. In the each half-hourly model fitted, the data
is split into training and validation sets, and the predictors are
selected into the model based on the Mean Squared Error (MSE) calculated
for the validation set. Starting from the full model, the predictive
power of each variable is evaluated by dropping one at a time. A
predictor, the removal of which contributed to a decrease in the
validation MSE, is omitted from the model in subsequent steps
\autocite{HF2010}. \textcite{FH2012} used a similar method except for
the fact that they considered the Mean Absolute Percentage Error (MAPE)
as the selection criterion. Therefore, both of these prior work use
stepwise variable selection methodology based on out-of-sample
forecasting performance.

\hypertarget{penalisation-methods}{%
\subsubsection{Penalisation Methods}\label{penalisation-methods}}

According to \textcite{Huang2010}, there are numerous penalised methods
for variable selection and parameter estimation in high-dimensional
settings, including the \emph{bridge estimator} proposed by
\textcite{Frank1993}, the \emph{Least Absolute Shrinkage and Selection
Operator (LASSO)} by \textcite{Tibshirani1996}, the \emph{Smoothly
Clipped Absolute Deviation Penalty} (SCAD) by \textcite{Fan2001}, and
the \emph{Minimum Concave Penalty} (MCP) by \textcite{Zhang2010}. Among
them, we observe that the LASSO and the SCAD penalties are appearing
popularly in literature.

\textcite{Tibshirani1996} introduced the regularisation method,
\textbf{\emph{LASSO}}, for estimating linear models, which minimises the
sum of squared residuals subject to the \(\ell_{1}\) penalty on the
coefficients. Assume the classical linear regression model
\(y_{i} = \sum_{j=1}^{p} {\beta_{j}x_{ij}} +\varepsilon_{i}\), fitted
for the data \((y_{i}, \bm{x_{i}})\), \(i = 1, \dots, n\), where
\(y_{i}\) is the response, \(\bm{x_{i}} = (x_{i1}, \dots, x_{ip})^T\) is
a \(p\)-dimensional vector of predictors,
\(\bm{\beta} = (\beta_{1}, \dots, \beta_{p})^{T}\) is the parameter
vector corresponding to \(\bm{x_{i}}\), and \(\varepsilon_{i}\) is the
random error. Then, the LASSO estimator, \(\bm{\hat{\beta}}_{LASSO}\),
can be obtained by \[
 \bm{\hat{\beta}}_{LASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {|\beta_{j}|}\right\},
\] where \(\bm{x_{j}} = \left (x_{1j}, \dots, x_{nj}\right )^{T}\), and
\(\lambda\) is a non-negative tuning parameter. The LASSO estimator
reduces to the Ordinary Least Squares (OLS) estimator if \(\lambda\) is
equal to zero \autocite{Konzen2016}. Due to the nature of the penalty
applied, LASSO shrinks some of the coefficients towards zero, and sets
the others exactly to zero, where the estimation of coefficients and
variable selection are performed simultaneously \autocite{Konzen2016}.

While showing that the LASSO is not consistent for variable selection in
certain situations, \textcite{Zou2006} introduced \textbf{\emph{Adaptive
Lasso}} (popularly known as ``adaLASSO''); an extension of the LASSO
method, which uses adaptive weights to penalise coefficients using the
LASSO (i.e.~\(\ell_{1}\)) penalty. Thus, the adaLASSO objective function
can be written as \[
 \bm{\hat{\beta}}_{adaLASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {w_{j}|\beta_{j}|}\right\},
\] where the vector of weights
\(\bm{w} = \left (w_{1}, \dots, w_{p} \right )^{T}\) is estimated by
\(\bm{\hat{w}} = 1/|\bm{\hat{\beta}}|^{\gamma}\) for \(\gamma > 0\),
which is a tuning parameter, and \(\bm{\hat{\beta}}\) being any
consistent estimator of \(\bm{\beta}\) \autocite{Zou2006}.

\textcite{Yuan2006} considered the problem of selecting groups of
variables, and discussed extensions of three variable selection and
estimation methods namely, \emph{LASSO} \autocite{Tibshirani1996},
\emph{Least Angle Regression Selection} \autocite[LARS,][]{Efron2004},
and \emph{Non-negative Garrotte} \autocite{Breiman1995}. Consider an
\(n\)-dimensional response vector \(\bm{y}\), and an \(n \times p\)
matrix of predictor values \(\bm{X}\). Then the \textbf{\emph{Group
Lasso}} estimator of the coefficients vector \(\bm{\beta}\) is obtained
by minimising \[
 \frac{1}{2}\left\lVert\bm{y} - \sum_{\ell=1}^{L} {\bm{X}_{\ell}\bm{\beta}_{\ell}}\right\rVert_{2}^{2} + \lambda\sum_{\ell=1}^{L} {\lVert\bm{\beta}_{\ell}\rVert}_{\bm{K}_{\ell}},
\] where \(\bm{X}_{\ell}\) is an \(n \times p_{\ell}\) sub-matrix in
\(\bm{X}\) that corresponds to the \(\ell^{th}\) group of predictors
(\(p_{\ell}\) is the number of predictors in \(\ell^{th}\) group),
\(\bm{\beta}_{\ell}\) is the corresponding vector of coefficients,
\(\ell = 1, \dots, L\),
\(\lVert\bm{\beta}_{\ell}\rVert_{\bm{K}_{\ell}} = (\bm{\beta}_{\ell}' \bm{K}_{\ell} \bm{\beta}_{\ell})^{\frac{1}{2}}\)
with \(\bm{K}_{1}, \dots, \bm{K}_{L}\) being a set of given positive
definite matrices, and \(\lambda\) is a non-negative tuning parameter.
Moreover, \textcite{Simon2013} proposed \textbf{\emph{Sparse-Group
Lasso}}, which is a convex combination of general Lasso and Group Lasso
methods, where the focus is on both ``groupwise sparsity'' (the number
of groups with at least one nonzero coefficient), and ``within group
sparsity'' (the number of nonzero coefficients within each nonzero
group).

According to \textcite{Fan2001}, a penalty function used in penalised
least squares approaches should have three properties. Firstly, it
should be singular at origin to generate a solution that is sparse.
Secondly, it should fulfill certain conditions to be stable in model
selection. Finally, it should be able to generate unbiased estimates for
large coefficients via being bounded by a constant. They argued that all
those three conditions are not satisfied by the penalisation methods
such as the bridge regression \autocite{Frank1993} and the LASSO
\autocite{Tibshirani1996}. Hence they proposed the \textbf{\emph{SCAD}}
penalty function, which is defined in terms of its first derivative as
\[
 p_{\lambda}'(\theta) = \lambda\left\{I(\theta\le\lambda) + \frac{(a\lambda - \theta)_{+}}{(a - 1)\lambda}I(\theta > \lambda)\right\},
\] for some \(a > 2\), and \(\theta > 0\) \autocite{Fan2001}. According
to \textcite{Fan2001}, the SCAD penalty function retains the favourable
properties of both best subset selection and ridge regression, while
having all three desired features, i.e., sparsity, stability, and
unbiasedness.

Based on the above penalisation methods that are originally developed
for linear models, \textcite{Huang2010} proposed a new penalisation
method for variable selection in nonparametric additive model
(Equation~\ref{eq-add}), named \textbf{\emph{Adaptive Group Lasso}}.
They approximated \(f_{j}\)'s using normalised B-spline bases, so that a
linear combination of B-spline basis functions is used to represent an
individual nonparametric component \(f_{j}\). The proposed method is a
generalisation of Adaptive Lasso method \autocite{Zou2006} to the Group
Lasso method \autocite{Yuan2006}.

When the nonparametric additive model in Equation~\ref{eq-add} is
considered, an obvious possibility is that some of the additive
components (i.e.~\(f_{j}\)'s) are being linear. For example, recall the
electricity demand forecasting problem \autocite{HF2010,FH2012}, where
some of the calendar effects are included into the model as linear
variables, whereas lagged temperature and lagged demand variables are
included using nonlinear additive components. Such situations suggest
the use of \emph{semi-parametric partially linear additive models} that
can be mathematically represented as \[
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \sum_{k=1}^{q} {w_{ik}\beta_{k}} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(\bm{x}_{j}\)'s, \(j = 1, \dots, p\), are a set of predictors
that enter the model as nonparametric components, whereas
\(\bm{w}_{k}\)'s, \(k = 1, \dots, q\) are another set of predictors that
are included as linear components. While several studies have assumed
that the number of nonparametric components are fixed, and performed
variable selection only among the linear components of the model
\autocite{Lian2012,Guo2013,Liu2011}, \textcite{Wang2014} introduced a
methodology for selecting both linear and nonlinear components
simultaneously, in the context of correlated, longitudinal data. They
proposed the use of a \textbf{\emph{Penalised Quadratic Inference
Function (PQIF) with double SCAD penalties}} for variable selection and
model estimation, where the correlation structure of the data was
incorporated into the estimation method (see \textcite{Wang2014} for
details).

\hypertarget{time-series-aspect}{%
\subsubsection{Time Series Aspect}\label{time-series-aspect}}

It is worthwhile to briefly mention that there are extensions of the
penalisation methods discussed above, which have specifically proposed
to take the autocorrelation and lag structures in time series data into
account.

\textcite{Wang2007} proposed an extension of the LASSO method for
Regression with Autoregressive Error (REGAR) models. \textcite{Park2013}
and \textcite{Konzen2016} proposed modifications to Adaptive Lasso
method to incorporate the lag structures presented in Autoregressive
Distributed Lag (ADL) models into the variable selection and estimation
methodology. The \textbf{\emph{Ordered Lasso}} was introduced by
\textcite{Tibshirani2016} to deal with time-lagged regression problems,
where we forecast the response value at time \(t\) using the predictor
values from \(K\) previous time points, assuming that the magnitude of
regression coefficients decreases as the lagged predictor moves away
from time \(t\).

However, it is important to note that all the models considered in the
above time series related work are linear; none of them include
nonparametric terms.

\hypertarget{sec-Index}{%
\subsection{Index Models}\label{sec-Index}}

\hypertarget{single-index-model}{%
\subsubsection{Single Index Model}\label{single-index-model}}

The nonparametric additive model (Equation~\ref{eq-add}) estimates the
relationship between the response and the predictors using a sum of
univariate nonlinear functions corresponding to each individual
predictor variable. Hence, it is incapable of handling the interactions
among the predictors, which are ubiquitous in real-world problems
\autocite{Zhang2008}.

As a remedy, the \textbf{\emph{Single Index Model}}, a generalisation of
the linear regression model where the linear predictor is replaced by a
semi-parametric component, is popularly being used in the literature
\autocite{Radchenko2015}. Let \(y_{i}\) be the response, and
\(\bm{x}_{i}\) be a \(p\)-dimensional predictor vector. Then the Single
Index Model can be written as \[
  y_{i} = g \left ( \bm{\alpha}^{T} \bm{x}_{i} \right ) + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(\bm{\alpha}\) is a \(p\)-dimensional vector of unknown
coefficients (i.e.~parameters), \(g\) is an unknown univariate function,
and \(\varepsilon_{i}\) is the random error
\autocite{Stoker1986,Hardle1993}. The linear combination
\(\bm{\alpha}^{T} \bm{x}_{i}\) is called the \emph{index}. SIM is viewed
as a viable alternative to the additive model since it offers more
flexibility and interpretability \autocite{Radchenko2015}.

According to \textcite{Radchenko2015}, SIMs have widely been used in
scenarios with fairly low and moderate dimensionality, where the
corresponding estimation and variable selection techniques are not
directly applicable to the high-dimensional setting. The error sum of
squares of the model being non-convex with respect to index
coefficients, is the main reason behind the existence of very limited
number of methods in high-dimensional case \autocite{Radchenko2015}. For
an extensive summary of available methods, we refer to
\textcite{Radchenko2015}.

\hypertarget{projection-pursuit-regression}{%
\subsubsection{Projection Pursuit
Regression}\label{projection-pursuit-regression}}

\textcite{Friedman1981} introduced \textbf{\emph{Projection Pursuit
Regression}} (PPR) by extending the nonparametric additive model
(Equation~\ref{eq-add}) to enable the modelling of interactions among
predictor variables. On the other hand, PPR is an extension of the
single index model to an \emph{``additive index model''}, given by \[
  y_{i} = \sum_{j=1}^{q} {g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(y_{i}\) is the response, \(\bm{x}_{i}\) is a
\(p\)-dimensional predictor vector,
\(\bm{\alpha}_{j} = \left ( \alpha_{j1}, \dots, \alpha_{jp} \right )^{T}, j = 1, \dots, q\)
are \(p\)-dimensional projection vectors (or vectors of \emph{``index
coefficients''}), \(g_{j}\)'s are unknown univariate functions, and
\(\varepsilon_{i}\) is the random error.

Instead of estimating a single index, PPR estimates multiple indices and
connects them to the response through a sum of univariate nonlinear
functions. These indices are constructed through a \emph{Projection
Pursuit} \autocites[PP,][]{Kruskal1969,Friedman1974} algorithm, which is
considered to be ``interesting'' low-dimensional projections of a
high-dimensional feature space, obtained through the maximisation of an
appropriate objective function or a ``projection index''
\autocite{Huber1985}.

According to \textcite{Zhang2008}, PPR increases the power of additive
models in high-dimensional settings, but it has two major drawbacks.
Firstly, since PP increases the freedom of the additive model, it tends
to overfit in a situation, where there are a lot of unimportant
predictors. Secondly, the interpretation of the model estimated by PPR
will be troublesome as many non-zero elements will be present in each
projection vector \(\bm{\alpha}_{j}\). To overcome these issues,
\textcite{Zhang2008} introduced an \(\ell_{1}\) regularised projection
pursuit algorithm, where the resultant regression model is named as
\textbf{\emph{Sparse Projection Pursuit Regression}} (SpPPR). In SpPPR,
an \(\ell_{1}\) penalty (i.e.~a LASSO penalty) on index coefficients is
added to the cost function (the squared error) at each iteration of the
PP, thereby performing variable selection and model estimation
simultaneously. See \textcite{Zhang2008} for more details.

Although \textcite{Zhang2008} claimed that the SpPPR algorithm can
detect important predictors even in a noisy data set, our experiments
show that it is not particularly scalable for large data sets with both
higher number of predictors and observations.

\hypertarget{group-wise-additive-index-model}{%
\subsubsection{Group-wise Additive Index
Model}\label{group-wise-additive-index-model}}

Even though PPR introduces flexibility and the ability to model
interactions among predictors into additive models, the indices obtained
through PPR contain all the predictors at hand. Hence, even with a
variable selection mechanism like SpPPR \autocite{Zhang2008}, PPR
creates indices possibly by mixing heterogeneous variables in a single
linear combination, making very little sense in terms of
interpretability \autocite{Masselot2022}.

Typically, in many real-world problems, natural groupings can be
identified in predictor variables. For example, naturally interacting
variables can be grouped together, such as several lags of a predictor,
weather related variables, and genes or proteins that are grouped by
biological pathways in a biological study
\autocite{Masselot2022,Wang2015}. According to \textcite{Wang2015},
incorporating such group knowledge into dimension reduction/variable
selection methodologies will improve the interpretability of the results
as well as the estimation accuracy.

This suggests the use of a \textbf{\emph{Group-wise Additive Index
Model}} (GAIM), which can be written as \[
  y_{i} = \sum_{j = 1}^{p} g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \varepsilon_{i}, \quad i = 1, \dots, n, 
\] where \(y_{i}\) is the univariate response,
\(\bm{x}_{ij} \in \mathbb{R}^{l{j}}\), \(j = 1, \dots, p\) are naturally
occurring \(p\) groups of predictors, which are \(p\) non-overlapping
subsets of \(\bm{x}_{i}\) - the vector of all predictors,
\(\bm{\alpha}_{j}\) is a \(l_{j}\)-dimensional vector of index
coefficients corresponding to the index
\(h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}\), \(g_{j}\) is an unknown
(possibly nonlinear) component function, and \(\varepsilon_{i}\) is the
random error, which is independent of \(\bm{x}_{i}\)
\autocite{Wang2015-mp,Masselot2022}.

Since GAIM uses groups of predictors that are naturally or logically
belonging together to construct indices, such derived indices will be
more expressive and interpretable. At the same time, GAIM contributes
towards a greater reduction in the number of unknown parameters, because
only a subset of predictors enters into an index, making GAIM a sparser
model compared to PPR \autocite{Wang2015-mp,Masselot2022}.

On account of these desirable properties of GAIM as a nonparametric
additive model, in this study, we focus on a GAIM framework to propose
our variable selection methodology.

\hypertarget{mathematical-optimisation-for-variable-selection}{%
\subsection{Mathematical Optimisation for Variable
Selection}\label{mathematical-optimisation-for-variable-selection}}

\hypertarget{mathematical-optimisation}{%
\subsubsection{Mathematical
Optimisation}\label{mathematical-optimisation}}

\emph{Optimisation} plays a major role in both decision science and
physical systems evaluation. \textbf{\emph{Mathematical Optimisation}}
or \textbf{\emph{Mathematical Programming}} can be defined as the
minimisation (or maximisation) of a function subject to restrictions on
the unknowns/parameters of that function \autocite{Nocedal2006}. Hence,
a mathematical optimisation problem can be written as
\begin{equation}\protect\hypertarget{eq-opt}{}{
\begin{aligned}
  \min_{\bm{x}} \quad & f_{0}(\bm{x})\\
  \text{s.t.} \quad & f_{i}(\bm{x}) \le b_{i}, \quad i = 1, \dots, m
\end{aligned}
}\label{eq-opt}\end{equation} where the vector of unknowns or parameters
of the problem is given by
\(\bm{x} = \left ( x_{1}, \dots, x_{n} \right )^{T}\), the
\emph{objective function} is denoted by
\(f_{0} : \mathbb{R}^{n} \rightarrow \mathbb{R}\), the \emph{constraint
functions} are given by
\(f_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}\), \(i = 1, \dots, m\),
and the bounds of the constraints are denoted by
\(\bm{b} = \left (b_{1}, \dots, b_{m} \right )^{T}\). A vector of values
\(\bm{x^{*}}\) that results in the smallest value for the objective
function among all vectors that satisfy the stated constraints, is
called the \emph{optimal} value or the \emph{solution} to the problem
\autocite{Boyd2004}. After mathematically formulating the optimisation
problem as above (Equation~\ref{eq-opt}), an appropriate
\emph{optimisation algorithm} is used to obtain the solution
\(\bm{x^{*}}\) \autocite{Nocedal2006}.

Based on the form of the objective function and the constraints, various
types of optimisation problems are identified.

An optimisation problem is known as a \textbf{\emph{Linear Program}}
(LP) when both the objective function and the constraints in
Equation~\ref{eq-opt} (i.e.~all \(f_{i}\), \(i = 0, \dots, m\)) are
linear. Hence, a LP can be written as
\begin{equation}\protect\hypertarget{eq-lp}{}{
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
}\label{eq-lp}\end{equation} where \(\bm{x}\) is the vector that
contains the parameters to be optimised, and
\(\bm{a_{0}} \in \mathbb{R}^{n}\) is the vector of coefficients of the
objective function. The matrix of coefficients in the constraints is
denoted by \(\bm{A} \in \mathbb{R}^{m \times n}\), and \(\bm{b}\) is the
vector containing the upper bounds of the constraints. All LPs are
\emph{convex} optimisation problems \autocite{Theusl2020}.

The LP problem given in Equation~\ref{eq-lp} can be generalised to
involve a quadratic term in the objective function, in which case it is
called a \textbf{\emph{Quadratic Program}} (QP). A QP can be written as
\[
\begin{aligned}
  \min_{\bm{x}} \quad & \frac{1}{2} \bm{x}^{T} \bm{Q_{0}} \bm{x} + \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
\] where \(\bm{Q_{0}} \in \mathbb{R}^{n \times n}\). Unless the matrix
\(\bm{Q_{0}}\) is positive semi-definite, a QP is non-convex
\autocite{Theusl2020}.

If a linear objective function is minimised over a \emph{convex cone},
such an optimisation problem is called a \textbf{\emph{Conic Program}}
(CP), which can be written as \[
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x} \\
  \text{s.t.} \quad & \bm{A}\bm{x} + s = \bm{b}, \quad s \in \mathcal{K},
\end{aligned}
\] where \(\mathcal{K}\) denotes a nonempty closed convex cone. CPs are
designed to model convex optimisation problems \autocite{Theusl2020}.

If we restrict some of the unknowns/parameters in an optimisation
problem to take only integer values, then that optimisation problem is
called a \textbf{\emph{Mixed Integer Program}} (MIP). For example, if we
constraint \(x_{k} \in \mathbb{Z}\) for at least one \(k\),
\(k \in \{1, \dots, n\}\) in the optimisation problem given by
Equation~\ref{eq-opt}, then the optimisation problem becomes a MIP. If
all the unknowns of an optimisation problem are constrained to be
integers, such a problem is referred to as a pure \textbf{\emph{Integer
Program}} (IP), whereas if all the unknowns are bounded between zero and
one (i.e.~\(\bm{x} \in \{ 0, 1 \}^{n}\)), the optimisation problem is
referred to as a \textbf{\emph{Binary (Integer) Program}}
\autocite{Theusl2020}. MIPs are hard to solve as they are non-convex due
to the integer constraints. However, a growth in the number of
commercial as well as non-commercial MIP solvers has made it possible to
solve MIP problems conveniently and directly.

\hypertarget{variable-selection}{%
\subsubsection{Variable Selection}\label{variable-selection}}

Mathematical optimisation is fundamentally important in statistics, as
many statistical problems including regression, classification, and
other types of estimation/approximation problems can be re-interpreted
as optimisation problems \autocite{Theusl2020}. Thus, the problem of
variable selection - one of the prolonged interests of statisticians,
has also benefited from using optimisation concepts, particularly MIP
and convex optimisation, in the recent past.

For example, \textcite{Bertsimas2016} used a mixed integer optimisation
procedure to solve the classical best subset selection problem in a
linear regression. They developed a discrete optimisation method by
extending modern first-order continuous optimisation techniques. The
method can produce near-optimal solutions that would serve as warm
starts for a MIP algorithm, which would choose the best \(k\) features
out of \(p\) predictors. Similarly, \textcite{Hazimeh2020} developed
fast and efficient algorithms based on coordinate descent and local
combinatorial optimisation to solve the same best subset selection (or
\(\ell_{0}\)-regularised least squares) problem through re-formulating
local combinatorial search problems as structured MIPs.

Furthermore, \textcite{Hazimeh2023} proposed a group-wise variable
selection methodology, based on discrete mathematical optimisation,
which is applicable to both \(\ell_{0}\)-regularised linear regression
and nonparametric additive models in a high-dimensional setting. They
formulated the group \(\ell_{0}\)-based estimation problem as a
\emph{Mixed Integer Second Order Cone Program (MISOCP)}, and proposed a
new customised Branch-and-Bound (BnB) algorithm
\autocite{Land1960,Little1963} to obtain the global optimal solution to
the MISOCP.

Through the study of above literature, we noticed that the mathematical
optimisation based algorithms significantly reduce computational cost of
variable selection procedures in high-dimensional settings. This is
largely due to the availability of efficient commercial solvers such as
\emph{Gurobi} and \emph{CPLEX}. This motivated us to focus on a
mathematical optimisation based procedure for developing our variable
selection methodology.

\hypertarget{sec-CGAIM}{%
\subsection{Constrained Group-wise Additive Index
Model}\label{sec-CGAIM}}

The \textbf{\emph{Constrained Group-wise Additive Index Model}} (CGAIM)
was proposed by \textcite{Masselot2022} for constructing comprehensive
and easily interpretable indices from a large set of explanatory
variables. The model of interest is a \emph{semi-parametric group-wise
additive index model} given by
\begin{equation}\protect\hypertarget{eq-2}{}{
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
}\label{eq-2}\end{equation} where \(y_{i}\) is the univariate response,
\(\beta_{0}\) is the model intercept,
\(\bm{x}_{ij} \in \mathbb{R}^{l{j}}\), \(j = 1, \dots, p\) are naturally
occurring \(p\) groups of predictor vectors (i.e.~it is assumed that the
predictor groupings are known in advance), which are \(p\) subsets of
\(\bm{x}_{i}\) - the \(q\)-dimensional vector of all predictors entering
indices, \(\bm{\alpha}_{j}\) is the vector of index coefficients
corresponding to the index \(h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}\),
and \(g_{j}\) is the corresponding nonlinear link function (possibly
estimated by a spline). The additional predictor variables that are
helpful in predicting \(y_{i}\), but do not enter any of the indices are
two-fold: a covariate that relates to the response through a nonlinear
function \(f_{k}\), denoted by \(w_{ik}\), and the vector of linear
covariates denoted by \(\bm{u}_{i}\).

This is an extension of the Group-wise Additive Index Model (GAIM) that
allows to impose constraints on the index coefficients as well as on the
nonlinear link functions. In CGAIM, linear constraints of the form
\(\bm{C}_{j}\bm{\alpha}_{j} \ge 0\) can be imposed on the index
coefficients, where \(\bm{C}_{j} \in \mathbb{R}^{d_{j} \times l_{j}}\),
and \(d_{j}\) is the number of constraints. Moreover, shape constraints
such as monotonicity, convexity or concavity can be imposed on the
nonparametric functions. This modification allows to incorporate prior
knowledge or operational requirements into the model estimation.

First, considering only the additive index part of the model, and given
\((y_{i}, x_{i1}, \dots, x_{iq}), \quad i = 1, \dots, n\) be the
observed data, where the \(q\) predictors are grouped into \(p\) groups,
the estimation problem of the CGAIM can be formulated as
\begin{equation}\protect\hypertarget{eq-3}{}{
\begin{aligned}
  \min_{\bm{\alpha}, \beta_{0}} \quad & \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij})\right ]^{2}, \\
  \text{s.t.} \quad & \bm{C}\bm{\alpha} \ge 0, \quad g_{j} \in m,
\end{aligned}
}\label{eq-3}\end{equation} where
\(\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}\),
\(\beta_{0}\) is the model intercept,
\(\bm{C} \in \mathbb{R}^{d \times q}\), \(d\) is the number of
constraints on the index coefficients vector \(\bm{\alpha}\), and \(m\)
is a shape constraint imposed on \(g_{j}\) \autocite{Masselot2022}.

Notice that \(\bm{\alpha}_{j}\)s behave non-linearly in
Equation~\ref{eq-3}, and hence, this is a non-linear least squares
problem. Accordingly, \textcite{Masselot2022} introduced an efficient
iterative algorithm for estimating the CGAIM based on
\textbf{\emph{Sequential Quadratic Programming}} (SQP), one of the most
successful techniques for solving nonlinear constrained optimisation
problems \autocite{Boggs1995}. The algorithm of \textcite{Masselot2022}
is summarised below.\\

\textbf{\emph{CGAIM Algorithm:}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialise index coefficients \(\bm{\alpha}_{j}\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Estimate
    \(\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}\)
    using the quadratic program (QP) given below in Equation~\ref{eq-4},
    or randomly. \begin{equation}\protect\hypertarget{eq-4}{}{
      \begin{aligned}
       \min_{\bm{\alpha}} \quad & \bm{\alpha}^{T}\bm{X}^{T}\bm{X}\bm{\alpha} - 2\bm{\alpha}^{T}\bm{X}^{T}\bm{y} \\
       \text{s.t.} \quad & \bm{C}\bm{\alpha} \ge 0,
      \end{aligned}
      }\label{eq-4}\end{equation} where \(\bm{y}\) is the vector of
    responses, and \(\bm{X}\) is the matrix of predictors entering
    indices.
  \item
    Scale each \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j, init}\) (the
    estimated vector of initial values for \(\bm{\alpha}_{j}\)) to have
    unit norm.
  \end{enumerate}
\item
  Update functions \(g_{j}\):\\
  Estimate \(g_{j}\)s by a Shape-Constrained Additive Model
  \autocite[SCAM,][]{Pya2015} (or a Generalised Additive Model
  \autocite[GAM,][]{Hastie1986} in unconstrained case) taking \(y_{i}\)
  as the response and the estimated indices
  \(\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{ij}\)s as
  predictors.
\item
  Update index coefficients \(\bm{\alpha}_{j}\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Obtain the update
    \(\bm{\delta} = \left ( \bm{\delta}_{1}, \dots, \bm{\delta}_{p}\right )^{T}, \bm{\delta} = \bm{\alpha}_{old} - \bm{\alpha}_{new}\),
    where \(\bm{\alpha}_{old}\) is the current value of \(\bm{\alpha}\),
    and \(\bm{\alpha}_{new}\) is the new value of \(\bm{\alpha}\) to be
    computed, using the QP given below in Equation~\ref{eq-5}.
    \begin{equation}\protect\hypertarget{eq-5}{}{
      \begin{aligned}
       \min_{\bm{\delta}} \quad & \bm{\delta}^{T}\bm{V}^{T}\bm{V}\bm{\delta} - 2\bm{\delta}^{T}\bm{V}^{T}\bm{r} \\
       \text{s.t.} \quad & \bm{C}\bm{\delta} + \bm{C}\bm{\alpha}_{old} \ge 0,
      \end{aligned}
      }\label{eq-5}\end{equation} where \(\bm{V}\) is the matrix of
    partial derivatives of the right hand side of Equation~\ref{eq-2},
    with respect to \(\bm{\alpha}_{j}\). The \(i^{th}\) line of
    \(\bm{V}\) contains
    \(\left [ \bm{v}_{i1}, \dots, \bm{v}_{ip} \right ]\), where
    \(\bm{v}_{ij} = \bm{x}_{ij}g_{j}^{'}(h_{ij})\). The current residual
    vector, which contains
    \(r_{i} = y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j(old)}^{T}\bm{x}_{ij})\),
    is denoted by \(\bm{r}\).
  \item
    Set updated index coefficients
    \(\bm{\alpha}_{new} = \bm{\alpha}_{old} + \bm{\delta}\).
  \item
    Scale each estimated index coefficients
    \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j(new)}\) to have unit norm.
  \end{enumerate}
\item
  Repeat steps 2 and 3 until convergence or a maximum number of
  iterations is reached.
\end{enumerate}

Note that the extension of the estimation process to include the
additional covariates \(w_{ik}\) and \(\bm{u}_{i}\) is simple. We can
easily add them to the SCAM (or the GAM) in step 2, noticing that they
do not enter the step of updating \(\bm{\alpha}_{j}\) as they are
constants with respect to \(\bm{\alpha}_{j}\), and hence vanish from the
matrix of partial derivatives \(\bm{V}\) \autocite{Masselot2022}.

\hypertarget{sec-SMI}{%
\section{Sparse Multiple Index Model}\label{sec-SMI}}

In this section, we develop a \textbf{\emph{Sparse Group-wise Additive
Index Model}} (hereafter referred to as SGAIM) to establish an objective
and a principled methodology for optimal predictor selection in
high-dimensional nonparametric additive index models.

As reviewed in Section~\ref{sec-CGAIM}, the CGAIM algorithm proposed by
\textcite{Masselot2022} can be used to estimate a semi-parametric
group-wise additive index model (with constraints on the index
coefficients and the non-linear functions where relevant), which is our
model of interest. However, when we provide a group of predictor
variables for which an index to be estimated, the CGAIM algorithm
estimates a coefficient for each of those predictors, where the
estimated index is a linear combination of all the predictors in the
corresponding group. In other words, the CGAIM does not perform any
variable selection, therein the burden of selecting the most predictive
features in a high-dimensional forecasting problem is still on the user.

Our proposed SGAIM can be used to address the issue and enable the
estimation of semi-parametric group-wise additive index models with
variable selection.

\hypertarget{optimisation-problem-formulation}{%
\subsection{Optimisation Problem
Formulation}\label{optimisation-problem-formulation}}

The model of interest is the same semi-parametric group-wise additive
index model as in \textcite{Masselot2022}, which can be written as \[
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where all the term are as defined in Section~\ref{sec-CGAIM}.
However, in our method, we allow the possibility for the index
coefficient vectors \(\bm{\alpha}_{j}\)s to have elements that are equal
to zero, so that the predictors corresponding to such zero coefficients
are dropped out from the model.

Let \(q\) be the total number of predictors entering \(p\)
non-overlapping groups of size \(l_{j}\), \(j = 1, \dots, p\)
(i.e.~\(\sum_{j = 1}^{p} l_{j} = q\)). The algorithm discussed in this
paper apply to the CGAIM estimator (Equation~\ref{eq-3}) (including
additional covariates that are not entering indices as well) with an
\(\ell_{0}\) penalty term and an optional \(\ell_{2}\) (ridge) penalty
term: \begin{equation}\protect\hypertarget{eq-sgaim}{}{
\begin{aligned}
  \min_{\bm{\alpha}, \beta_{0}} \quad \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) - \sum_{k = 1}^{d}f_{k}(w_{ik}) - \bm{\theta}^{T}\bm{u}_{i}\right]^{2} \\
  + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}\bm{1}\left(\alpha_{jm} \neq 0\right) + \lambda_{2}\sum_{j = 1}^{p}\|\bm{\alpha}_{j}\|_2^2
\end{aligned}
}\label{eq-sgaim}\end{equation} where \(\bm{1}(\cdot)\) is the indicator
function, \(\lambda_{0} > 0\) is a tuning parameter that controls the
number of selected predictors, and \(\lambda_{2} \ge 0\) is another
tuning parameter that controls the strength of the additional shrinkage
imposed on the estimated index coefficients.

Applying an \(\ell_{2}\)-penalty in addition to the \(\ell_{0}\)-penalty
is motivated by related literature
\autocite{Hazimeh2020,Mazumder2022,Hazimeh2023}, where it is suggested
that the prediction performance of best-subset selection is enhanced by
the inclusion of an additional ridge penalty, especially when a low
signal-to-noise ratio (SNR) is present.

\hypertarget{mip-formulation}{%
\subsection{MIP Formulation}\label{mip-formulation}}

To solve the optimisation problem (Equation~\ref{eq-sgaim}), we present
a big-M based MIP formulation:
\begin{equation}\protect\hypertarget{eq-sgaim-mip}{}{
\begin{aligned}
  \min_{\bm{\alpha}, \beta_{0}, \bm{z}} \quad & \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}{g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij})} - \sum_{k = 1}^{d} {f_{k}(w_{ik})} - \bm{\theta}^{T}\bm{u}_{i}\right ]^{2} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}z_{jm} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}} \alpha_{jm}^{2} \\
  \text{s.t.} \quad & \left |\alpha_{jm}\right | \le Mz_{jm}, \\
  & z_{jm} \in \{0, 1\}, \\
  & j = 1, \dots, p, \quad m = 1, \dots, l_{j},
\end{aligned}
}\label{eq-sgaim-mip}\end{equation} where
\(\bm{z} = \left (\bm{z}_{1}^{T}, \dots, \bm{z}_{p}^{T} \right )^{T}\),
\(\bm{z}_{j} = \left (z_{j1}, \dots, z_{jl_{j}} \right )^{T}\),
\(j = 1, \dots, p\) such that
\(z_{jm} \in \{0, 1\}, m = 1, \dots, l_{j}\) for all \(j\). In other
words, we introduce a binary (i.e.~indicator) variable corresponding to
each predictor, which is used to make the \emph{``on-or-off''} decision
with respect to that particular predictor. A pre-specified \emph{big-M
parameter} is denoted by \(M < \infty\), and it should be sufficiently
large. If \(\bm{\alpha^{*}}\) is an optimal solution to the problem
given in Equation~\ref{eq-sgaim-mip}, then the big-M parameter should
satisfy \(max \left (\left |\alpha_{jm}^{*}\right | \right ) \le M\),
where \(j \in \{1, \dots, p\}\), and \(m \in \{1, \dots, l_{j}\}\).

Here, the big-M constraints ensure that \(\alpha_{jm}\) is zero if and
only if \(z_{jm}\) is zero, and if \(z_{jm} = 1\), then
\(\left |\alpha_{jm}\right | \le M\). At the same time, the
\(\ell_{0}\)-penalty term
\(\lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}z_{jm}\) influences
some of the binary variables \(z_{jm}\) to be zero, while the
\(\ell_{2}\)-penalty term
\(\lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}\alpha_{jm}^{2}\)
enforces additional shrinkage on the estimated coefficients. Therefore,
these components together perform a variable selection. This modified
problem formulation transforms the quadratic optimisation problem in
\textcite{Masselot2022} (Equation~\ref{eq-3}) to a \emph{Mixed Integer
Quadratic Program (MIQP)}.

Therefore, our main contribution in this paper is two-fold. Firstly, we
propose a new algorithm to objectively select variables in a
semi-parametric additive index model, while contributing towards an
estimated model with a higher forecasting accuracy. Secondly, the
proposed variable selection methodology will contribute towards
estimating a parsimonious model in a high-dimensional setting, even if
the required domain knowledge for selecting the best set of predictors
is unavailable.

\hypertarget{estimation-algorithm}{%
\subsection{Estimation Algorithm}\label{estimation-algorithm}}

In this section, we show how to efficiently find a minimiser for the
problem given in Equation~\ref{eq-sgaim-mip}.

\hypertarget{sec-step1}{%
\subsubsection{Initialising Index Coefficients}\label{sec-step1}}

First, as explained earlier in this section, we introduce \(q\) (total
number of predictors entering indices) binary variables corresponding to
each predictor. Then, we propose to obtain an initial value for the
index coefficients
\(\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}\)
through solving a MIQP as given below.
\begin{equation}\protect\hypertarget{eq-6}{}{
\begin{aligned}
  \min_{\bm{\alpha}, \bm{z}} \quad & \bm{\alpha}^{T}\bm{X}^{T}\bm{X}\bm{\alpha} - 2\bm{\alpha}^{T}\bm{X}^{T}\bm{y} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}z_{jm} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}\alpha_{jm}^{2} \\
  \text{s.t.} \quad & \left |\alpha_{jm}\right | \le Mz_{jm}, \\
  & z_{jm} \in \{0, 1\}, \\
  & j = 1, \dots, p, \quad m = 1, \dots, l_{j},
\end{aligned}
}\label{eq-6}\end{equation} where \(\lambda_{0}\) and \(\lambda_{2}\)
are non-negative tuning parameters, and \(M\) is the pre-specified big-M
value. This is same as solving a penalised least squares problem,
considering only the predictors that are entering indices.

Alternatively, an initial value for \(\bm{\alpha}\), without any
variable selection or shrinkage, can be obtained by the same QP as in
the first step of CGAIM algorithm (Equation~\ref{eq-4}), which is
essentially a multiple linear regression.

Once the estimate for \(\bm{\alpha}\) is obtained, the estimated initial
index coefficient vector of each index
\(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j, init}\) is scaled to have unit
norm to ensure identifiability.

\hypertarget{sec-step2}{%
\subsubsection{Estimating Nonlinear Functions}\label{sec-step2}}

Once we have an estimate for \(\bm{\alpha}\), estimating the SGAIM is
equivalent to estimating a GAM as \[
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\hat{h}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(y_{i}\) is taken as the response, and the estimated indices
\(\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{ij}\)s, and the
additional covariates that are not entering any index are taken as
predictors.

The R packages \textbf{\emph{mgcv}} \autocite{Wood2011}, and
\textbf{\emph{gam}} \autocite{Hastie2023}, for example, can be used to
fit GAMs.

\hypertarget{sec-step3}{%
\subsubsection{Updating Index Coefficients}\label{sec-step3}}

We estimate the new value of index coefficients \(\bm{\alpha}_{new}\)
through an MIQP given in Equation~\ref{eq-7} below, which is again a
modification of the QP used in step 3 of the CGAIM algorithm
(Equation~\ref{eq-5}) to achieve variable selection.
\begin{equation}\protect\hypertarget{eq-7}{}{
\begin{aligned}
  \min_{\bm{\alpha}_{new}, \bm{z}_{new}} \quad & (\bm{\alpha}_{new} - \bm{\alpha}_{old})^{T}\bm{V}^{T}\bm{V}(\bm{\alpha}_{new} - \bm{\alpha}_{old}) - 2(\bm{\alpha}_{new} - \bm{\alpha}_{old})^{T}\bm{V}^{T}\bm{r} \\
  & + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}z_{jm(new)} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}\alpha_{jm(new)}^{2} \\
  \text{s.t.} \quad & \left |\alpha_{jm(new)}\right | \le Mz_{jm(new)}, \\
  & z_{jm(new)} \in \{0, 1\}, \\
  & j = 1, \dots, p, \quad m = 1, \dots, l_{j},
\end{aligned}
}\label{eq-7}\end{equation} where \(z_{jm(new)}\) are binary variables,
and all the other terms are as defined in step 3 of the CGAIM algorithm.

Similar to the explanation given by \textcite{Masselot2022}, the MIQP
objective function in above Equation~\ref{eq-7} ignores the
\emph{Hessian} (or the matrix of second derivatives of
Equation~\ref{eq-2}, with respect to \(\bm{\alpha}_{j}\)), and considers
only the matrix of first derivatives, which is a \emph{quasi-Newton}
step. The \emph{Quasi-Newton Method} is an alternative to the
\emph{Newton's Method} that avoids the calculation of the Hessian to
circumvent its computational burden \autocite{Peng2022}. Therefore, the
\(\bm{\alpha}\) updating step given in above Equation~\ref{eq-7} is
assured to be in a \emph{descent direction}.

Moreover, as in the explanation in Section~\ref{sec-CGAIM}, the
additional covariates \(w_{ik}\) and \(\bm{u}_{i}\) do not step in to
the process of updating \(\bm{\alpha}_{j}\), because they are constants
with respect to \(\bm{\alpha}_{j}\). Therefore, they disappear from
\(\bm{V}\), the matrix of partial derivatives of the right hand side of
Equation~\ref{eq-2}, with respect to \(\bm{\alpha}_{j}\).

Furthermore, similar to Section~\ref{sec-step1}, once the new estimate
\(\bm{\alpha}_{new}\) is obtained, we scale each estimated index
coefficient vector \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j(new)}\) to
have unit norm.

We iterate the above steps described in Section~\ref{sec-step2} and
Section~\ref{sec-step3} until the reduction ratio of the Mean Squared
Error (MSE) obtained between two successive iterations reaches a
pre-specified convergence tolerance. Alternatively, it is also possible
to terminate the algorithm when a pre-specified maximum number of
iterations is reached.

Here, to obtain an estimated model with the best possible forecasting
accuracy, it is important to select appropriate values for the
non-negative penalty parameters \(\lambda_{0}\) and \(\lambda_{2}\). One
possible way to do this is to estimate the model on a training set over
a grid of possible values for \(\lambda_{0}\) and \(\lambda_{2}\), and
then select the combination that yields the lowest MSE on a validation
set, which is not used for training the models.

Moreover, it is also crucial to choose a suitable value for the big-M
parameter, as the strength of the MIP formulation depends on the choice
of a good lower bound \autocite{Bertsimas2016}. According to
\textcite{Hazimeh2023}, several methods have been used to select \(M\)
in practice. For a description on estimating \(M\) in a linear
regression setting, refer to \textcite{Bertsimas2016}.

Additionally, the choice of convergence tolerance and the maximum number
of iterations will depend on the nature of the problem/data to which the
algorithm is applied. In the empirical applications presented in
Section~\ref{sec-application}, we have used a convergence tolerance of
\(0.001\), and \(50\) maximum iterations, where the algorithm is
terminated on whichever is reached first.

The following \textbf{\emph{Algorithm 1}} summarises the key steps of
the SGAIM algorithm.

\textbf{\emph{Algorithm 1: SGAIM Algorithm}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialise \(\bm{\alpha}\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Obtain \(\bm{\alpha}_{init}\) using the MIQP in Equation~\ref{eq-6}
    or the QP in Equation~\ref{eq-4}
  \item
    Scale each \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j, init}\) to have
    unit norm
  \end{enumerate}
\item
  Estimate \(g_{j}\)s:\\
  Estimate \(g_{j}\)s using a GAM taking \(y_{i}\) as the response,
  \(\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{ij}\)s as predictors
\item
  Update \(\bm{\alpha}\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Estimate the new value \(\bm{\alpha}_{new}\) through the MIQP in
    Equation~\ref{eq-7}
  \item
    Scale each \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j(new)}\) to have
    unit norm
  \end{enumerate}
\item
  Iterate:\\
  Repeat steps 2 and 3 until a convergence tolerance or a maximum number
  of iterations is reached
\end{enumerate}

\hypertarget{sec-simulation}{%
\section{Simulation Example}\label{sec-simulation}}

\hypertarget{sec-application}{%
\section{Empirical Applications}\label{sec-application}}

\hypertarget{forecasting-daily-mortality}{%
\subsection{Forecasting Daily
Mortality}\label{forecasting-daily-mortality}}

We apply the SGAIM algorithm to a data set from \textcite{Masselot2022},
to forecast daily mortality based on heat exposure.

Studying the effects of various environmental exposures such as weather
related variables, pollutants and man-made environmental conditions etc.
on human health, is of significant importance in environmental
epidemiology. Therefore, forecasting daily deaths taking heat related
variables as predictors, and constructing interpretable indices of those
predictors that reflect heat-related mortality risk, is an interesting
application.

\hypertarget{description-of-the-data}{%
\subsubsection{Description of the Data}\label{description-of-the-data}}

For this analysis, we consider daily mortality and heat exposure data
for the Metropolitan Area of Montreal, Province of Quebec, Canada, from
1990 to 2014, for the months June, July, and August (i.e.~summer
season). The daily all-cause mortality data were obtained from the
National Institute of Public Health, Province of Quebec, while
\emph{``DayMet''} - a 1 km × 1 km gridded data set
\autocite{Thornton2021} was used to extract daily temperature and
humidity data \autocite{Masselot2022}.

Figure~\ref{fig-deaths} shows the time plots of daily deaths during the
summer for the years from 1990 to 1993. The series for only four years
are plotted separately as a faceted grid for visual clarity.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-deaths-1.pdf}

}

\caption{\label{fig-deaths}Daily mortality in summer in Montreal, Canada
- from 1990 to 1993}

\end{figure}

The three main predictors considered in this empirical study are maximum
temperature, minimum temperature, and vapor pressure (to represent the
level of humidity). The number of daily deaths are plotted against each
of these predictors in Figure~\ref{fig-Tmax}, Figure~\ref{fig-Tmin}, and
Figure~\ref{fig-Vp} respectively, where we can observe that the
relationships between all these predictors and the response are slightly
non-linear.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-Tmax-1.pdf}

}

\caption{\label{fig-Tmax}Daily mortality in summer (from 1990 to 2014)
plotted against maximum temperature}

\end{figure}

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-Tmin-1.pdf}

}

\caption{\label{fig-Tmin}Daily mortality in summer (from 1990 to 2014)
plotted against minimum temperature}

\end{figure}

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-Vp-1.pdf}

}

\caption{\label{fig-Vp}Daily mortality in summer (from 1990 to 2014)
plotted against vapor pressure}

\end{figure}

\hypertarget{predictors-considered}{%
\subsubsection{Predictors Considered}\label{predictors-considered}}

\textbf{1) Current maximum/minimum temperatures and lags:}

In addition to current maximum and minimum temperatures, the temperature
measurements up to 14 days prior (i.e.~\(0^{th}\) to \(14^{th}\) lag)
are also considered as predictors to the forecasting model, because it
is obvious that not only the current temperature, but also the
temperatures that were prevailing in the recent past can add up to the
adverse level of heat exposure of a person.

\textbf{2) Current vapor pressure and lags:}

Similar to temperature variables, the current value and 14 lags of vapor
pressure are considered as predictors, as a proxy to the level of
humidity.

\textbf{3) Calendar effects:}

Finally, a couple of calendar variables; \emph{day of the season (DOS)}
and \emph{Year}, are incorporated into the model to capture annual trend
and seasonality, and also to control the autocorrelation in residuals,
which is a common practice in environmental epidemiology
\autocite{Masselot2022}.

\hypertarget{modelling-framework}{%
\subsubsection{Modelling Framework}\label{modelling-framework}}

Maximum temperature lags, minimum temperature lags and vapor pressure
lags are categorised into three groups of predictors, where we estimate
an index for each of those groups using the proposed SGAIM algorithm.

The two calendar variables, \emph{DOS} and \emph{Year}, are included
into the model as separate nonparametric components that do not enter
any of the indices.

Hence, the relevant SGAIM can be expressed as
\begin{equation}\protect\hypertarget{eq-heat}{}{
\begin{aligned}
  \textbf{Deaths} = & \beta_{0} + g_{1}(\textbf{Tmax\_Lags} * \bm{\alpha}_{1}) + g_{2}(\textbf{Tmin\_Lags} * \bm{\alpha}_{2}) + g_{3}(\textbf{Vp\_Lags} * \bm{\alpha}_{3}) \\
  & + f_{1}(\textbf{DOS}) + f_{2}(\textbf{Year})+ \bm{\varepsilon},
\end{aligned}
}\label{eq-heat}\end{equation} where

\begin{itemize}
\item
  \(\textbf{Deaths}\) is the vector containing the observations of
  number of daily deaths;
\item
  \(\textbf{Tmax\_Lags}\) is a matrix containing lags \(0, \dots, 14\)
  of maximum temperature;
\item
  \(\textbf{Tmin\_Lags}\) is a matrix containing lags \(0, \dots, 14\)
  of minimum temperature;
\item
  \(\textbf{Vp\_Lags}\) is a matrix containing lags \(0, \dots, 14\) of
  vapor pressure;
\item
  \(\bm{\alpha}_{j}, j = 1, 2, 3\) are the index coefficient vectors of
  length 15 each;
\item
  \(g_{j}, j = 1, 2, 3\), \(f_{1}\) and \(f_{2}\) are unknown
  nonparametric functions;
\item
  \(\beta_{0}\) is the model intercept;
\item
  \(\bm{\varepsilon}\) is the vector of errors.
\end{itemize}

The data from 1990 to 2012 are used as the training set to estimate the
model. The data corresponding to the three summer months of year 2013
are kept aside as the validation set, while the data of year 2014 are
separated to be the test set to evaluate the forecasting performance of
the estimated model.

Then we apply the proposed SGAIM algorithm to estimate the index
coefficient vectors \(\bm{\alpha}_{j}, j = 1, 2, 3\)
(Equation~\ref{eq-heat}). After obtaining the optimal values for
\(\bm{\alpha}_{j}\)s, we calculate the indices, and fit a GAM on the
training data, taking the estimated indices and the calendar variables
as predictors. Then the forecasting accuracy on the test set is
evaluated using Mean Squared Error (MSE) and Mean Absolute Error (MAE).

\hypertarget{results}{%
\subsubsection{Results}\label{results}}

\textcolor{purple}{(Work in progress.)}

\textbf{Point Forecasts:}

We considered 14 lags of each heat-related predictor variable in the
model. However, the CGAIM fitted in \textcite{Masselot2022} has
considered only up to the 2nd lag of the each predictor. Hence, for
comparison purposes, we fitted two sets of models; one set of models
with 14 lags and the other set with only 2 lags.

Similar to the previous application, we tuned the penalty parameters
\(\lambda_{0}\) and \(\lambda_{2}\), over ranges of integers from 1 to
15, and 0 to 15 respectively, on the validation set.

\textbf{\emph{Case 1: SGAIM with 14 lags of each predictor}}

The combination \((\lambda_{0} = 15, \lambda_{2} = 0)\) (i.e.~without
ridge penalty) yielded the lowest MSE and MAE on the validation set
(\textbf{\emph{SGAIM (15, 0)}}), where 4 maximum temperature lags (0, 1,
2, 3) were selected by the algorithm for the \emph{maximum temperature
index}, 2 lags of minimum temperature (2, 14) was selected for the
\emph{minimum temperature index}, and 3 lags of vapor pressure (1, 2,
12) were selected for the \emph{vapor pressure index}. Hence, the
algorithm selected only 9 variables for the estimated \emph{SGAIM (15,
0)}, out of the total 45 predictors entering indices. Similar to the
previous application, we used \(M = 10\), a convergence tolerance of
\(0.001\) and \(50\) maximum iterations in estimating all SGAIMs.

We evaluated the forecasting error of the model selected using two
different subsets of the original test set:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{\emph{Test Set 1:}} original test set of 3 months (June, July
  and August of 2014); and
\item
  \textbf{\emph{Test Set 2:}} a test set of 1 month (June 2014).
\end{enumerate}

Note that similar to the previous application of electricity demand
forecasting, we assumed that the future values of the maximum/minimum
temperatures and vapor pressure are known to use in the forecasting
model.

The MSE and MAE values obtained on the two variations of the test set
for \emph{SGAIM (15, 0)} are presented in Table~\ref{tbl-heat1}. The
forecasting errors of a CGAIM with the number of predictor lags
increased to 14, are also presented for comparison purposes. Here,
following \textcite{Masselot2022}, the index coefficients of the CGAIM
were constrained to be positive and decreasing, and the nonparametric
link functions were constrained to be monotonically increasing. We also
fitted an unconstrained GAIM (without variable selection), where the
results are presented in Table~\ref{tbl-heat1}. The actual series of
number of deaths and the predicted series from the \emph{SGAIM (15, 0)}
and the benchmark models on \emph{Test Set 2} are plotted in
Figure~\ref{fig-heat14pred} for further comparison.

\hypertarget{tbl-heat1}{}
\begin{table}[!h]
\caption{\label{tbl-heat1}Daily mortality forecasting (with 14 lags of predictors) - Out-of-sample
point forecast results }\tabularnewline

\centering
\begin{tabular}{lr>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Test Set 1} & \multicolumn{2}{c}{Test Set 2} \\
\cmidrule(l{3pt}r{3pt}){3-4} \cmidrule(l{3pt}r{3pt}){5-6}
\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Predictors} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE}\\
\midrule
SGAIM (15, 0) & 11 & 87.577 & 7.160 & 102.483 & \textbf{7.870}\\
CGAIM & 47 & 85.679 & 7.208 & \textbf{101.689} & 8.032\\
Unconstrained GAIM & 47 & \textbf{81.740} & \textbf{6.920} & 103.036 & 8.066\\
\bottomrule
\end{tabular}
\end{table}

According to Table~\ref{tbl-heat1}, in terms of the test MSE,
\emph{SGAIM (15, 0)} is unable to outperform either the constrained or
unconstrained GAIMs in \emph{Test Set 1}. \emph{SGAIM (15, 0)} reports a
slightly lower test MSE than the unconstrained GAIM, and a slightly
lower test MAE in comparison to both CGAIM and unconstrained GAIM in
\emph{Test Set 2}, but the reductions are not of a considerable
magnitude. However, note that \emph{SGAIM (15, 0)} achieves a
forecasting performance comparable to the benchmark models with a
considerably lesser number of predictors.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-heat14pred-1.pdf}

}

\caption{\label{fig-heat14pred}Actual number of deaths vs.~predicted
number of deaths from ``SGAIM (15, 0)'' and benchmark models for Test
Set 2}

\end{figure}

\textbf{\emph{Case 2: SGAIM with 2 lags of each predictor}}

For the SGAIM with only up to 2 lags of the predictors, the combination
\((\lambda_{0} = 1, \lambda_{2} = 0)\) (i.e.~without ridge penalty)
yielded the lowest MSE on the validation set (\textbf{\emph{SGAIM (1,
0)}}). This model selected all the 3 lags (0, 1, 2) of maximum
temperature for the \emph{maximum temperature index}, only the current
value of minimum temperature for the \emph{minimum temperature index},
and lags 1 and 2 of vapor pressure for the \emph{vapor pressure index}.
Hence, the algorithm selected 6 predictors out of the total 9 predictors
entering indices.

We evaluated the forecasting error of the estimated \emph{SGAIM (1, 0)}
using the same two different subsets of the original test set, where the
results are reported in Table~\ref{tbl-heat2}. The forecasting errors of
CGAIM and unconstrained GAIM with only up to 2 lags for each predictor,
are also presented as benchmark models. The actual series of number of
deaths and the predicted series from the \emph{SGAIM (1, 0)} and the
benchmark models on \emph{Test Set 2} are plotted in
Figure~\ref{fig-heat2pred} for further comparison.

\hypertarget{tbl-heat2}{}
\begin{table}[!h]
\caption{\label{tbl-heat2}Daily mortality forecasting (with 2 lags of predictors) - Out-of-sample
point forecast results }\tabularnewline

\centering
\begin{tabular}{lr>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Test Set 1} & \multicolumn{2}{c}{Test Set 2} \\
\cmidrule(l{3pt}r{3pt}){3-4} \cmidrule(l{3pt}r{3pt}){5-6}
\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Predictors} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE}\\
\midrule
SGAIM (1, 0) & 8 & 85.892 & 7.186 & 103.321 & \textbf{7.999}\\
CGAIM & 11 & 83.625 & \textbf{7.011} & \textbf{101.689} & 8.032\\
Unconstrained GAIM & 11 & \textbf{83.458} & 7.066 & 103.036 & 8.066\\
\bottomrule
\end{tabular}
\end{table}

According to Table~\ref{tbl-heat2}, the SGAIM fitted considering only 2
lags of each predictor too was not able to outperform the corresponding
CGAIM or unconstrained GAIM in terms of MSE. Similar to above \emph{Case
1}, the test MAE reported by the \emph{SGAIM (1, 0)} is slightly lower
than the CGAIM and unconstrained GAIM in \emph{Test Set 2}, but again
the reduction is not considerably large.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-heat2pred-1.pdf}

}

\caption{\label{fig-heat2pred}Actual number of deaths vs.~predicted
number of deaths from ``SGAIM (1, 0)'' and benchmark models for Test Set
2}

\end{figure}

Both the empirical applications presented above were performed using
\textbf{\emph{R statistical software}} \autocite{R2023}, and the
\textbf{\emph{Rstudio}} integrated development environment (IDE)
\autocite{Rstudio2023}. We used the commercial MIP solver
\textbf{\emph{``Gurobi''}} \autocite{gurobi2023} to solve the MIQPs
related to the proposed SGAIM algorithm, through the
\textbf{\emph{Gurobi plug-in (ROI.plugin.gurobi)}}
\autocite{Schwendinger2023} available from the \textbf{\emph{R
Optimization Infrastructure (ROI)}} \autocite{Hornik2023,Theusl2020}
package. Furthermore, the GAMs were fitted using the R package
\textbf{\emph{mgcv}} \emph{(v1.8.42)} \autocite{Wood2011}.

\hypertarget{sec-conclusion}{%
\section{Conclusions}\label{sec-conclusion}}

\textcolor{purple}{(To be completed.)} \newline

\textbf{\large{Acknowledgement}}

We thank Professor Louise Ryan for joining the discussions during the
initial stage of the project, and for her valuable comments and feedback
on this research work.

\printbibliography

\end{document}
