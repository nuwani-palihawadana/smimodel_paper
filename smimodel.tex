\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{multirow}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{
            pdftitle={Sparse Multiple Index Models for High-dimensional Nonparametric Forecasting},
            pdfkeywords={Additive index models, Variable selection,
Dimension reduction, Mixed integer programming},
            colorlinks=true,
            linkcolor=blue,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Sparse Multiple Index Models for High-dimensional Nonparametric
Forecasting}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\usepackage[tabular,lf]{sourcesanspro}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}
        \placefig{2}{1.5}{width=5cm}{_extensions/numbats/wp/monash2}
        \placefig{16.9}{1.5}{width=2.1cm}{_extensions/numbats/wp/MBSportrait}
        \begin{textblock}{4}(16.9,4)ISSN 1440-771X\end{textblock}
        \begin{textblock}{7}(12.7,27.9)\hfill
        \includegraphics[height=0.7cm]{_extensions/numbats/wp/AACSB}~~~
        \includegraphics[height=0.7cm]{_extensions/numbats/wp/EQUIS}~~~
        \includegraphics[height=0.7cm]{_extensions/numbats/wp/AMBA}
        \end{textblock}
        \vspace*{2cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \footnotesize http://monash.edu/business/ebs/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}\vspace*{2cm}}}
\def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false, date=year}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}

\wp{no/yr}
\jel{C10,C14,C22}

\RequirePackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}
\def\placefig#1#2#3#4{\begin{textblock}{.1}(#1,#2)\rlap{\includegraphics[#3]{#4}}\end{textblock}}




\author{Nuwani~Kodikara Palihawadana, Rob J~Hyndman, Xiaoqian~Wang}
\addresses{\textbf{Nuwani Kodikara Palihawadana}\newline
Department of Econometrics \& Business Statistics\newline
Clayton VIC 3800\newline
Australia\newline
{Email: nuwani.kodikarapalihawadana@monash.edu}\newline Corresponding author\newline\\[0.5cm]
\textbf{Rob J Hyndman}\newline
Department of Econometrics \& Business Statistics\newline
Clayton VIC 3800\newline
Australia\newline
{Email: rob.hyndman@monash.edu}\\[0.5cm]
\textbf{Xiaoqian Wang}\newline
Department of Econometrics \& Business Statistics\newline
Clayton VIC 3800\newline
Australia\newline
{Email: xiaoqian.wang@monash.edu}\\[0.5cm]
}

\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf Kodikara Palihawadana, Hyndman, Wang: \@date}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother

% Adjust headwidth in case user has changed geometry in header-includes
\renewcommand{\headwidth}{\textwidth}

\begin{document}
\maketitle
\begin{abstract}
High-dimensionality is a common phenomenon in real-world forecasting
problems. Oftentimes, forecasts are contingent on a long history of
predictors, while the relationships between some predictors and the
response of interest exhibit complex nonlinear patterns. In such a
situation, a nonlinear ``transfer function'' model, with additivity
constraints to mitigate the issue of \emph{curse of dimensionality}, is
a conspicuous choice. Particularly, nonparametric \emph{additive index
models} greatly reduce the number of parameters to be estimated in
comparison to a general additive model. In this paper, we present a
novel algorithm for estimating high-dimensional nonparametric additive
index models, with simultaneous variable selection, which we call
\textbf{\emph{SMI}} (\textbf{\emph{S}}parse \textbf{\emph{M}}ultiple
\textbf{\emph{I}}ndex) \textbf{\emph{Model}}. The SMI Model algorithm is
based on an iterative procedure that applies mixed integer programming
to solve an \(\ell_{0}\)-regularised nonlinear least squares problem. We
demonstrate the functionality and the characteristics of the proposed
algorithm through a simple simulation exercise. We also illustrate the
use of the SMI Model algorithm in two empirical applications related to
forecasting heat exposure related daily mortality and daily solar
intensity.
\end{abstract}
\begin{keywords}
Additive index models, Variable selection, Dimension reduction, Mixed
integer programming
\end{keywords}

\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, interior hidden, borderline west={3pt}{0pt}{shadecolor}, sharp corners, enhanced, frame hidden, boxrule=0pt]}{\end{tcolorbox}}\fi

\hypertarget{sec-introduction}{%
\section{Introduction}\label{sec-introduction}}

Forecasts are often contingent on a very long history of predictors. For
example, when forecasting half-hourly electricity demand, it is common
to use at least a week of historical half-hourly temperatures and other
weather observations \autocite{HF2010}. Similarly, when forecasting bore
levels, rainfall data from up to thousand days earlier can impact the
result \autocite{Bakker2019} due to the complex flow dynamics of
rainfall into aquifers.

On the other hand, in most of these applications, the relationships
between the predictors and the response variable exhibit complex
nonlinear patterns. For instance, the relationship between electricity
demand and temperature is often nonlinear \autocite{HF2010,FH2012}.

These examples suggest a possible nonlinear ``\emph{transfer function}''
model of the form
\begin{equation}\protect\hypertarget{eq-transferfun}{}{
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}, y_{t-1},\dots,y_{t-k}) + \varepsilon_{t},
}\label{eq-transferfun}\end{equation} where \(y_{t}\) is the observation
of the response variable at time \(t\), \(\bm{x}_{t}\) is a vector of
predictors at time \(t\), and \(\varepsilon_{t}\) is the random error.
By including lagged values of \(y_{t}\) along with the lagged
predictors, we allow for any serial correlation in the data. However, it
makes the resulting function difficult to interpret. An alternative
formulation is \[
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}) + g(\varepsilon_{t}, \varepsilon_{t-1},\dots,\varepsilon_{t-k}),
\] which is more difficult to estimate, but makes it simpler to
interpret the effect of the predictors on the response variable.

When applying the transfer function model to forecast lengthy time
series with complex patterns, the form of \(f\) is nonlinear, involving
complicated interactions, and with a high value of \(p\).

Typically, the form of \(f\) involves many ad hoc model choices. It is
essentially impossible to estimate a \(p\)-dimensional function for
large \(p\) due to the curse of dimensionality
\autocite{Bellman57,Stone82}. Instead, we normally impose some form of
additivity, along with some low-order interactions.

For example, \textcite{FH2012} proposed a \textbf{\emph{semi-parametric
additive model}} to obtain short-term forecasts of the half-hourly
electricity demand for power systems in the Australian National
Electricity Market. In this model, \(f\) is assumed to be fully
additive, and is used to capture the effects of recent predictor values
on the demand. The main objective behind the use of this proposed
semi-parametric model is to allow nonparametric components in a
regression-based modelling framework with serially correlated errors
\autocite{FH2012}. The model fitted for each half-hourly period (\(q\))
can be written as \[
 \log(y_{t,q}) = h_{q}(t) + f_{q}(w_{1,t},w_{2,t}) + a_{q}(y_{t-,p}) + \varepsilon_{t},
\] where the response variable is the logarithm of electricity demand at
time \(t\) (measured in the half-hourly intervals) during period \(q\).
The term \(h_{q}(t)\) models several calendar effects that are included
as linear terms. The temperature effects are modelled using the
nonparametric component \(f_{q}(w_{1,t},w_{2,t})\), while the
nonparametric term \(a_{q}(y_{t-,p})\) captures the lagged effects of
the response. It is important to notice here that the error term
\(\varepsilon_{t}\) is serially uncorrelated in each half-hourly model,
because the serial correlation is eliminated by the inclusion of the
lagged responses in the model. However, there will still be some
correlation between the residuals from the various half-hourly models
\autocite{FH2012}.

Similarly, a \textbf{\emph{distributed lag model}} was proposed by
\textcite{Wood2017} to forecast daily death rate in Chicago using
measurements of several air pollutants. In this model, the response
variable is modelled via a sum of smooth functions of lagged predictor
variables, which is quite similar in nature to the semi-parametric
additive model used by \textcite{FH2012}. However, unlike in
\textcite{FH2012}, \textcite{Wood2017} suggested to allow the smooth
functions for lags of the same covariate to vary smoothly over lags,
preventing large differences in estimated effects between adjacent lags.
Thus, the model is of the form \[
 \log(y_{t}) = f_{1}(t) + \sum_{k=0}^{K} f_{2}(p_{t-k}, k) + \sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k),
\] where \(y_{t}\) is the death rate at day \(t\), and \(f_{1}\) is a
nonparametric term to capture the \emph{time} effect. The model
incorporates the current value (\(k = 0\)) and several lagged values
(\(k = 1, \dots, K\)) of the predictors, where the \emph{distributed lag
effect} of a single predictor variable, and of an interaction of two
predictor variables are captured by the sum of nonparametric terms
\(\sum_{k=0}^{K} f_{2}(p_{t-k}, k)\) and
\(\sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k)\) respectively. The smooth
functions \(f_{2}\) and \(f_{3}\) are proposed to be estimated using
\emph{tensor product smooths}.

For more examples, \textcite{Ho2020} used semi-paramteric additive
models to estimate ground-level \(PM_{2.5}\) concentrations in Taiwan,
while nonparametric additive models were utilised by
\textcite{Ibrahim2021} for predicting census survey response rates.
Furthermore, \textcite{Ravindra2019} provided a comprehensive review of
the applications of additive models for environmental data, with a
special focus on air pollution, climate change, and human health related
studies.

While such models have been used to address problems including
electricity demand, air quality related mortality rate and groundwater
level forecasting etc.
\autocite{FH2012,HF2010,Wood2017,Peterson2014,Rajaee2019}, there are
still a number of unresolved issues in their applications. In this
paper, we attempt to address two of those issues. Firstly, even though
nonparametric additive models act as a remedy to the curse of
dimensionality as we discussed earlier, the estimation of the model is
still challenging in a high-dimensional setting due to the large number
of nonparametric components to be estimated. Secondly, there is a
noticeable subjectivity in the selection of predictor variables (from
the available predictors) for the model, where in most of the
applications of interest we discussed above, the predictor choices in
the final model are mainly based on empirical explorations or domain
expertise.

There are a number of previous studies that have attempted to address
the issue of variable selection in nonparametric/semi-parametric
additive models to some extend, using various techniques. For example,
\textcite{Huang2010} used a \emph{Least Absolute Shrinkage and Selection
Operator (LASSO)} \autocite{Tibshirani1996} based procedure for variable
selection in nonparametric additive models, whereas \textcite{FH2012}
used a straightforward backward elimination technique to achieve
selection. Moreover, \textcite{Ibrahim2021} and \textcite{Hazimeh2023}
used Mixed Integer Programming based methodologies to provide a solution
to the \emph{best subset selection} problem in nonparametric additive
models. More details of these methods are discussed later in
Section~\ref{sec-background}.

In this paper, however, we are interested in high-dimensional
applications that exhibit complicated interactions among predictors
(specially in the presence of large number of lagged variables), as well
as correlated errors. In such a situation, \emph{``index models''}
(refer Section~\ref{sec-Index}) seem to be useful for improving the
flexibility of the broader class of nonparametric additive models
\autocite{Radchenko2015}, while mitigating the difficulty of estimating
a nonparametric component for each individual predictor.

To our knowledge, no previous research has been done to look at how the
predictor choices can be made more objective and principled in
nonparametric additive index models. Hence, our goal was to develop a
methodology for optimal predictor selection in the context of
high-dimensional nonparametric additive index models. Moreover, due to
computational advancements in the field, the use of \emph{Mathematical
Optimisation} concepts in solving statistical problems has gained a lot
of interest in the recent past \autocite{Theusl2020}. This motivated us
to develop a variable selection algorithm based on mathematical
optimisation techniques.

Additionally, it is crucial to point out that any such variable
selection methodology naturally renders inferential statistics invalid,
since we do not assume the resulting model obtained through the variable
selection procedure to represent the true data generating process.
Hence, our focus in this paper is only on improving forecasts, but not
on making inferences on the resulting parameter estimates.

The rest of this paper is organised as follows. In
Section~\ref{sec-background}, we provide a concise exposition of related
ideas and previous work, while establishing the foundation for this
paper. Section~\ref{sec-SMI} presents our proposed model, \emph{Sparse
Multiple Index Model} (SMI Model), and describes the variable selection
algorithm and estimation procedure. In Section~\ref{sec-simulation}, we
demonstrate the functionality and the characteristics of the proposed
algorithm through a simulation experiment. Section~\ref{sec-application}
illustrates two empirical applications of the proposed estimation and
variable selection methodology, related to forecasting heat exposure
related daily mortality and daily solar intensity. Concluding remarks
are given in Section~\ref{sec-conclusion}.

\hypertarget{sec-background}{%
\section{Background}\label{sec-background}}

\hypertarget{variable-selection-in-nonparametric-additive-models}{%
\subsection{Variable Selection in Nonparametric Additive
Models}\label{variable-selection-in-nonparametric-additive-models}}

As discussed in Section~\ref{sec-introduction}, the estimation of
nonparametric function \(f\) (Equation~\ref{eq-transferfun}) becomes
infeasible in high-dimensional settings (i.e.~number of predictors is
very large) due to curse of dimensionality. As a result,
\emph{nonparametric additive models} have been employed with growing
popularity. Let \((y_{i}, \bm{x}_{i}), i = 1, \dots, n\), be independent
and identically distributed (i.i.d) observations, and
\(\bm{x}_{i} = (x_{i1}, \dots, x_{ip})^{T}\) be a \(p\)-dimensional
vector of predictor values. Then a nonparametric additive model can be
written as \begin{equation}\protect\hypertarget{eq-add}{}{
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \varepsilon_{i}, \quad i = 1, \dots, n,
}\label{eq-add}\end{equation} where \(f_{j}\)'s are unknown functions
(probably non-linear and smooth), and \(\varepsilon_{i}\) is the random
error \autocite{Lian2012}. Even such an additivity condition is imposed,
estimating the optimal predictive model will still be troublesome when
\(p\) is very large (probably even larger than the sample size \(n\))
due to over-fitting \autocite{Lian2012}. Thus, it is natural to bring in
the sparsity assumption, and assume that some of \(f_{j}\)'s are zero,
which gives rise to the need of a variable selection method to
differentiate between zero and non-zero components, while estimating the
non-zero components \autocite{Huang2010}.

\hypertarget{sec-backward}{%
\subsubsection{Backward Elimination}\label{sec-backward}}

In the problem of forecasting long-term peak electricity demand,
\textcite{HF2010} used a stepwise procedure for variable selection
through cross-validation. In the each half-hourly model fitted, the data
is split into training and validation sets, and the predictors are
selected into the model based on the Mean Squared Error (MSE) calculated
for the validation set. Starting from the full model, the predictive
power of each variable is evaluated by dropping one at a time. A
predictor, the removal of which contributed to a decrease in the
validation MSE, is omitted from the model in subsequent steps
\autocite{HF2010}. \textcite{FH2012} used a similar method except for
the fact that they considered the Mean Absolute Percentage Error (MAPE)
as the selection criterion. Therefore, both of these prior work use
stepwise variable selection methodology based on out-of-sample
forecasting performance.

\hypertarget{penalisation-methods}{%
\subsubsection{Penalisation Methods}\label{penalisation-methods}}

According to \textcite{Huang2010}, there are numerous penalised methods
for variable selection and parameter estimation in high-dimensional
settings, including the \emph{bridge estimator} proposed by
\textcite{Frank1993}, the \emph{Least Absolute Shrinkage and Selection
Operator (LASSO)} by \textcite{Tibshirani1996}, the \emph{Smoothly
Clipped Absolute Deviation Penalty} (SCAD) by \textcite{Fan2001}, and
the \emph{Minimum Concave Penalty} (MCP) by \textcite{Zhang2010}. Among
them, we observe that the LASSO and the SCAD penalties are appearing
popularly in literature.

\textcite{Tibshirani1996} introduced the regularisation method,
\textbf{\emph{LASSO}}, for estimating linear models, which minimises the
sum of squared residuals subject to the \(\ell_{1}\) penalty on the
coefficients. Assume the classical linear regression model
\(y_{i} = \sum_{j=1}^{p} {\beta_{j}x_{ij}} +\varepsilon_{i}\), fitted
for the data \((y_{i}, \bm{x_{i}})\), \(i = 1, \dots, n\), where
\(y_{i}\) is the response, \(\bm{x_{i}} = (x_{i1}, \dots, x_{ip})^T\) is
a \(p\)-dimensional vector of predictors,
\(\bm{\beta} = (\beta_{1}, \dots, \beta_{p})^{T}\) is the parameter
vector corresponding to \(\bm{x_{i}}\), and \(\varepsilon_{i}\) is the
random error. Then, the LASSO estimator, \(\bm{\hat{\beta}}_{LASSO}\),
can be obtained by \[
 \bm{\hat{\beta}}_{LASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {|\beta_{j}|}\right\},
\] where \(\bm{x_{j}} = \left (x_{1j}, \dots, x_{nj}\right )^{T}\), and
\(\lambda\) is a non-negative tuning parameter. The LASSO estimator
reduces to the Ordinary Least Squares (OLS) estimator if \(\lambda\) is
equal to zero \autocite{Konzen2016}. Due to the nature of the penalty
applied, LASSO shrinks some of the coefficients towards zero, and sets
the others exactly to zero, where the estimation of coefficients and
variable selection are performed simultaneously \autocite{Konzen2016}.

While showing that the LASSO is not consistent for variable selection in
certain situations, \textcite{Zou2006} introduced \textbf{\emph{Adaptive
Lasso}} (popularly known as ``adaLASSO''); an extension of the LASSO
method, which uses adaptive weights to penalise coefficients using the
LASSO (i.e.~\(\ell_{1}\)) penalty. Thus, the adaLASSO objective function
can be written as \[
 \bm{\hat{\beta}}_{adaLASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {w_{j}|\beta_{j}|}\right\},
\] where the vector of weights
\(\bm{w} = \left (w_{1}, \dots, w_{p} \right )^{T}\) is estimated by
\(\bm{\hat{w}} = 1/|\bm{\hat{\beta}}|^{\gamma}\) for \(\gamma > 0\),
which is a tuning parameter, and \(\bm{\hat{\beta}}\) being any
consistent estimator of \(\bm{\beta}\) \autocite{Zou2006}.

\textcite{Yuan2006} considered the problem of selecting groups of
variables, and discussed extensions of three variable selection and
estimation methods namely, \emph{LASSO} \autocite{Tibshirani1996},
\emph{Least Angle Regression Selection} \autocite[LARS,][]{Efron2004},
and \emph{Non-negative Garrotte} \autocite{Breiman1995}. Consider an
\(n\)-dimensional response vector \(\bm{y}\), and an \(n \times p\)
matrix of predictor values \(\bm{X}\). Then the \textbf{\emph{Group
Lasso}} estimator of the coefficients vector \(\bm{\beta}\) is obtained
by minimising \[
 \frac{1}{2}\left\lVert\bm{y} - \sum_{\ell=1}^{L} {\bm{X}_{\ell}\bm{\beta}_{\ell}}\right\rVert_{2}^{2} + \lambda\sum_{\ell=1}^{L} {\lVert\bm{\beta}_{\ell}\rVert}_{\bm{K}_{\ell}},
\] where \(\bm{X}_{\ell}\) is an \(n \times p_{\ell}\) sub-matrix in
\(\bm{X}\) that corresponds to the \(\ell^{th}\) group of predictors
(\(p_{\ell}\) is the number of predictors in \(\ell^{th}\) group),
\(\bm{\beta}_{\ell}\) is the corresponding vector of coefficients,
\(\ell = 1, \dots, L\),
\(\lVert\bm{\beta}_{\ell}\rVert_{\bm{K}_{\ell}} = (\bm{\beta}_{\ell}' \bm{K}_{\ell} \bm{\beta}_{\ell})^{\frac{1}{2}}\)
with \(\bm{K}_{1}, \dots, \bm{K}_{L}\) being a set of given positive
definite matrices, and \(\lambda\) is a non-negative tuning parameter.
Moreover, \textcite{Simon2013} proposed \textbf{\emph{Sparse-Group
Lasso}}, which is a convex combination of general Lasso and Group Lasso
methods, where the focus is on both ``groupwise sparsity'' (the number
of groups with at least one nonzero coefficient), and ``within group
sparsity'' (the number of nonzero coefficients within each nonzero
group).

According to \textcite{Fan2001}, a penalty function used in penalised
least squares approaches should have three properties. Firstly, it
should be singular at origin to generate a solution that is sparse.
Secondly, it should fulfill certain conditions to be stable in model
selection. Finally, it should be able to generate unbiased estimates for
large coefficients via being bounded by a constant. They argued that all
those three conditions are not satisfied by the penalisation methods
such as the bridge regression \autocite{Frank1993} and the LASSO
\autocite{Tibshirani1996}. Hence they proposed the \textbf{\emph{SCAD}}
penalty function, which is defined in terms of its first derivative as
\[
 p_{\lambda}'(\theta) = \lambda\left\{I(\theta\le\lambda) + \frac{(a\lambda - \theta)_{+}}{(a - 1)\lambda}I(\theta > \lambda)\right\},
\] for some \(a > 2\), and \(\theta > 0\) \autocite{Fan2001}. According
to \textcite{Fan2001}, the SCAD penalty function retains the favourable
properties of both best subset selection and ridge regression, while
having all three desired features, i.e., sparsity, stability, and
unbiasedness.

Based on the above penalisation methods that are originally developed
for linear models, \textcite{Huang2010} proposed a new penalisation
method for variable selection in nonparametric additive model
(Equation~\ref{eq-add}), named \textbf{\emph{Adaptive Group Lasso}}.
They approximated \(f_{j}\)'s using normalised B-spline bases, so that a
linear combination of B-spline basis functions is used to represent an
individual nonparametric component \(f_{j}\). The proposed method is a
generalisation of Adaptive Lasso method \autocite{Zou2006} to the Group
Lasso method \autocite{Yuan2006}.

When the nonparametric additive model in Equation~\ref{eq-add} is
considered, an obvious possibility is that some of the additive
components (i.e.~\(f_{j}\)'s) are being linear. For example, recall the
electricity demand forecasting problem \autocite{HF2010,FH2012}, where
some of the calendar effects are included into the model as linear
variables, whereas lagged temperature and lagged demand variables are
included using nonlinear additive components. Such situations suggest
the use of \emph{semi-parametric partially linear additive models} that
can be mathematically represented as \[
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \sum_{k=1}^{q} {w_{ik}\beta_{k}} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(\bm{x}_{j}\)'s, \(j = 1, \dots, p\), are a set of predictors
that enter the model as nonparametric components, whereas
\(\bm{w}_{k}\)'s, \(k = 1, \dots, q\) are another set of predictors that
are included as linear components. While several studies have assumed
that the number of nonparametric components are fixed, and performed
variable selection only among the linear components of the model
\autocite{Lian2012,Guo2013,Liu2011}, \textcite{Wang2014} introduced a
methodology for selecting both linear and nonlinear components
simultaneously, in the context of correlated, longitudinal data. They
proposed the use of a \textbf{\emph{Penalised Quadratic Inference
Function (PQIF) with double SCAD penalties}} for variable selection and
model estimation, where the correlation structure of the data was
incorporated into the estimation method (see \textcite{Wang2014} for
details).

\hypertarget{time-series-aspect}{%
\subsubsection{Time Series Aspect}\label{time-series-aspect}}

It is worthwhile to briefly mention that there are extensions of the
penalisation methods discussed above, which have specifically proposed
to take the autocorrelation and lag structures in time series data into
account.

\textcite{Wang2007} proposed an extension of the LASSO method for
Regression with Autoregressive Error (REGAR) models. \textcite{Park2013}
and \textcite{Konzen2016} proposed modifications to Adaptive Lasso
method to incorporate the lag structures presented in Autoregressive
Distributed Lag (ADL) models into the variable selection and estimation
methodology. The \textbf{\emph{Ordered Lasso}} was introduced by
\textcite{Tibshirani2016} to deal with time-lagged regression problems,
where we forecast the response value at time \(t\) using the predictor
values from \(K\) previous time points, assuming that the magnitude of
regression coefficients decreases as the lagged predictor moves away
from time \(t\).

However, it is important to note that all the models considered in the
above time series related work are linear; none of them include
nonparametric terms.

\hypertarget{sec-Index}{%
\subsection{Index Models}\label{sec-Index}}

\hypertarget{single-index-model}{%
\subsubsection{Single Index Model}\label{single-index-model}}

The nonparametric additive model (Equation~\ref{eq-add}) estimates the
relationship between the response and the predictors using a sum of
univariate nonlinear functions corresponding to each individual
predictor variable. Hence, it is incapable of handling the interactions
among the predictors, which are ubiquitous in real-world problems
\autocite{Zhang2008}.

As a remedy, the \textbf{\emph{Single Index Model}}, a generalisation of
the linear regression model where the linear predictor is replaced by a
semi-parametric component, is popularly being used in the literature
\autocite{Radchenko2015}. Let \(y_{i}\) be the response, and
\(\bm{x}_{i}\) be a \(p\)-dimensional predictor vector. Then the single
index model can be written as \[
  y_{i} = g \left ( \bm{\alpha}^{T} \bm{x}_{i} \right ) + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(\bm{\alpha}\) is a \(p\)-dimensional vector of unknown
coefficients (i.e.~parameters), \(g\) is an unknown univariate function,
and \(\varepsilon_{i}\) is the random error
\autocite{Stoker1986,Hardle1993}. The linear combination
\(\bm{\alpha}^{T} \bm{x}_{i}\) is called the \emph{index}. Single index
model is viewed as a viable alternative to the additive model since it
offers more flexibility and interpretability \autocite{Radchenko2015}.

According to \textcite{Radchenko2015}, single index models have widely
been used in scenarios with fairly low and moderate dimensionality,
where the corresponding estimation and variable selection techniques are
not directly applicable to the high-dimensional setting. The error sum
of squares of the model being non-convex with respect to index
coefficients, is the main reason behind the existence of very limited
number of methods in high-dimensional case \autocite{Radchenko2015}. For
an extensive summary of available methods, we refer to
\textcite{Radchenko2015}.

\hypertarget{sec-multi-index}{%
\subsubsection{Multiple Index Models}\label{sec-multi-index}}

\textbf{Projection Pursuit Regression}

\textcite{Friedman1981} introduced \textbf{\emph{Projection Pursuit
Regression (PPR)}} by extending the nonparametric additive model
(Equation~\ref{eq-add}) to enable the modelling of interactions among
predictor variables. On the other hand, PPR is an extension of the
single index model to an \emph{``additive index model''}, given by \[
  y_{i} = \sum_{j=1}^{q} {g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(y_{i}\) is the response, \(\bm{x}_{i}\) is a
\(p\)-dimensional predictor vector,
\(\bm{\alpha}_{j} = \left ( \alpha_{j1}, \dots, \alpha_{jp} \right )^{T}, j = 1, \dots, q\)
are \(p\)-dimensional projection vectors (or vectors of \emph{``index
coefficients''}), \(g_{j}\)'s are unknown univariate functions, and
\(\varepsilon_{i}\) is the random error.

Instead of estimating a single index, PPR estimates multiple indices and
connects them to the response through a sum of univariate nonlinear
functions. These indices are constructed through a \emph{Projection
Pursuit (PP)} \autocite{Kruskal1969,Friedman1974} algorithm, which is
considered to be ``interesting'' low-dimensional projections of a
high-dimensional feature space, obtained through the maximisation of an
appropriate objective function or a ``projection index''
\autocite{Huber1985}.

According to \textcite{Zhang2008}, PPR increases the power of additive
models in high-dimensional settings, but it has two major drawbacks.
Firstly, since PP increases the freedom of the additive model, it tends
to overfit in a situation, where there are a lot of unimportant
predictors. Secondly, the interpretation of the model estimated by PPR
will be troublesome as many non-zero elements will be present in each
projection vector \(\bm{\alpha}_{j}\). To overcome these issues,
\textcite{Zhang2008} introduced an \(\ell_{1}\) regularised projection
pursuit algorithm, where the resultant regression model is named as
\textbf{\emph{Sparse Projection Pursuit Regression}} (SpPPR). In SpPPR,
an \(\ell_{1}\) penalty (i.e.~a LASSO penalty) on index coefficients is
added to the cost function (the squared error) at each iteration of the
PP, thereby performing variable selection and model estimation
simultaneously. See \textcite{Zhang2008} for more details.

Although \textcite{Zhang2008} claimed that the SpPPR algorithm can
detect important predictors even in a noisy data set, our experiments
show that it is not particularly scalable for large data sets with both
higher number of predictors and observations. \newline

\textbf{Group-wise Additive Index Model}

Even though PPR introduces flexibility and the ability to model
interactions among predictors into additive models, the indices obtained
through PPR contain all the predictors at hand. Hence, even with a
variable selection mechanism like SpPPR \autocite{Zhang2008}, PPR
creates indices possibly by mixing heterogeneous variables in a single
linear combination, making very little sense in terms of
interpretability \autocite{Masselot2022}.

Typically, in many real-world problems, natural groupings can be
identified in predictor variables. For example, naturally interacting
variables can be grouped together, such as several lags of a predictor,
weather related variables, and genes or proteins that are grouped by
biological pathways in a biological study
\autocite{Masselot2022,Wang2015}.

This suggests the use of a \textbf{\emph{Group-wise Additive Index Model
(GAIM)}}, which can be written as \[
  y_{i} = \sum_{j = 1}^{p} g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \varepsilon_{i}, \quad i = 1, \dots, n, 
\] where \(y_{i}\) is the univariate response,
\(\bm{x}_{ij} \in \mathbb{R}^{l{j}}\), \(j = 1, \dots, p\) are naturally
occurring \(p\) groups of predictors, which are \(p\) non-overlapping
subsets of \(\bm{x}_{i}\) - the vector of all predictors,
\(\bm{\alpha}_{j}\) is a \(l_{j}\)-dimensional vector of index
coefficients corresponding to the index
\(h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}\), \(g_{j}\) is an unknown
(possibly nonlinear) component function, and \(\varepsilon_{i}\) is the
random error, which is independent of \(\bm{x}_{i}\)
\autocite{Wang2015-mp,Masselot2022}.

Since GAIM uses groups of predictors that are naturally or logically
belonging together to construct indices, such derived indices will be
more expressive and interpretable. However, at the same time, this
introduces a certain level of subjectivity into the model formulation as
different users can group the available predictors in different ways
based on different logical reasoning.

In this paper, our aim is to reduce that subjectivity induced by
personal judgment or domain expertise. Hence, we propose a methodology
that injects more objectivity into the estimation of multiple index
models by algorithmically grouping predictors into indices, resulting in
a model with a higher predictive accuracy. \newline

\textbf{Constrained Group-wise Additive Index Model}

The \textbf{\emph{Constrained Group-wise Additive Index Model (CGAIM)}}
was proposed by \textcite{Masselot2022} for constructing comprehensive
and easily interpretable indices from a large set of explanatory
variables. The model of interest is a \emph{semi-parametric group-wise
additive index model} given by \[
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(y_{i}\) is the univariate response, \(\beta_{0}\) is the
model intercept, \(\bm{x}_{ij} \in \mathbb{R}^{l_{j}}\),
\(j = 1, \dots, p\) are naturally occurring \(p\) groups of predictor
vectors (i.e.~it is assumed that the predictor groupings are known in
advance), which are \(p\) subsets of \(\bm{x}_{i}\) - the
\(q\)-dimensional vector of all predictors entering indices,
\(\bm{\alpha}_{j}\) is the vector of index coefficients corresponding to
the index \(h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}\), and \(g_{j}\) is
the corresponding nonlinear link function (possibly estimated by a
spline). The additional predictor variables that are helpful in
predicting \(y_{i}\), but do not enter any of the indices are two-fold:
a covariate that relates to the response through a nonlinear function
\(f_{k}\), denoted by \(w_{ik}\), and the vector of linear covariates
denoted by \(\bm{u}_{i}\).

This is an extension of the GAIM that allows to impose constraints on
the index coefficients as well as on the nonlinear link functions. In
CGAIM, linear constraints of the form
\(\bm{C}_{j}\bm{\alpha}_{j} \ge 0\) can be imposed on the index
coefficients, where \(\bm{C}_{j} \in \mathbb{R}^{d_{j} \times l_{j}}\),
and \(d_{j}\) is the number of constraints. Moreover, shape constraints
such as monotonicity, convexity or concavity can be imposed on the
nonparametric functions. This modification allows to incorporate prior
knowledge or operational requirements into the model estimation.

First, considering only the additive index part of the model, and given
\((y_{i}, x_{i1}, \dots, x_{iq}), \quad i = 1, \dots, n\) be the
observed data, where the \(q\) predictors are grouped into \(p\) groups,
the estimation problem of the CGAIM can be formulated as
\begin{equation}\protect\hypertarget{eq-3}{}{
\begin{aligned}
  \min_{\bm{\alpha}, \beta_{0}} \quad & \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij})\right ]^{2}, \\
  \text{s.t.} \quad & \bm{C}\bm{\alpha} \ge 0, \quad g_{j} \in m,
\end{aligned}
}\label{eq-3}\end{equation} where
\(\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}\),
\(\beta_{0}\) is the model intercept,
\(\bm{C} \in \mathbb{R}^{d \times q}\), \(d\) is the number of
constraints on the index coefficients vector \(\bm{\alpha}\), and \(m\)
is a shape constraint imposed on \(g_{j}\) \autocite{Masselot2022}.

Notice that \(\bm{\alpha}_{j}\)s behave non-linearly in
Equation~\ref{eq-3}, and hence, this is a non-linear least squares
problem. Accordingly, \textcite{Masselot2022} introduced an efficient
iterative algorithm for estimating the CGAIM based on
\textbf{\emph{Sequential Quadratic Programming}} (SQP), one of the most
successful techniques for solving nonlinear constrained optimisation
problems \autocite{Boggs1995}. For details of the CGAIM algorithm refer
to \textcite{Masselot2022}.

\hypertarget{mathematical-optimisation-for-variable-selection}{%
\subsection{Mathematical Optimisation for Variable
Selection}\label{mathematical-optimisation-for-variable-selection}}

\hypertarget{mathematical-optimisation}{%
\subsubsection{Mathematical
Optimisation}\label{mathematical-optimisation}}

\emph{Optimisation} plays a major role in both decision science and
physical systems evaluation. \textbf{\emph{Mathematical Optimisation}}
or \textbf{\emph{Mathematical Programming}} can be defined as the
minimisation (or maximisation) of a function subject to restrictions on
the unknowns/parameters of that function \autocite{Nocedal2006}. Hence,
a mathematical optimisation problem can be written as
\begin{equation}\protect\hypertarget{eq-opt}{}{
\begin{aligned}
  \min_{\bm{x}} \quad & f_{0}(\bm{x})\\
  \text{s.t.} \quad & f_{i}(\bm{x}) \le b_{i}, \quad i = 1, \dots, m
\end{aligned}
}\label{eq-opt}\end{equation} where the vector of unknowns or parameters
of the problem is given by
\(\bm{x} = \left ( x_{1}, \dots, x_{n} \right )^{T}\), the
\emph{objective function} is denoted by
\(f_{0} : \mathbb{R}^{n} \rightarrow \mathbb{R}\), the \emph{constraint
functions} are given by
\(f_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}\), \(i = 1, \dots, m\),
and the bounds of the constraints are denoted by
\(\bm{b} = \left (b_{1}, \dots, b_{m} \right )^{T}\). A vector of values
\(\bm{x^{*}}\) that results in the smallest value for the objective
function among all vectors that satisfy the stated constraints, is
called the \emph{optimal} value or the \emph{solution} to the problem
\autocite{Boyd2004}. After mathematically formulating the optimisation
problem as above (Equation~\ref{eq-opt}), an appropriate
\emph{optimisation algorithm} is used to obtain the solution
\(\bm{x^{*}}\) \autocite{Nocedal2006}.

Based on the form of the objective function and the constraints, various
types of optimisation problems are identified.

An optimisation problem is known as a \textbf{\emph{Linear Program}}
(LP) when both the objective function and the constraints in
Equation~\ref{eq-opt} (i.e.~all \(f_{i}\), \(i = 0, \dots, m\)) are
linear. Hence, a LP can be written as
\begin{equation}\protect\hypertarget{eq-lp}{}{
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
}\label{eq-lp}\end{equation} where \(\bm{x}\) is the vector that
contains the parameters to be optimised, and
\(\bm{a_{0}} \in \mathbb{R}^{n}\) is the vector of coefficients of the
objective function. The matrix of coefficients in the constraints is
denoted by \(\bm{A} \in \mathbb{R}^{m \times n}\), and \(\bm{b}\) is the
vector containing the upper bounds of the constraints. All LPs are
\emph{convex} optimisation problems \autocite{Theusl2020}.

The LP problem given in Equation~\ref{eq-lp} can be generalised to
involve a quadratic term in the objective function, in which case it is
called a \textbf{\emph{Quadratic Program}} (QP). A QP can be written as
\[
\begin{aligned}
  \min_{\bm{x}} \quad & \frac{1}{2} \bm{x}^{T} \bm{Q_{0}} \bm{x} + \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
\] where \(\bm{Q_{0}} \in \mathbb{R}^{n \times n}\). Unless the matrix
\(\bm{Q_{0}}\) is positive semi-definite, a QP is non-convex
\autocite{Theusl2020}.

If a linear objective function is minimised over a \emph{convex cone},
such an optimisation problem is called a \textbf{\emph{Conic Program}}
(CP), which can be written as \[
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x} \\
  \text{s.t.} \quad & \bm{A}\bm{x} + s = \bm{b}, \quad s \in \mathcal{K},
\end{aligned}
\] where \(\mathcal{K}\) denotes a nonempty closed convex cone. CPs are
designed to model convex optimisation problems \autocite{Theusl2020}.

If we restrict some of the unknowns/parameters in an optimisation
problem to take only integer values, then that optimisation problem is
called a \textbf{\emph{Mixed Integer Program}} (MIP). For example, if we
constraint \(x_{k} \in \mathbb{Z}\) for at least one \(k\),
\(k \in \{1, \dots, n\}\) in the optimisation problem given by
Equation~\ref{eq-opt}, then the optimisation problem becomes a MIP. If
all the unknowns of an optimisation problem are constrained to be
integers, such a problem is referred to as a pure \textbf{\emph{Integer
Program}} (IP), whereas if all the unknowns are bounded between zero and
one (i.e.~\(\bm{x} \in \{ 0, 1 \}^{n}\)), the optimisation problem is
referred to as a \textbf{\emph{Binary (Integer) Program}}
\autocite{Theusl2020}. MIPs are hard to solve as they are non-convex due
to the integer constraints. However, a growth in the number of
commercial as well as non-commercial MIP solvers has made it possible to
solve MIP problems conveniently and directly.

\hypertarget{variable-selection}{%
\subsubsection{Variable Selection}\label{variable-selection}}

Mathematical optimisation is fundamentally important in statistics, as
many statistical problems including regression, classification, and
other types of estimation/approximation problems can be re-interpreted
as optimisation problems \autocite{Theusl2020}. Thus, the problem of
variable selection - one of the prolonged interests of statisticians,
has also benefited from using optimisation concepts, particularly MIP
and convex optimisation, in the recent past.

For example, \textcite{Bertsimas2016} used a mixed integer optimisation
procedure to solve the classical best subset selection problem in a
linear regression. They developed a discrete optimisation method by
extending modern first-order continuous optimisation techniques. The
method can produce near-optimal solutions that would serve as warm
starts for a MIP algorithm, which would choose the best \(k\) features
out of \(p\) predictors. Similarly, \textcite{Hazimeh2020} developed
fast and efficient algorithms based on coordinate descent and local
combinatorial optimisation to solve the same best subset selection (or
\(\ell_{0}\)-regularised least squares) problem through re-formulating
local combinatorial search problems as structured MIPs.

Furthermore, \textcite{Hazimeh2023} proposed a group-wise variable
selection methodology, based on discrete mathematical optimisation,
which is applicable to both \(\ell_{0}\)-regularised linear regression
and nonparametric additive models in a high-dimensional setting. They
formulated the group \(\ell_{0}\)-based estimation problem as a
\emph{Mixed Integer Second Order Cone Program (MISOCP)}, and proposed a
new customised Branch-and-Bound (BnB) algorithm
\autocite{Land1960,Little1963} to obtain the global optimal solution to
the MISOCP.

Through the study of above literature, we noticed that the mathematical
optimisation based algorithms reduce computational cost of variable
selection procedures in high-dimensional settings. This is largely due
to the availability of efficient commercial solvers such as
\emph{Gurobi} and \emph{CPLEX}. This motivated us to focus on a
mathematical optimisation based procedure for developing our variable
selection methodology.

\hypertarget{sec-SMI}{%
\section{Sparse Multiple Index Model}\label{sec-SMI}}

In this section, we develop a \textbf{\emph{Sparse Multiple Index
Model}} (hereafter referred to as SMI Model) to establish an objective
and a principled methodology for estimating high-dimensional
nonparametric additive index models, with optimal predictor selection.

\hypertarget{sec-model}{%
\subsection{Model}\label{sec-model}}

The model of interest is a semi-parametric additive index model, which
can be written as \begin{equation}\protect\hypertarget{eq-semipara}{}{
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
}\label{eq-semipara}\end{equation} where \(y_{i}\) is the univariate
response, \(\beta_{0}\) is the model intercept,
\(\bm{x}_{ij} \in \mathbb{R}^{l_{j}}\), \(j = 1, \dots, p\) are \(p\)
subsets of \(\bm{x}_{i}\) - the \(q\)-dimensional vector of all
predictors entering indices, \(\bm{\alpha}_{j}\) is the
\(l_{j}\)-dimensional vector of index coefficients corresponding to the
index \(h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}\), and \(g_{j}\) is the
corresponding nonlinear link function (possibly estimated by a spline).

Based on the above model formulation, we make three main assumptions as
follows to define the \textbf{SMI Model}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The number of indices (i.e.~the number of subsets of predictors) \(p\)
  is unknown, and will be estimated through the proposed algorithm;
\item
  The split of the predictors among indices is unknown, and will be
  determined by the proposed algorithm; and
\item
  A predictor variable (that is entering indices) can only enter one
  index (i.e.~overlapping of predictors among indices is not allowed).
\end{enumerate}

These assumptions further imply that the index coefficient vectors
\(\bm{\alpha}_{j}\)s and the corresponding nonlinear link functions
\(g_{j}\)s are also unknown, and will be estimated through the proposed
algorithm.

Most importantly, we allow the possibility for the index coefficient
vectors \(\bm{\alpha}_{j}\)s to have elements that are equal to zero, so
that the predictors corresponding to such zero coefficients are dropped
out from the model, achieving variable selection.

Moreover, it is important to note here that the possible number of
indices in a SMI model ranges from \(1\) (i.e.~all \(q\) predictors are
in a single index) to \(q\) (i.e.~each predictor is in a separate
index). In other words, both the Single Index Model and the Additive
Model are special cases of SMI Model.

In addition to the predictor variables that are entering indices, we
also allow for predictors that do not enter any of the indices. These
additional predictors are two-fold: a covariate that relates to the
response through nonlinear function \(f_{k}\) denoted by \(w_{ik}\),
\(k = 1, \dots, d\), and the vector of linear covariates denoted by
\(\bm{u}_{i}\).

\hypertarget{optimisation-problem-formulation}{%
\subsection{Optimisation Problem
Formulation}\label{optimisation-problem-formulation}}

Let \(q\) be the total number of predictors entering \(p\)
non-overlapping subsets of size \(l_{j}\), \(j = 1, \dots, p\)
(i.e.~\(\sum_{j = 1}^{p} l_{j} = q\)).The algorithm discussed in this
paper apply to the SMI Model estimator given below, where the squared
error of the model (Equation~\ref{eq-semipara}) is minimised together
with an \(\ell_{0}\) penalty term and an \(\ell_{2}\) (ridge) penalty
term: \begin{equation}\protect\hypertarget{eq-smi}{}{
\begin{aligned}
  \min_{p, \bm{\alpha}, \bm{g}, \beta_{0}} \quad \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) - \sum_{k = 1}^{d}f_{k}(w_{ik}) - \bm{\theta}^{T}\bm{u}_{i}\right]^{2} \\
  + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}\bm{1}\left(\alpha_{jm} \neq 0\right) + \lambda_{2}\sum_{j = 1}^{p}\|\bm{\alpha}_{j}\|_2^2
\end{aligned}
}\label{eq-smi}\end{equation} where
\(\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}\),
\(\bm{g} = \{g_{1}, g_{2}, \dots, g_{p}\}\), \(\bm{1}(\cdot)\) is the
indicator function, \(\lambda_{0} > 0\) is a tuning parameter that
controls the number of selected predictors, and \(\lambda_{2} \ge 0\) is
another tuning parameter that controls the strength of the additional
shrinkage imposed on the estimated index coefficients.

Applying an \(\ell_{2}\)-penalty in addition to the \(\ell_{0}\)-penalty
is motivated by related literature
\autocite{Hazimeh2020,Mazumder2022,Hazimeh2023}, where it is suggested
that the prediction performance of best-subset selection is enhanced by
the inclusion of an additional ridge penalty, especially when a low
signal-to-noise ratio (SNR) is present.

\hypertarget{miqp-formulation}{%
\subsection{MIQP Formulation}\label{miqp-formulation}}

To solve the optimisation problem (Equation~\ref{eq-smi}), we present a
big-M based Mixed Integer Quadratic Program (MIQP) formulation:
\begin{equation}\protect\hypertarget{eq-smi-mip}{}{
\begin{aligned}
  \min_{p, \bm{\alpha}, \bm{g}, \beta_{0}, \bm{z}} \quad & \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}{g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} - \sum_{k = 1}^{d} {f_{k}(w_{ik})} - \bm{\theta}^{T}\bm{u}_{i}\right ]^{2} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q} \alpha_{jm}^{2} \\
  \text{s.t.} \quad & \left |\alpha_{jm}\right | \le Mz_{jm} \quad \forall j, \forall m, \\
  & \sum_{j = 1}^{p}z_{jm} \le 1 \quad \forall m, \\
  & z_{jm} \in \{0, 1\}, \\
  & j = 1, \dots, p, \quad m = 1, \dots, q,
\end{aligned}
}\label{eq-smi-mip}\end{equation} where \(p\) is the (unknown) number of
indices, \(\bm{x}_{i}\) is the \(q\)-dimensional vector of all
predictors entering indices,
\(\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}\),
\(\bm{g} = \{g_{1}, g_{2}, \dots, g_{p}\}\), and
\(\bm{z} = \left (\bm{z}_{1}^{T}, \dots, \bm{z}_{p}^{T} \right )^{T}\),
\(\bm{z}_{j} = \left (z_{j1}, \dots, z_{jq} \right )^{T}\),
\(j = 1, \dots, p\) such that \(z_{jm} \in \{0, 1\}, m = 1, \dots, q\)
for all \(j\). In other words, we introduce a binary (i.e.~indicator)
variable corresponding to each predictor in each index. A pre-specified
\emph{big-M parameter} is denoted by \(M < \infty\), and it should be
sufficiently large. If \(\bm{\alpha^{*}}\) is an optimal solution to the
problem given in Equation~\ref{eq-smi-mip}, then the big-M parameter
should satisfy
\(max \left (\left |\alpha_{jm}^{*}\right | \right ) \le M\), where
\(j \in \{1, \dots, p\}\), and \(m \in \{1, \dots, q\}\).

Notice that, here we formulate the MIQP to include all \(q\) predictors
in each index so that in this case, \(\bm{\alpha}_{j}\) is a
\(q\)-dimensional vector of index coefficients. However at the same
time, as mentioned earlier, we introduce a set of binary variables
corresponding to each predictor in each index, which serves two main
purposes: firstly, these binary variables are used to make the
\emph{``on-or-off''} decisions of the predictors in the model; secondly,
they contributes to decide which predictors belong to which index.

To further elaborate, first, the big-M constraints ensure that
\(\alpha_{jm}\) is zero if and only if \(z_{jm}\) is zero, and if
\(z_{jm} = 1\), then \(\left |\alpha_{jm}\right | \le M\). At the same
time, the \(\ell_{0}\)-penalty term
\(\lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm}\) influences some of
the binary variables \(z_{jm}\) to be zero, while the
\(\ell_{2}\)-penalty term
\(\lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q}\alpha_{jm}^{2}\) enforces
additional shrinkage on the estimated coefficients. Therefore, these
components together perform a variable selection.

Next, when the set of binary variables
\(\bm{Z}_{m} = \{z_{1m}, z_{2m}, \dots, z_{pm}\}\) corresponding to the
\(m^{th}\), \(m = 1, \dots, q\), predictor in all \(p\) indices is
considered, according to the constraint
\(\sum_{j = 1}^{p}z_{jm} \le 1\), only one or no binary variables in the
set can take the value one, ensuring that the \(m^{th}\) predictor does
not repeat in more than one index. In other words, if the \(j^{th}\)
element of \(\bm{Z}_{m}\), \(z_{jm}\), takes the value one, none of the
other elements in the set \(\bm{Z}_{m}\) can take the value one,
indicating that the \(m^{th}\) predictor enter into the \(j^{th}\) index
in the model. On the other hand, if all the elements of the set
\(\bm{Z}_{m}\) are zero, then the \(m^{th}\) predictor will be dropped
out from the model.

Thus, our main contribution in this paper is two-fold. Firstly, we
propose a novel algorithm to objectively estimate a semi-parametric
additive index model, while contributing towards an estimated model with
a higher forecasting accuracy. Secondly, the proposed variable selection
methodology will contribute towards estimating a parsimonious model in a
high-dimensional setting, even if the required domain knowledge for
selecting the best set of predictors is unavailable.

\hypertarget{estimation-algorithm}{%
\subsection{Estimation Algorithm}\label{estimation-algorithm}}

In this section, we show how to efficiently find a minimiser for the
problem given in Equation~\ref{eq-smi-mip}.

Since the number of indices \(p\), the vector of index coefficients
\(\bm{\alpha}\), as well as the set of nonparametric functions
\(\bm{g}\) are unknown, it is mathematically impossible to solve the
above MIQP given in Equation~\ref{eq-smi-mip} directly. Hence, we
propose an iterative algorithm to solve the problem.

\hypertarget{sec-step1}{%
\subsubsection{Initialising the Index Structure and Index
Coefficients}\label{sec-step1}}

Since the number of indices, the split of the predictors among indices
as well as the index coefficients are not pre-specified, first, we need
to provide an initialisation for the index structure (i.e.~number of
indices (\(p\)) and the split of predictors among indices) and the index
coefficients (\(\bm{\alpha}\)) of the model for start solving the MIQP
given in Equation~\ref{eq-smi-mip}.

Based on our experiments on the new algorithm, we propose five
alternative methods for initialising the SMI Model as follows.

\textbf{1. ``PPR'' - Projection Pursuit Regression Based
Initialisation:}

As discussed in Section~\ref{sec-Index}, Projection Pursuit Regression
model is a multiple index model, where each index consists of all the
available predictors. Since in SMI Model we assume that there are no
overlapping indices, it is impossible to use an estimated PPR model
directly as a starting model for the algorithm. Thus, we follow the
steps presented below to come up with a feasible initialisation for the
index structure and the coefficients.

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  Scale all the variables of the data set by dividing each variable by
  its standard deviation (so that it is possible to compare the
  estimated coefficients among predictors).
\item
  Fit a PPR model and obtain estimated index coefficients. (The user can
  decide the number of indices to be estimated (\emph{num\_ind}); we use
  \emph{num\_ind = 5} as the default value.)
\item
  Calculate a threshold as \[
    threshold = \max(PPR \quad coefficients) * 0.1
  \]
\item
  Assign zero for all the coefficients that are lower than the
  calculated threshold.
\item
  If any predictor appears in more than one index, assign that predictor
  to the index in which that particular predictor has the maximum
  coefficient (among the coefficients corresponding to that predictor in
  all the indices), and make the coefficients corresponding to that
  predictor to be zero in all the other indices.
\item
  After performing the above steps i-v, if any of the originally
  estimated indices have all zero coefficients, such an index will be
  dropped out of the model.
\end{enumerate}

Now, the index structure and the index coefficients obtained through the
above steps are considered to a feasible initialisation for the SMI
Model algorithm.

Furthermore, it is important to state here that once the optimal SMI
Model is obtained through the proposed algorithm, each index coefficient
will be back-transformed into the original scale of the corresponding
predictor variable to roll back the effect of scaling at the beginning.

\textbf{2. ``Additive'' - Nonparametric Additive Model Based
Initialisation:}

As mentioned previously in Section~\ref{sec-model}, the nonparametric
additive model is a special case of SMI Model, where the number of
indices equals the number of predictors entering indices (\(p = q\))
(i.e.~number of predictors in each index = 1). Hence, it is a feasible
starting point for the SMI Model algorithm. (Notice that in this case,
we consider the index coefficient corresponding to each predictor to be
1.)

\textbf{3. ``Linear'' - Linear Regression Based Initialisation:}

In this option, we first regress the response variable on the predictors
using a multiple linear regression. Then, we construct a single index
(i.e.~\(p = 1\)) using the estimated regression coefficients as the
index coefficients of the predictors. Since Single Index Model is also a
special case of the SMI Model, this will be a feasible starting point.

\textbf{4. ``Multiple'' - Selecting an Initial Model by Comparing
Multiple Models:}

Through our experiments on the new algorithm, we identified that in some
situations, the final optimised SMI Model often changes based on the
initialisation that we provide to the algorithm. Hence, through this
initialisation option, we consider a number of different models as
initial models, optimise the SMI Model for each of those initial models,
and pick the initial model that results in the SMI Model with the lowest
loss for the MIP problem.

Here, the user can decide on the number of models to be considered
(\emph{num\_models}) as well as the the number of indices to be
considered in all the models (\emph{num\_ind} - same for all
\emph{num\_models}). We use \emph{num\_models = 5} and \emph{num\_ind =
5} as default values.

\textbf{5. ``User Input'' - User Specified Initialisation:}

As mentioned earlier, since the number of indices, the split of the
predictors among indices as well as the index coefficients are unknown,
and will be optimally estimated by the proposed algorithm,
theoretically, it is possible to start the algorithm at any given random
initialisation of the model. Hence, this final option allow the user to
provide their desired initialisation for the algorithm by specifying the
number of indices, the split of predictors among the indices and the
initial index coefficients.

In other words, this option provides the freedom for the user to utilise
their domain expertise or prior knowledge in initialising the algorithm.

In all of the above initialisation options, once the estimate for
\(\bm{\alpha}\) is obtained, the estimated initial index coefficient
vector of each index \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j, init}\)
is scaled to have unit norm to ensure identifiability.

The characteristics and the performance of the proposed algorithm based
on each of these initialisation options, vary depending on the nature of
the application. Some initial experimental insights regarding the same
will be discussed in more detail in Section~\ref{sec-simulation}.

\hypertarget{sec-step2}{%
\subsubsection{Estimating Nonlinear Functions}\label{sec-step2}}

Once we have an estimate for \(\bm{\alpha}\), estimating the SMI Model
is equivalent to estimating a GAM as \[
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\hat{h}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(y_{i}\) is taken as the response, and the estimated indices
\(\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{i}\), and the
additional covariates that are not entering any index are taken as
predictors.

The R packages \textbf{\emph{mgcv}} \autocite{Wood2011}, and
\textbf{\emph{gam}} \autocite{Hastie2023}, for example, can be used to
fit GAMs.

\hypertarget{sec-step3}{%
\subsubsection{Updating the Index Structure and Index
Coefficients}\label{sec-step3}}

\hypertarget{sec-step3_1}{%
\paragraph{Updating Index Coefficients}\label{sec-step3_1}}

We estimate the new value of index coefficients \(\bm{\alpha}_{new}\)
through a MIQP: \begin{equation}\protect\hypertarget{eq-smi-update}{}{
\begin{aligned}
  \min_{\bm{\alpha}_{new}, \bm{z}_{new}} \quad & (\bm{\alpha}_{new} - \bm{\alpha}_{old})^{T}\bm{V}^{T}\bm{V}(\bm{\alpha}_{new} - \bm{\alpha}_{old}) - 2(\bm{\alpha}_{new} - \bm{\alpha}_{old})^{T}\bm{V}^{T}\bm{r} \\
  & + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm(new)} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q}\alpha_{jm(new)}^{2} \\
  \text{s.t.} \quad & \left |\alpha_{jm(new)}\right | \le Mz_{jm(new)} \quad \forall j, \forall m,\\
  & z_{jm(new)} \in \{0, 1\}, \\
  & \sum_{j = 1}^{p}z_{jm(new)} \le 1 \quad \forall m, \\
  & j = 1, \dots, p, \quad m = 1, \dots, q,
\end{aligned}
}\label{eq-smi-update}\end{equation} where \(\bm{\alpha}_{old}\) is the
current value of \(\bm{\alpha}\), and \(z_{jm(new)}\) are the new set of
binary variables to be estimated. \(\bm{V}\) is the matrix of partial
derivatives of the right hand side of Equation~\ref{eq-semipara}, with
respect to \(\bm{\alpha}_{j}\). The \(i^{th}\) line of \(\bm{V}\)
contains \(\left [ \bm{v}_{i1}, \dots, \bm{v}_{ip} \right ]\), where
\(\bm{v}_{ij} = \bm{x}_{i}g_{j}^{'}(h_{ij})\). The current residual
vector, which contains
\(r_{i} = y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j(old)}^{T}\bm{x}_{i})\),
is denoted by \(\bm{r}\).

Similar to the explanation given by \textcite{Masselot2022}, the MIQP
objective function in above Equation~\ref{eq-smi-update} ignores the
\emph{Hessian} (or the matrix of second derivatives of
Equation~\ref{eq-semipara}, with respect to \(\bm{\alpha}_{j}\)), and
considers only the matrix of first derivatives, which is a
\emph{quasi-Newton} step. The \emph{Quasi-Newton Method} is an
alternative to the \emph{Newton's Method} that avoids the calculation of
the Hessian to circumvent its computational burden \autocite{Peng2022}.
Therefore, the \(\bm{\alpha}\) updating step given in above
Equation~\ref{eq-smi-update} is assured to be in a \emph{descent
direction}.

It is important to note that the additional covariates \(w_{ik}\) and
\(\bm{u}_{i}\) do not step in to the process of updating
\(\bm{\alpha}_{j}\), because they are constants with respect to
\(\bm{\alpha}_{j}\), and thus they disappear from \(\bm{V}\).

Moreover, when \(\bm{\alpha}_{new}\) is obtained, if any of the
estimated individual index coefficient vectors \(\bm{\alpha}_{j(new)}\)
contains all zeros (i.e.~zero index), such indices will be dropped out
from the model.

Furthermore, similar to Section~\ref{sec-step1}, once the new estimate
\(\bm{\alpha}_{new}\) is obtained, we scale each estimated index
coefficient vector \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j(new)}\) to
have unit norm.

\hypertarget{sec-step3_2}{%
\paragraph{Estimating Nonlinear Functions}\label{sec-step3_2}}

Similar to Section~\ref{sec-step2}, once we obtain the new estimate
\(\bm{\alpha}_{new}\) for \(\bm{\alpha}\), we estimate a GAM taking
\(y_{i}\) as the response, and the new estimated indices
\(\hat{h}_{ij(new)} = \hat{\bm{\alpha}}_{j(new)}^{T}\bm{x}_{i}\), and
the additional covariates that are not entering any index as predictors.

\hypertarget{sec-step3_3}{%
\paragraph{Iterating}\label{sec-step3_3}}

We iterate the above steps described in Section~\ref{sec-step3_1} and
Section~\ref{sec-step3_2} until the reduction ratio of the objective
(loss) function value of the MIQP given in Equation~\ref{eq-smi-mip},
calculated between two successive iterations, reaches a pre-specified
convergence tolerance, or the loss increases for three consecutive
iterations. Alternatively, it is also possible to stop the iterations
when a pre-specified maximum number of iterations is reached.

Here, to obtain an estimated model with the best possible forecasting
accuracy, it is important to select appropriate values for the
non-negative penalty parameters \(\lambda_{0}\) and \(\lambda_{2}\). One
possible way to do this is to estimate the model over a grid of possible
values for \(\lambda_{0}\) and \(\lambda_{2}\), and then select the
combination that yields the lowest loss function value.

Moreover, it is also crucial to choose a suitable value for the big-M
parameter, as the strength of the MIP formulation depends on the choice
of a good lower bound \autocite{Bertsimas2016}. According to
\textcite{Hazimeh2023}, several methods have been used to select \(M\)
in practice. For a description on estimating \(M\) in a linear
regression setting, refer to \textcite{Bertsimas2016}.

Additionally, the choice of convergence tolerance and the maximum number
of iterations will depend on the nature of the problem/data to which the
algorithm is applied. In the empirical applications presented in
Section~\ref{sec-application}, we have used a convergence tolerance of
\(0.001\), and \(50\) maximum iterations, where the iterations are
terminated on whichever is reached first.

\hypertarget{sec-step3_4}{%
\paragraph{Updating Index Structure}\label{sec-step3_4}}

Once the initially chosen model is optimised through above step in
Section~\ref{sec-step3_3}, next, we consider changing the index
structure of the model to exploit any benefits in terms of further
minimising the loss function of the MIQP given in
Equation~\ref{eq-smi-mip}.

Since reducing number of indices of the model automatically happens
through dropping indices corresponding to any zero indices at each
optimisation iteration, in this step, we consider adding indices into
the current model.

Thus, once we obtain the optimal model at the step in
Section~\ref{sec-step3_3} (say the number of indices in the model =
\(p\)), we check for any predictors that are dropped out from the model.
If any such predictors are available, we construct a new index
comprising of those dropped out predictors and add that to the current
model. Then the step in Section~\ref{sec-step3_3} is performed again for
the new model with, say \(p+1\) indices.

If no predictors were dropped out from the first model (obtained from
Section~\ref{sec-step3_3}), then the algorithm terminates at this point,
and the optimal model obtained from Section~\ref{sec-step3_3} will be
the final SMI Model.

\hypertarget{sec-step3_5}{%
\paragraph{Iterating}\label{sec-step3_5}}

We iteratively perform the step in Section~\ref{sec-step3_4}, increasing
the number of indices of the existing model by one in each iteration. At
the end of each iteration we check for the following termination
criteria:

\begin{itemize}
\item
  \emph{If the number of indices in the current model \textbf{equals}
  the number of indices in the new model:}

  \begin{itemize}
  \item
    \emph{If the absolute difference between the index coefficients in
    the current model and the new model is \textbf{equal} up to a
    pre-specified tolerance:}

    \begin{itemize}
    \item
      \emph{If the loss of the new model is \textbf{greater} than the
      current model:} Terminate the algorithm, and the current model is
      the final SMI Model;

      Else
    \item
      \emph{If the loss of the new model is \textbf{lower} than the
      current model:} Terminate the algorithm, and the new model is the
      final SMI Model;
    \end{itemize}

    Else
  \item
    \emph{If the absolute difference between the index coefficients in
    the current model and the new model is \textbf{not equal} up to a
    pre-specified tolerance:}

    \begin{itemize}
    \tightlist
    \item
      \emph{If the loss of the new model is \textbf{greater} than the
      current model:} Terminate the algorithm, and the current model is
      the final SMI Model;
    \end{itemize}
  \end{itemize}

  Else
\item
  \emph{If the number of indices in the current model \textbf{does not
  equal} the number of indices in the new model:}

  \begin{itemize}
  \tightlist
  \item
    \emph{If the loss of the new model is \textbf{greater} than the
    current model:} Terminate the algorithm, and the current model is
    the final SMI Model;
  \end{itemize}

  Else
\item
  \emph{If the number of indices of the current model equals the number
  of predictors (entering indices) (i.e.~maximum possible number of
  indices id reached):} Terminate the algorithm, and the current model
  is the final SMI Model.
\end{itemize}

The following \textbf{\emph{Algorithm 1}} summarises the key steps of
the SMI Model algorithm. \newline

\textbf{\emph{Algorithm 1: SMI Model Algorithm}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialise index structure and \(\bm{\alpha}\):\\
  Initialise \(p\), split of the predictors among indices, and obtain
  \(\bm{\alpha}_{init}\) using one of the five methods in
  Section~\ref{sec-step1}, and scale each
  \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j, init}\) to have unit norm
\item
  Estimate \(g_{j}\)s:\\
  Estimate \(g_{j}\)s using a GAM taking \(y_{i}\) as the response,
  \(\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{i}\)s as predictors
\item
  Update index structure and \(\bm{\alpha}\):

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Estimate the new value \(\bm{\alpha}_{new}\) through the MIQP in
    Equation~\ref{eq-smi-update}, and scale each
    \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j(new)}\) to have unit norm.
  \item
    Estimate new \(g_{j}\)s as in step 2.
  \item
    Iterate above steps i and ii until a convergence tolerance is
    reached OR loss increases for three consecutive iterations OR
    maximum number of iterations is reached.
  \item
    Update index structure:

    \begin{enumerate}
    \def\labelenumiii{\alph{enumiii}.}
    \tightlist
    \item
      If any predictors are dropped out from the model in above step
      iii, add a new index consisting of the dropped out predictors, and
      optimise the model through step iii.
    \item
      If no predictors are dropped out from the model in above step iii,
      terminate the algorithm.
    \end{enumerate}
  \item
    Iterate above step iv, while increasing \(p\) by 1 in each
    iteration, until one of the following terminations criteria is
    reached.

    \begin{itemize}
    \item
      \textbf{If} (No.of indices in current model \(=\) No.of indices in
      new model):

      \begin{itemize}
      \item
        \textbf{If} (abs(current index coefficients \(-\) new index
        coefficients) \(\le\) tolerance):

        \begin{itemize}
        \item
          \textbf{If} (loss of new model \(\ge\) loss of current model):
          \textbf{Terminate} - current model is the final SMI Model;

          \textbf{Else}
        \item
          \textbf{If} (loss of new model \(<\) loss of current model):
          \textbf{Terminate} - new model is the final SMI Model;
        \end{itemize}

        \textbf{Else}
      \item
        \textbf{If} (abs(current index coefficients \(-\) new index
        coefficients) \(>\) tolerance):

        \begin{itemize}
        \tightlist
        \item
          \textbf{If} (loss of new model \(\ge\) loss of current model):
          \textbf{Terminate} - current model is the final SMI Model;
        \end{itemize}
      \end{itemize}

      \textbf{Else}
    \item
      \textbf{If} (No.of indices in current model \(\neq\) No.of indices
      in new model):

      \begin{itemize}
      \tightlist
      \item
        \textbf{If} (loss of new model \(\ge\) loss of current model):
        \textbf{Terminate} - current model is the final SMI Model;
      \end{itemize}

      \textbf{Else}
    \item
      \textbf{If} (No.~of indices in current model \(= q\)):
      \textbf{Terminate} - current model is the final SMI Model.
    \end{itemize}
  \end{enumerate}
\end{enumerate}

\hypertarget{sec-simulation}{%
\section{Simulation Experiment}\label{sec-simulation}}

This section presents the results of a simple simulation experiment
conducted to demonstrate the performance and characteristics of the
proposed SMI Model algorithm. Particularly, we try to investigate how
the estimated SMI Model varies depending on the initialisation (as
discussed in Section~\ref{sec-step1}) used.

\hypertarget{sec-datagen}{%
\subsection{Data Generation}\label{sec-datagen}}

\textbf{Generating predictor variables:}

First, we generate two series each of length 1205: \(x\), from random
uniform distribution with the support \([0, 1]\), and \(z\), from random
normal distribution with mean \(= 5\), and standard deviation \(= 2\).
Next, we construct lagged series up to \(5^{th}\) lag of both \(x\) and
\(z\). These current and lagged series of \(x\) and \(z\)
(i.e.~\(x = x_{0}, x_{1}, \dots, x_{5}, z = z_{0}, z_{1}, \dots, z_{5}\))
were taken as predictors in the simulation experiment.

\textbf{Generating response variables:}

We generated two response variables \(y_{1}\) and \(y_{2}\), with two
different index structures: ``single-index'' and ``2-index'', and added
a random normal noise component of two different strengths as follows:

\begin{itemize}
\item
  Low noise level - \(N(\mu = 0, \sigma = 0.1)\):\\
  \(y_{1} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + N(\mu = 0, \sigma = 0.1)\)\\
  \(y_{2} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + (0.35*x_{2} + 0.7*x_{5})^2 + N(\mu = 0, \sigma = 0.1)\)
\item
  High noise level - \(N(\mu = 0, \sigma = 0.5)\):\\
  \(y_{1} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + N(\mu = 0, \sigma = 0.5)\)\\
  \(y_{2} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + (0.35*x_{2} + 0.7*x_{5})^2 + N(\mu = 0, \sigma = 0.5)\)
\end{itemize}

Hence, notice that the response \(y_{1}\) is constructed using a single
index consisting of the predictor variables \(x_{0}, x_{1},\) and
\(x_{3}\), whereas the other response \(y_{2}\) is constructed using two
indices, where the first index consists of the predictors
\(x_{0}, x_{1},\) and \(x_{3}\), and the second index consists of
\(x_{2}\) and \(x_{5}\). The variable \(x_{4}\) or none of the \(z\)
variables were used in generating \(y_{1}\) and \(y_{2}\).

Once the data set is constructed, the first five observations are
discarded due to the missing values introduced by the lagged variables,
which leaves a data set of 1200 observations. We use the first 1000
observations as the training set, whereas the remaining 200 observations
are kept a side as the test set for evaluating the forecast error of the
estimated models.

\hypertarget{sec-exp}{%
\subsection{Experiment setup}\label{sec-exp}}

In this section, we explain the set up of the simulation experiment
conducted.

We estimated SMI Models through the proposed algorithm for each of the
two response variables (the two ``true models''), using three different
sets of predictors as inputs. Our aim was to examine whether the
algorithm correctly picks the relevant predictor variables (and drops
the irrelevant predictors), and estimate the correct index structure of
the true model.

The three different sets of predictors considered are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  All ``\(x\)'' variables (i.e.~only the ``relevant'' predictors as the
  responses, \(y_{1}\) and \(y_{2}\), were constructed using only
  ``\(x\)'' variables);
\item
  All ``\(x\)'' variables and all ``\(z\)'' variables (i.e.~``relevant''
  predictors plus ``irrelevant'' predictors); and
\item
  A part of ``\(x\)'' variables (\(x_{0}, x_{1}\) and \(x_{2}\)) and all
  ``\(z\)'' variables (i.e.~some ``relevent'' predictors plus
  ``irrelevant'' predictors).
\end{enumerate}

We applied the proposed SMI Model algorithm using each of the above
predictor combinations, for each of the two variations of the responses
in terms of the noise level. Moreover, we also considered each of the
first four initialisation options that we discussed in
Section~\ref{sec-step1}, for each of the two responses. For ``PPR'' and
``Multiple'' options, we used the default values of the required
parameters as stated in Section~\ref{sec-step1}. (Moreover, we used
\(M = 10\), a convergence tolerance of \(0.001\) and \(50\) maximum
iterations in step 3.3 (Section~\ref{sec-step3_3}), and a convergence
tolerance for coefficients of \(0.001\) in step 3.5
(Section~\ref{sec-step3_5}) in estimating all SMI Models.)

\hypertarget{sec-sim-results}{%
\subsection{Results}\label{sec-sim-results}}

We summarise the results of the simulation experiment in
Table~\ref{tbl-simulation}. Here, under the four columns corresponding
to each of the four initialisation options considered, we indicate the
index structure (i.e.~the number of indices and the split of the
variables among indices) estimated by the proposed algorithm, for each
of the different combinations investigated based on response, input
predictors, and noise levels.

\hypertarget{tbl-simulation}{}
\begin{table}[!h]
\caption{\label{tbl-simulation}Simulation experiment results }\tabularnewline

\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}{llllll}
\toprule
\textbf{True Model} & \textbf{Predictors} & \textbf{PPR} & \textbf{Additive} & \textbf{Linear} & \textbf{Multiple}\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Low noise level}}\\
 &  & 1 index & 1 index & 1 index & 1 index\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all x} & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$)\\
\cmidrule{2-6}
 &  & 1 index & 1 index & 1 index & 1 index\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all x + all z} & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$)\\
\cmidrule{2-6}
 &  & 1 index & 3 indices & 1 index & 1 index\\
\cmidrule{3-6}
\multirow{-6}{*}[1\dimexpr\aboverulesep+\belowrulesep+\cmidrulewidth]{\raggedright\arraybackslash \hspace{1em}$y_{1}$} & \multirow{-2}{*}{\raggedright\arraybackslash some x + all z} & ($x_{0}, x_{1}, z_{2}, z_{4}$) & ($x_{0}, x_{1}$) ($z_{4}$) ($z_{1}$) & ($x_{0}, x_{1}, z_{2}, z_{4}$) & ($x_{0}, x_{1}, z_{2}, z_{4}$)\\
\cmidrule{1-6}
 &  & 2 indices & 2 indices & 1 index & 2 indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all x} & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$)\\
\cmidrule{2-6}
 &  & 2 indices & 2 indices & 1 index & 2 \vphantom{1} indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all x + all z} & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$)\\
\cmidrule{2-6}
 &  & 3 indices & 2 indices & 1 index & 2 \vphantom{1} indices\\
\cmidrule{3-6}
\multirow{-6}{*}[1\dimexpr\aboverulesep+\belowrulesep+\cmidrulewidth]{\raggedright\arraybackslash \hspace{1em}$y_{2}$} & \multirow{-2}{*}{\raggedright\arraybackslash some x + all z} & ($x_{0}, x_{1}, z_{2}$) ($x_{2}$) ($z_{3}, z_{4}$) & ($x_{0}, x_{1}, z_{4}$) ($x_{2}$) & ($x_{0}, x_{1}, x_{2}, z_{2}$) & ($x_{0}, x_{1}$) ($x_{2}, z_{2}, z_{3}$)\\
\cmidrule{1-6}
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{High noise level}}\\
 &  & 1 index & 2 indices & 1 index & 1 index\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all x} & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}$) ($x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$)\\
\cmidrule{2-6}
 &  & 1 index & 2 indices & 1 index & 2 indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all x + all z} & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) ($z_{0}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) ($z_{0}$)\\
\cmidrule{2-6}
 &  & 3 indices & 3 indices & 1 index & 3 indices\\
\cmidrule{3-6}
\multirow{-6}{*}[1\dimexpr\aboverulesep+\belowrulesep+\cmidrulewidth]{\raggedright\arraybackslash \hspace{1em}$y_{1}$} & \multirow{-2}{*}{\raggedright\arraybackslash some x + all z} & ($x_{0}, x_{1}$) ($z_{1}$) ($z_{4}$) & ($x_{0}, x_{1}$) ($z_{1}$) ($z_{4}$) & ($x_{0}, x_{1}, z_{2}, z_{4}$) & ($x_{0}, x_{1}$) ($z_{0}, z_{4}$) ($z_{1}$)\\
\cmidrule{1-6}
 &  & 3 indices & 3 indices & 2 indices & 3 indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all x} & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) ($x_{4}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) ($x_{4}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}$) ($x_{4}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) ($x_{4}$)\\
\cmidrule{2-6}
 &  & 2 indices & 2 indices & 1 index & 2 indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all x + all z} & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}, z_{1}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}, z_{1}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}, z_{0}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$)\\
\cmidrule{2-6}
 &  & 3 indices & 2 indices & 1 index & 2 indices\\
\cmidrule{3-6}
\multirow{-6}{*}[1\dimexpr\aboverulesep+\belowrulesep+\cmidrulewidth]{\raggedright\arraybackslash \hspace{1em}$y_{2}$} & \multirow{-2}{*}{\raggedright\arraybackslash some x + all z} & ($x_{0}, x_{1}, z_{0}, z_{3}$) ($x_{2}$) ($z_{1}, z_{4}, z_{5}$) & ($x_{0}, x_{1}, z_{0}, z_{1}, z_{3}, z_{4}$) ($x_{2}$) & ($x_{0}, x_{1}, x_{2}, z_{0}, z_{3}, z_{4}$) & ($x_{0}, x_{1}, z_{0}, z_{1}, z_{3}, z_{4}$) ($x_{2}$)\\
\bottomrule
\end{tabular}}
\end{table}

Firstly, when the low noise level is considered, for the first true
model \(y_{1}\), when either ``all x'' or ``all x + all z'' predictors
are provided, all four initialisation options estimate the correct index
structure of the model, while selecting the correct set of predictors
and dropping out the irrelevant predictors. When only a part of the
``\(x\)'' variables are provided, the models estimated using all four
initialisations have selected some noise variables. This indicates that
when the available predictors are not sufficient to capture the signal
of the data, the algorithm might select irrelevant variables to make up
for the missing signal.

When \(y_{2}\) with low noise level is considered, in both ``all x'' and
``all x + all z'' cases, except for the ``Linear'' initialisation, the
algorithm has estimated the correct index structure with all the other
three initialisation options. The ``Linear'' option has selected the
correct variables into the model, however has not identified the 2-index
structure. This suggests that it might be better to initialise the
algorithm with a higher number of indices rather than with a lower
number of indices. When only a part of ``\(x\)'' variables are provided,
similar to the case of \(y_{1}\), models estimated for \(y_{2}\) with
all four initialisations, have selected noise variables.

Furthermore, we evaluated the forecasting error of the fitted models on
the test set using Mean Squared Error (MSE). For \(y_{1}\), in both the
cases ``all x'' and ``all x + all z'', the models estimated using all
four initialisations resulted in a test MSE of \(\approx 0.01\), which
is the random squared error of the true model. This confirms the
accuracy with which the SMI Model algorithm estimated the index
structure for \(y_{1}\). When only a part of ``\(x\)'' variables are
provided, the test MSE increased to \(\approx 0.23\), as the estimated
models contain noise variables.

For \(y_{2}\), in both the cases ``all x'' and ``all x + all z'', all
the models estimated resulted in a test MSE of \(\approx 0.16\). This is
an interesting result as the test MSEs of the estimated models with
correct index structure: ``PPR'', ``Additive'', and ``Multiple'', are
not very different to the test MSE of an estimated model with incorrect
index structure, but with correct predictors: ``Linear''. This suggests
that the selection of the predictor variables is more important than
determining the index structure of the model.

It is also worth mentioning here that in real-world forecasting
problems, the true data generating process (DGP) is unknown, and we do
not expect an estimated model to represent the true DGP. Therefore, as
long as the estimated model exhibits good forecasting accuracy, the
index structure of the estimated model is less important.

Similar to the case of \(y_{1}\), when only a part of ``\(x\)''
variables are provided, the test MSE of the estimated models for
\(y_{2}\) increased to \(\approx 0.34\), probably due to the inclusion
of noise variables.

Moreover, when the above test MSE values of the models estimated for
\(y_{2}\) are considered, in contrast to the case of \(y_{1}\), those
values are higher than the random squared error of the true model.
Intuitively, this might be due to the increased complexity of the model
\(y_{2}\) in comparison to \(y_{1}\), where the total estimation error
of two nonlinear link functions (corresponding to the two indices) for
\(y_{2}\), might be higher than the error of estimating a single
nonlinear function for \(y_{1}\).

As expected, the accuracy with which the SMI Model algorithm estimates
the index structure is in general lower with the high noise level, in
comparison to the low noise level. In the case of \(y_{1}\), when ``all
x'' variables are provided, except for ``Additive'' option, all the
other initialisations have correctly estimated the model. When ``all x +
all z'' variables are provided, only the ``PPR'' and ``Linear'' options
have estimated the correct model, whereas ``Additive'' and ``Multiple''
options have included a noise variable. Similar to the low noise level
case, when only a part of ``\(x\)'' variables are provided, all four
initialisations have led the algorithm to estimate models with noise
variables.

Next, when \(y_{2}\) is considered at high noise level, when ``all x''
variables are provided, the estimated models using all four
initialisation options have included the variable \(x_{4}\) (as an
additional index), which should have been omitted. When ``all x + all
z'' variables are provided, the option ``Multiple'' has led the
algorithm to the correct model, while all the other three options have
included a noise variable. Similar to the previous cases, when only a
part of ``\(x\)'' variables are provided, the models estimated using all
four initialisation options have included multiple noise variables,
probably to cover up for the missing signal variables.

In both the cases ``all x'' and ``all x + all z'', all the models
estimated for \(y_{1}\), except for ``Additive'' option in ``all x''
case, resulted in a test MSE of \(\approx 0.23\) (which is slightly
lower than the random squared error of the true model; this probably
indicates a slight level of over-fitting), irrespective of the fact that
in ``all x + all z'' case, ``Additive'' and ``Multiple'' options
included a noise variable. This is an indication of the effect of the
low signal-to-noise ratio in the data. The model estimated using
``Additive'' option in ``all x'' case has resulted in a slightly higher
test MSE (\(\approx 0.27\)), probably due to the incorrect index
structure. When only a part of ``\(x\)'' variables are provided, the
test MSE has increased (\(\approx 0.47\)), which is expected.

Similar to previously discussed low noise level case, the estimated
models for \(y_{2}\) have resulted in higher test MSE values in
comparison to the models for \(y_{1}\). When ``all x'' and ``all x + all
z'' variables are provided, all the estimated models resulted in test
MSE values in the range \(\approx (0.35, 0.37)\), irrespective of the
different index structure and predictor choices, which is again an
indication of the low signal-to-noise ratio in the data. Due to missing
relevant predictors (and the inclusion of noise predictors in the
model), the estimated models in ``some x + all z'' case resulted in test
MSEs \(\approx 0.57\).

In the above simulation experiment, we did not perform any tuning for
the penalty parameters \(\lambda_{0}\) and \(\lambda_{2}\), as through
our experiments we observed that for this simple example, different
values of penalty parameters have a negligible impact on the estimated
models. We used the default values \(\lambda_{0} = 1\) and
\(\lambda_{2} = 1\) in estimating all the models presented above in
Table~\ref{tbl-simulation}.

Finally, the above simulation study indicates that the choice of the
initialisation depends on the data/application. Thus, the users of the
algorithm are encouraged to follow a trial-and-error procedure for
choosing the best initial model for a given application.

In the next section, we illustrate the forecasting performance of the
SMI Model in two real-world applications.

\hypertarget{sec-application}{%
\section{Empirical Applications}\label{sec-application}}

\hypertarget{forecasting-daily-mortality}{%
\subsection{Forecasting Daily
Mortality}\label{forecasting-daily-mortality}}

We apply the SMI Model algorithm to a data set from
\textcite{Masselot2022}, to forecast daily mortality based on heat
exposure.

Studying the effects of various environmental exposures such as weather
related variables, pollutants and man-made environmental conditions etc.
on human health, is of significant importance in environmental
epidemiology. Therefore, forecasting daily deaths taking heat related
variables as predictors is an interesting application.

\hypertarget{description-of-the-data}{%
\subsubsection{Description of the Data}\label{description-of-the-data}}

For this analysis, we consider daily mortality and heat exposure data
for the Metropolitan Area of Montreal, Province of Quebec, Canada, from
1990 to 2014, for the months June, July, and August (i.e.~summer
season). The daily all-cause mortality data were obtained from the
National Institute of Public Health, Province of Quebec, while
\emph{``DayMet''} - a 1 km × 1 km gridded data set
\autocite{Thornton2021} was used to extract daily temperature and
humidity data \autocite{Masselot2022}.

Figure~\ref{fig-deaths} shows the time plots of daily deaths during the
summer for the years from 1990 to 1993. The series for only four years
are plotted separately as a faceted grid for visual clarity.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-deaths-1.pdf}

}

\caption{\label{fig-deaths}Daily mortality in summer in Montreal, Canada
- from 1990 to 1993}

\end{figure}

The three main predictors considered in this empirical study are maximum
temperature, minimum temperature, and vapor pressure (to represent the
level of humidity). The number of daily deaths are plotted against each
of these predictors in Figure~\ref{fig-Tmax}, Figure~\ref{fig-Tmin}, and
Figure~\ref{fig-Vp} respectively, where we can observe that the
relationships between all these predictors and the response are slightly
non-linear.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-Tmax-1.pdf}

}

\caption{\label{fig-Tmax}Daily mortality in summer (from 1990 to 2014)
plotted against maximum temperature}

\end{figure}

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-Tmin-1.pdf}

}

\caption{\label{fig-Tmin}Daily mortality in summer (from 1990 to 2014)
plotted against minimum temperature}

\end{figure}

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-Vp-1.pdf}

}

\caption{\label{fig-Vp}Daily mortality in summer (from 1990 to 2014)
plotted against vapor pressure}

\end{figure}

\hypertarget{predictors-considered}{%
\subsubsection{Predictors Considered}\label{predictors-considered}}

\textbf{1) Current maximum/minimum temperatures and lags:}

In addition to current maximum and minimum temperatures, the temperature
measurements up to 14 days prior (i.e.~\(0^{th}\) to \(14^{th}\) lag)
are also considered as predictors in the forecasting model, because it
is obvious that not only the current temperature, but also the
temperatures that were prevailing in the recent past can add up to the
adverse level of heat exposure of a person.

\textbf{2) Current vapor pressure and lags:}

Similar to temperature variables, the current value and 14 lags of vapor
pressure are considered as predictors, as a proxy to the level of
humidity.

\textbf{3) Calendar effects:}

Finally, a couple of calendar variables; \emph{day of the season (DOS)}
and \emph{Year}, are incorporated into the model to capture annual trend
and seasonality, and also to control the autocorrelation in residuals,
which is a common practice in environmental epidemiology
\autocite{Masselot2022}.

\hypertarget{modelling-framework}{%
\subsubsection{Modelling Framework}\label{modelling-framework}}

Maximum temperature lags, minimum temperature lags and vapor pressure
lags are considered as predictors that are entering indices. The two
calendar variables, \emph{DOS} and \emph{Year}, are included into the
model as separate nonparametric components that do not enter any of the
indices.

Hence, the relevant SMI Model can be expressed as
\begin{equation}\protect\hypertarget{eq-heat}{}{
\begin{aligned}
  \textbf{Deaths} = & \beta_{0} + \sum_{j = 1}^{p}{g_{j}(\bm{X}\bm{\alpha}_{j})} + f_{1}(\textbf{DOS}) + f_{2}(\textbf{Year})+ \bm{\varepsilon},
\end{aligned}
}\label{eq-heat}\end{equation} where

\begin{itemize}
\item
  \(\textbf{Deaths}\) is the vector containing the observations of
  number of daily deaths;
\item
  \(\beta_{0}\) is the model intercept;
\item
  \(p\) is the unknown number of indices that will be estimated through
  the algorithm;
\item
  \(\bm{X}\) is the matrix containing the predictor variables that are
  entering indices (i.e.~maximum temperature lags, minimum temperature
  lags and vapor pressure lags);
\item
  \(\bm{\alpha}_{j}, j = 1, \dots, p\) are the index coefficient vectors
  of length = number of predictors entering indices (\(q\)) \(= 45\)
  each;
\item
  \(g_{j}, j = 1, \dots, p\), \(f_{1}\) and \(f_{2}\) are unknown
  nonparametric functions; and
\item
  \(\bm{\varepsilon}\) is the vector of errors.
\end{itemize}

The data from 1990 to 2012 are used as the training set to estimate the
model, while the data of year 2014 are separated to be the test set to
evaluate the forecasting performance of the estimated model. The data
corresponding to the three summer months of year 2013 are kept aside as
a validation set, which is used to estimate some of the benchmark models
for comparison purposes.

Then we apply the proposed SMI Model algorithm to the training set to
estimate the model. Finally, the forecasting accuracy on the test set is
evaluated using Mean Squared Error (MSE) and Mean Absolute Error (MAE).

\hypertarget{results}{%
\subsubsection{Results}\label{results}}

We estimated the SMI Model for the mortality data using three different
initialisation options: ``PPR'', ``Additive'' and ``Linear'', for
comparison purposes. Further, we tuned the penalty parameters
\(\lambda_{0}\) and \(\lambda_{2}\), over ranges of integers from 1 to
12, and 0 to 12 respectively, through a greedy search based on in-sample
MSE. (Here, a greedy search is used instead of a grid search to reduce
computational time.)

The penalty parameter combination
\((\lambda_{0} = 11, \lambda_{2} = 12)\) was selected for the model
fitted with ``PPR'' initialisation. The estimated model,
\textbf{\emph{SMI Model (11, 12) - PPR }}, did not drop any of the index
variables, and resulted in 7 indices. The best penalty parameter
combination for the model estimated taking ``Additive'' model as the
starting point was \((\lambda_{0} = 1, \lambda_{2} = 0)\). The estimated
SMI Model did not drop any index variables or indices, and thus the
final model, \textbf{\emph{SMI Model (1, 0) - Additive }}, is equivalent
to a nonparametric additive model. The model estimated with ``Linear''
initialisation selected \((\lambda_{0} = 1, \lambda_{2} = 11)\) as the
penalty parameter combination (\textbf{\emph{SMI Model (1, 11) - Linear
}}), and resulted in 2 indices, without dropping any of the index
variables. (We used \(M = 10\), a convergence tolerance of \(0.001\),
\(50\) maximum iterations, and a convergence tolerance of \(0.001\) for
coefficients in estimating all the SMI Models.)

We evaluated the forecasting error of the estimated models using two
different subsets of the original test set:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{\emph{Test Set 1:}} original test set of 3 months (June, July
  and August of 2014); and
\item
  \textbf{\emph{Test Set 2:}} a test set of 1 month (June 2014).
\end{enumerate}

Note that in this application, we assumed that the future values of the
maximum/minimum temperatures and vapor pressure are known to use in the
forecasting model.

The MSE and MAE values obtained on the two variations of the test set
for the estimated SMI Models are presented in Table~\ref{tbl-heat}.
According to the results obtained, the SMI Model estimated with ``PPR''
initialisation, \textbf{\emph{SMI Model (11, 12) - PPR }}, shows the
best forecasting performance on both the test sets, out of the three
different SMI Models estimated.

Furthermore, we present test set forecasting errors of four different
benchmark models in Table~\ref{tbl-heat}, to compare with the estimated
SMI Models. First, the forecasting errors of a projection pursuit
regression (PPR) model is presented. The number of indices of the PPR
model was taken as 7, which is the number of indices estimated by
\textbf{\emph{SMI Model (11, 12) - PPR }} - the estimated SMI Model with
the best forecasting accuracy. Next, a nonparametric additive model
formulated through backward elimination as proposed by \textcite{FH2012}
(Section~\ref{sec-backward}), and the CGAIM proposed by
\textcite{Masselot2022} (Section~\ref{sec-multi-index}), are also
presented for comparison purposes. In the case of CGAIM, following
\textcite{Masselot2022}, maximum temperature lags, minimum temperature
lags and vapor pressure lags are categorised into three groups, where we
estimated an index for each of those groups using the CGAIM algorithm.
Furthermore, the index coefficients of the CGAIM were constrained to be
positive and decreasing, and the nonparametric link functions were
constrained to be monotonically increasing. We also fitted an
(unconstrained) GAIM, where the results are presented in
Table~\ref{tbl-heat}.

\hypertarget{tbl-heat}{}
\begin{table}[!h]
\caption{\label{tbl-heat}Daily mortality forecasting - Out-of-sample point forecast results }\tabularnewline

\centering
\begin{tabular}{lrr>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Test Set 1} & \multicolumn{2}{c}{Test Set 2} \\
\cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7}
\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Predictors} & \multicolumn{1}{c}{Indices} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE}\\
\midrule
SMI Model (11, 12) - PPR & 47 & 7 & \textbf{76.328} & \textbf{6.721} & \textbf{92.116} & \textbf{7.540}\\
SMI Model (1, 0) - Additive & 47 & 45 & 160.454 & 10.031 & 208.072 & 11.317\\
SMI Model (1, 11) - Linear & 47 & 2 & 159.579 & 9.925 & 203.678 & 11.034\\
PPR & 47 & 7 & 89.095 & 7.791 & 100.820 & 8.397\\
Backward Elimination & 34 & NA & 154.837 & 10.004 & 164.510 & 9.920\\
\addlinespace
CGAIM & 47 & 3 & 85.679 & 7.208 & 101.689 & 8.032\\
GAIM & 47 & 3 & 81.740 & 6.920 & 101.654 & 8.210\\
\bottomrule
\end{tabular}
\end{table}

According to Table~\ref{tbl-heat}, \textbf{\emph{SMI Model (11, 12) -
PPR}} outperforms all the four benchmark models in terms of forecasting
accuracy, for both \emph{Test Set 1} and \emph{Test Set 2}. However, the
SMI Models estimated using ``Additive'' or ``Linear'' initialisations
have inferior forecasting performance to all four benchmark models
considered.

The actual series of number of deaths and the predicted series from the
\textbf{\emph{SMI Model (11, 12) - PPR}} and the benchmark models on
\emph{Test Set 2} are plotted in Figure~\ref{fig-heatPred} for further
comparison.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-heatPred-1.pdf}

}

\caption{\label{fig-heatPred}Actual number of deaths vs.~predicted
number of deaths from ``SMI Model (11, 12) - PPR'' and benchmark models
for Test Set 2}

\end{figure}

\hypertarget{forecasting-daily-solar-intensity}{%
\subsection{Forecasting Daily Solar
Intensity}\label{forecasting-daily-solar-intensity}}

\textcolor{purple}{(To be completed.)}

Next, we utilise the SMI Model algorithm to forecast daily solar
intensity, using other weather conditions.

According to \textcite{EI2023}, the portion of the world's primary
energy consumption fulfilled by renewable energy (except for
hydroelectricity) has reached 7.5\% in 2022. The capacity addition by
solar and wind power in 2022 is recorded as 266 GW, where 72\% of that
capacity addition was attributed to solar energy \autocite{EI2023}.
Therefore, obtaining better forecasts of solar power generation is
crucial for planning and management of power systems. As solar power
generation is undoubtedly associated with solar intensity, forecasting
solar intensity a useful step towards accurate forecasting of solar
power generation.

\hypertarget{description-of-the-data-1}{%
\subsubsection{Description of the
Data}\label{description-of-the-data-1}}

We use solar intensity and other weather variables measured at a Davis
weather station in Amherst, Massachusetts, obtained from the \emph{UMass
Trace Repository} \autocite{Umass2023}. The data was recorded at every 5
minutes, from 21th February 2006 to 27th February 2013, using sensors
for measuring temperature, wind chill, humidity, dew point, wind speed,
wind direction, rain, pressure, solar intensity, and UV.

However, the data contained missing entries recorded as ``-100000'',
which we removed from the data set. Moreover, for this analysis, we
converted the 5 minutes data to daily data by averaging each variable
over days.

Figure~\ref{fig-solar} shows the time plot of daily solar intensity for
the entire period, which clearly depicts the annual seasonality in the
data. (As observed in Figure~\ref{fig-solar}, there are days for which
the observations were missing. We excluded those days from the analysis,
and used only the days for which the data are available.)

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-solar-1.pdf}

}

\caption{\label{fig-solar}Daily solar intensity in Amherst,
Massachusetts - from February 2006 to February 2013}

\end{figure}

The variables temperature, dew point, wind, rain and humidity were
considered to be the main set of predictors in the model. The daily
solar intensity is plotted against each of these predictors in
Figure~\ref{fig-preds}, where we can observe that the relationships
between these predictors and the response are non-linear.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-preds-1.pdf}

}

\caption{\label{fig-preds}Daily solar intensity against other weather
variables}

\end{figure}

\hypertarget{predictors-considered-1}{%
\subsubsection{Predictors Considered}\label{predictors-considered-1}}

\textbf{1) Solar intensity lags:}

Three lags of the daily solar intensity itself are used as predictors to
incorporate the serial correlations presented in the data into the
modelling process. Intuitively, the solar intensity of a particular day
would have a relationship to the solar intensity of adjacent days.

\textbf{2) Current weather variables and lags:}

In addition to current temperature, dew point, wind speed, rain and
humidity, the measurements of 3 previous days (i.e.~\(0^{th}\) to
\(3^{rd}\) lag) of each of the above weather variables are also
considered as predictors in the forecasting model.

\textbf{3) Calendar effects:}

Finally, a couple of calendar variables; \emph{Month} (12 months of the
year) and \emph{Season} (the 4 seasons of the year: Spring, Summer,
Autumn and Winter), are incorporated into the model to capture annual
seasonality, and also to control the autocorrelation in residuals.

\hypertarget{modelling-framework-1}{%
\subsubsection{Modelling Framework}\label{modelling-framework-1}}

The lags of solar intensity, and the lags of weather variables are
considered as predictors that are entering indices. The two calendar
variables, \emph{Month} and \emph{Season}, are included into the model
as linear (categorical) predictor variables.

Hence, the relevant SMI Model can be expressed as
\begin{equation}\protect\hypertarget{eq-solar}{}{
\begin{aligned}
  \textbf{Solar} = & \beta_{0} + \sum_{j = 1}^{p}{g_{j}(\bm{X}\bm{\alpha}_{j})} + \theta_{1}\textbf{Month} + \theta_{2}\textbf{Season}+ \bm{\varepsilon},
\end{aligned}
}\label{eq-solar}\end{equation} where

\begin{itemize}
\item
  \(\textbf{Solar}\) is the vector containing the observations of daily
  solar intensity;
\item
  \(\beta_{0}\) is the model intercept;
\item
  \(p\) is the unknown number of indices that will be estimated through
  the algorithm;
\item
  \(\bm{X}\) is the matrix containing the predictor variables that are
  entering indices (i.e.~solar intensity, temperature, dew point, wind
  speed, rain and humidity lags);
\item
  \(\bm{\alpha}_{j}, j = 1, \dots, p\) are the index coefficient vectors
  of length = number of predictors entering indices (\(q\)) \(= 23\)
  each;
\item
  \(g_{j}, j = 1, \dots, p\) are unknown nonparametric functions;
\item
  \(\theta_{1}\) and \(\theta_{2}\) are the two coefficients
  corresponding to the two linear predictor variables; and
\item
  \(\bm{\varepsilon}\) is the vector of errors.
\end{itemize}

The data from February 2006 to October 2012 are used as the training set
to estimate the model, while the data of the months January and February
2013 are separated to be the test set to evaluate the forecasting
performance of the estimated model. The data corresponding to the months
November and December 2013 are kept aside as a validation set, which is
required to estimate some of the benchmark models for comparison
purposes.

Then we apply the proposed SMI Model algorithm to the training set to
estimate the model, and the forecasting accuracy on the test set is
evaluated using Mean Squared Error (MSE) and Mean Absolute Error (MAE).

\hypertarget{results-1}{%
\subsubsection{Results}\label{results-1}}

Both the empirical applications presented above were performed using
\textbf{\emph{R statistical software}} \autocite{R2023}, and the
\textbf{\emph{Rstudio}} integrated development environment (IDE)
\autocite{Rstudio2024}. We used the commercial MIP solver
\textbf{\emph{``Gurobi''}} \autocite{gurobi2023} to solve the MIQPs
related to the proposed SGAIM algorithm, through the
\textbf{\emph{Gurobi plug-in (ROI.plugin.gurobi)}}
\autocite{Schwendinger2023} available from the \textbf{\emph{R
Optimization Infrastructure (ROI)}} \autocite{Hornik2023,Theusl2020}
package. Furthermore, the GAMs were fitted using the R package
\textbf{\emph{mgcv}} \emph{(v1.9.1)} \autocite{Wood2011}.

\hypertarget{sec-conclusion}{%
\section{Conclusions}\label{sec-conclusion}}

\textcolor{purple}{(To be completed.)} \newline

\textbf{\large{Acknowledgements}}

We thank Professor Louise Ryan for joining the discussions during the
initial stage of the project, and for her valuable comments and feedback
on this research work.

Furthermore, this research is partially supported by the Monash
eResearch Centre through the use of the MonARCH (Monash Advanced
Research Computing Hybrid) HPC Cluster.

\printbibliography

\end{document}
