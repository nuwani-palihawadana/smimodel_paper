\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{multirow}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{
            pdftitle={Sparse Multiple Index Models for High-dimensional Nonparametric Forecasting},
            pdfkeywords={Additive index models, Variable selection,
Dimension reduction, Mixed integer programming},
            colorlinks=true,
            linkcolor=blue,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Sparse Multiple Index Models for High-dimensional Nonparametric
Forecasting}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\usepackage[tabular,lf]{sourcesanspro}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}
        \placefig{2}{1.5}{width=5cm}{_extensions/numbats/wp/monash2}
        \placefig{16.9}{1.5}{width=2.1cm}{_extensions/numbats/wp/MBSportrait}
        \begin{textblock}{4}(16.9,4)ISSN 1440-771X\end{textblock}
        \begin{textblock}{7}(12.7,27.9)\hfill
        \includegraphics[height=0.7cm]{_extensions/numbats/wp/AACSB}~~~
        \includegraphics[height=0.7cm]{_extensions/numbats/wp/EQUIS}~~~
        \includegraphics[height=0.7cm]{_extensions/numbats/wp/AMBA}
        \end{textblock}
        \vspace*{2cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \footnotesize http://monash.edu/business/ebs/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}\vspace*{2cm}}}
\def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false, date=year}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}

\wp{no/yr}
\jel{C10,C14,C22}

\RequirePackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}
\def\placefig#1#2#3#4{\begin{textblock}{.1}(#1,#2)\rlap{\includegraphics[#3]{#4}}\end{textblock}}




\author{Nuwani~Palihawadana, Rob J~Hyndman, Xiaoqian~Wang}
\addresses{\textbf{Nuwani Palihawadana}\newline
Department of Econometrics \& Business Statistics\newline
Clayton VIC 3800\newline
Australia\newline
{Email: nuwani.kodikarapalihawadana@monash.edu}\newline Corresponding author\newline\\[0.5cm]
\textbf{Rob J Hyndman}\newline
Department of Econometrics \& Business Statistics\newline
Clayton VIC 3800\newline
Australia\newline
{Email: rob.hyndman@monash.edu}\\[0.5cm]
\textbf{Xiaoqian Wang}\newline
Department of Econometrics \& Business Statistics\newline
Clayton VIC 3800\newline
Australia\newline
{Email: xiaoqian.wang@monash.edu}\\[0.5cm]
}

\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf Palihawadana, Hyndman, Wang: \@date}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother

% Adjust headwidth in case user has changed geometry in header-includes
\renewcommand{\headwidth}{\textwidth}

\begin{document}
\maketitle
\begin{abstract}
High-dimensionality is a common phenomenon in real-world forecasting
problems. Oftentimes, forecasts are contingent on a long history of
predictors, while the relationships between some predictors and the
response of interest exhibit complex nonlinear patterns. In such a
situation, a nonlinear ``transfer function'' model, with additivity
constraints to mitigate the issue of \emph{curse of dimensionality}, is
a conspicuous choice. Particularly, nonparametric \emph{additive index
models} greatly reduce the number of parameters to be estimated in
comparison to a general additive model. In this paper, we present a
novel algorithm for estimating high-dimensional nonparametric additive
index models, with simultaneous variable selection, which we call
\textbf{\emph{SMI}} (\textbf{\emph{S}}parse \textbf{\emph{M}}ultiple
\textbf{\emph{I}}ndex) \textbf{\emph{Model}}. The SMI Modelling
algorithm is based on an iterative procedure that applies mixed integer
programming to solve an \(\ell_{0}\)-regularised nonlinear least squares
problem. We demonstrate the functionality and the characteristics of the
proposed algorithm through a simple simulation exercise. We also
illustrate the use of the SMI Modelling algorithm in two empirical
applications related to forecasting heat exposure related daily
mortality and daily solar intensity.
\end{abstract}
\begin{keywords}
Additive index models, Variable selection, Dimension reduction, Mixed
integer programming
\end{keywords}

\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, frame hidden, sharp corners, interior hidden, borderline west={3pt}{0pt}{shadecolor}, enhanced, breakable]}{\end{tcolorbox}}\fi

\hypertarget{sec-introduction}{%
\section{Introduction}\label{sec-introduction}}

Forecasts are often contingent on a very long history of predictors. For
example, when forecasting half-hourly electricity demand, it is common
to use at least a week of historical half-hourly temperatures and other
weather observations \autocite{HF2010}. Similarly, when forecasting bore
levels, rainfall data from up to thousand days earlier can impact the
result \autocite{Bakker2019} due to the complex flow dynamics of
rainfall into aquifers.

On the other hand, in most of these applications, the relationships
between the predictors and the response variable exhibit complex
nonlinear patterns. For instance, the relationship between electricity
demand and temperature is often nonlinear \autocite{HF2010,FH2012}.

These examples suggest a possible nonlinear ``\emph{transfer function}''
model of the form
\begin{equation}\protect\hypertarget{eq-transferfun}{}{
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}, y_{t-1},\dots,y_{t-k}) + \varepsilon_{t},
}\label{eq-transferfun}\end{equation} where \(y_{t}\) is the observation
of the response variable at time \(t\), \(\bm{x}_{t}\) is a vector of
predictors at time \(t\), and \(\varepsilon_{t}\) is the random error.
By including lagged values of \(y_{t}\) along with the lagged
predictors, we allow for any serial correlation in the data. However, it
makes the resulting function difficult to interpret. An alternative
formulation is \[
 y_t = f(\bm{x}_{t}, \bm{x}_{t-1}, \dots,\bm{x}_{t-p}) + g(\varepsilon_{t}, \varepsilon_{t-1},\dots,\varepsilon_{t-k}),
\] which is more difficult to estimate, but makes it simpler to
interpret the effect of the predictors on the response variable.

When applying the transfer function model to forecast lengthy time
series with complex patterns, the form of \(f\) is nonlinear, involving
complicated interactions, and with a high value of \(p\).

Typically, the form of \(f\) involves many ad hoc model choices. It is
essentially impossible to estimate a \(p\)-dimensional function for
large \(p\) due to the curse of dimensionality
\autocite{Bellman57,Stone82}. Instead, we normally impose some form of
additivity, along with some low-order interactions.

For example, \textcite{FH2012} proposed a \textbf{\emph{semi-parametric
additive model}} to obtain short-term forecasts of the half-hourly
electricity demand for power systems in the Australian National
Electricity Market. In this model, \(f\) is assumed to be fully
additive, and is used to capture the effects of recent predictor values
on the demand. The main objective behind the use of this proposed
semi-parametric model is to allow nonparametric components in a
regression-based modelling framework with serially correlated errors
\autocite{FH2012}. The model fitted for each half-hourly period (\(q\))
can be written as \[
 \log(y_{t,q}) = h_{q}(t) + f_{q}(w_{1,t},w_{2,t}) + a_{q}(y_{t-,p}) + \varepsilon_{t},
\] where the response variable is the logarithm of electricity demand at
time \(t\) (measured in the half-hourly intervals) during period \(q\).
The term \(h_{q}(t)\) models several calendar effects that are included
as linear terms. The temperature effects are modelled using the
nonparametric component \(f_{q}(w_{1,t},w_{2,t})\), while the
nonparametric term \(a_{q}(y_{t-,p})\) captures the lagged effects of
the response. It is important to notice here that the error term
\(\varepsilon_{t}\) is serially uncorrelated in each half-hourly model,
because the serial correlation is eliminated by the inclusion of the
lagged responses in the model. However, there will still be some
correlation between the residuals from the various half-hourly models
\autocite{FH2012}.

Similarly, a \textbf{\emph{distributed lag model}} was proposed by
\textcite{Wood2017} to forecast daily death rate in Chicago using
measurements of several air pollutants. In this model, the response
variable is modelled via a sum of smooth functions of lagged predictor
variables, which is quite similar in nature to the semi-parametric
additive model used by \textcite{FH2012}. However, unlike in
\textcite{FH2012}, \textcite{Wood2017} suggested to allow the smooth
functions for lags of the same covariate to vary smoothly over lags,
preventing large differences in estimated effects between adjacent lags.
Thus, the model is of the form \[
 \log(y_{t}) = f_{1}(t) + \sum_{k=0}^{K} f_{2}(p_{t-k}, k) + \sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k),
\] where \(y_{t}\) is the death rate at day \(t\), and \(f_{1}\) is a
nonparametric term to capture the \emph{time} effect. The model
incorporates the current value (\(k = 0\)) and several lagged values
(\(k = 1, \dots, K\)) of the predictors, where the \emph{distributed lag
effect} of a single predictor variable, and of an interaction of two
predictor variables are captured by the sum of nonparametric terms
\(\sum_{k=0}^{K} f_{2}(p_{t-k}, k)\) and
\(\sum_{k=0}^{K} f_{3}(o_{t-k}, w_{t-k}, k)\) respectively. The smooth
functions \(f_{2}\) and \(f_{3}\) are proposed to be estimated using
\emph{tensor product smooths}.

For more examples, \textcite{Ho2020} used semi-paramteric additive
models to estimate ground-level \(PM_{2.5}\) concentrations in Taiwan,
while nonparametric additive models were utilised by
\textcite{Ibrahim2021} for predicting census survey response rates.
Furthermore, \textcite{Ravindra2019} provided a comprehensive review of
the applications of additive models for environmental data, with a
special focus on air pollution, climate change, and human health related
studies.

While such models have been used to address problems including
electricity demand, air quality related mortality rate and groundwater
level forecasting etc.
\autocite{FH2012,HF2010,Wood2017,Peterson2014,Rajaee2019}, there are
still a number of unresolved issues in their applications. In this
paper, we attempt to address two of those issues. Firstly, even though
nonparametric additive models act as a remedy to the curse of
dimensionality as we discussed earlier, the estimation of the model is
still challenging in a high-dimensional setting due to the large number
of nonparametric components to be estimated. Secondly, there is a
noticeable subjectivity in the selection of predictor variables (from
the available predictors) for the model, where in most of the
applications of interest we discussed above, the predictor choices in
the final model are mainly based on empirical explorations or domain
expertise.

There are a number of previous studies that have attempted to address
the issue of variable selection in nonparametric/semi-parametric
additive models to some extend, using various techniques. For example,
\textcite{Huang2010} used a \emph{Least Absolute Shrinkage and Selection
Operator (LASSO)} \autocite{Tibshirani1996} based procedure for variable
selection in nonparametric additive models, whereas \textcite{FH2012}
used a straightforward backward elimination technique to achieve
selection. Moreover, \textcite{Ibrahim2021} and \textcite{Hazimeh2023}
used Mixed Integer Programming based methodologies to provide a solution
to the \emph{best subset selection} problem in nonparametric additive
models. More details of these methods are discussed later in
Section~\ref{sec-background}.

In this paper, however, we are interested in high-dimensional
applications that exhibit complicated interactions among predictors
(specially in the presence of large number of lagged variables), as well
as correlated errors. In such a situation, \emph{``index models''}
(refer Section~\ref{sec-Index}) seem to be useful for improving the
flexibility of the broader class of nonparametric additive models
\autocite{Radchenko2015}, while mitigating the difficulty of estimating
a nonparametric component for each individual predictor.

To our knowledge, no previous research has been done to look at how the
predictor choices can be made more objective and principled in
nonparametric additive index models. Hence, our goal was to develop a
methodology for optimal predictor selection in the context of
high-dimensional nonparametric additive index models. Moreover, due to
computational advancements in the field, the use of \emph{Mathematical
Optimisation} concepts in solving statistical problems has gained a lot
of interest in the recent past \autocite{Theusl2020}. This motivated us
to develop a variable selection algorithm based on mathematical
optimisation techniques.

Additionally, it is crucial to point out that any such variable
selection methodology naturally renders inferential statistics invalid,
since we do not assume the resulting model obtained through the variable
selection procedure to represent the true data generating process.
Hence, our focus in this paper is only on improving forecasts, but not
on making inferences on the resulting parameter estimates.

The rest of this paper is organised as follows. In
Section~\ref{sec-background}, we provide a concise exposition of related
ideas and previous work, while establishing the foundation for this
paper. Section~\ref{sec-SMI} presents our proposed model, \emph{Sparse
Multiple Index Model} (SMI Model), and describes the variable selection
algorithm and estimation procedure. In Section~\ref{sec-simulation}, we
demonstrate the functionality and the characteristics of the proposed
algorithm through a simulation experiment. Section~\ref{sec-application}
illustrates two empirical applications of the proposed estimation and
variable selection methodology, related to forecasting heat exposure
related daily mortality and daily solar intensity. Concluding remarks
are given in Section~\ref{sec-conclusion}.

\hypertarget{sec-background}{%
\section{Background}\label{sec-background}}

\hypertarget{variable-selection-in-nonparametric-additive-models}{%
\subsection{Variable Selection in Nonparametric Additive
Models}\label{variable-selection-in-nonparametric-additive-models}}

As discussed in Section~\ref{sec-introduction}, the estimation of
nonparametric function \(f\) (Equation~\ref{eq-transferfun}) becomes
infeasible in high-dimensional settings (i.e.~number of predictors is
very large) due to curse of dimensionality. As a result,
\emph{nonparametric additive models} have been employed with growing
popularity. Let \((y_{i}, \bm{x}_{i}), i = 1, \dots, n\), be independent
and identically distributed (i.i.d) observations, and
\(\bm{x}_{i} = (x_{i1}, \dots, x_{ip})^{T}\) be a \(p\)-dimensional
vector of predictor values. Then a nonparametric additive model can be
written as \begin{equation}\protect\hypertarget{eq-add}{}{
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \varepsilon_{i}, \quad i = 1, \dots, n,
}\label{eq-add}\end{equation} where \(f_{j}\)'s are unknown functions
(probably non-linear and smooth), and \(\varepsilon_{i}\) is the random
error \autocite{Lian2012}. Even such an additivity condition is imposed,
estimating the optimal predictive model will still be troublesome when
\(p\) is very large (probably even larger than the sample size \(n\))
due to over-fitting \autocite{Lian2012}. Thus, it is natural to bring in
the sparsity assumption, and assume that some of \(f_{j}\)'s are zero,
which gives rise to the need of a variable selection method to
differentiate between zero and non-zero components, while estimating the
non-zero components \autocite{Huang2010}.

\hypertarget{sec-backward}{%
\subsubsection{Backward Elimination}\label{sec-backward}}

In the problem of forecasting long-term peak electricity demand,
\textcite{HF2010} used a stepwise procedure for variable selection
through cross-validation. In the each half-hourly model fitted, the data
is split into training and validation sets, and the predictors are
selected into the model based on the Mean Squared Error (MSE) calculated
for the validation set. Starting from the full model, the predictive
power of each variable is evaluated by dropping one at a time. A
predictor, the removal of which contributed to a decrease in the
validation MSE, is omitted from the model in subsequent steps
\autocite{HF2010}. \textcite{FH2012} used a similar method except for
the fact that they considered the Mean Absolute Percentage Error (MAPE)
as the selection criterion. Therefore, both of these prior work use
stepwise variable selection methodology based on out-of-sample
forecasting performance.

\hypertarget{penalisation-methods}{%
\subsubsection{Penalisation Methods}\label{penalisation-methods}}

According to \textcite{Huang2010}, there are numerous penalised methods
for variable selection and parameter estimation in high-dimensional
settings, including the \emph{bridge estimator} proposed by
\textcite{Frank1993}, the \emph{Least Absolute Shrinkage and Selection
Operator (LASSO)} by \textcite{Tibshirani1996}, the \emph{Smoothly
Clipped Absolute Deviation Penalty} (SCAD) by \textcite{Fan2001}, and
the \emph{Minimum Concave Penalty} (MCP) by \textcite{Zhang2010}. Among
them, we observe that the LASSO and the SCAD penalties are appearing
popularly in literature.

\textcite{Tibshirani1996} introduced the regularisation method,
\textbf{\emph{LASSO}}, for estimating linear models, which minimises the
sum of squared residuals subject to the \(\ell_{1}\) penalty on the
coefficients. Assume the classical linear regression model
\(y_{i} = \sum_{j=1}^{p} {\beta_{j}x_{ij}} +\varepsilon_{i}\), fitted
for the data \((y_{i}, \bm{x_{i}})\), \(i = 1, \dots, n\), where
\(y_{i}\) is the response, \(\bm{x_{i}} = (x_{i1}, \dots, x_{ip})^T\) is
a \(p\)-dimensional vector of predictors,
\(\bm{\beta} = (\beta_{1}, \dots, \beta_{p})^{T}\) is the parameter
vector corresponding to \(\bm{x_{i}}\), and \(\varepsilon_{i}\) is the
random error. Then, the LASSO estimator, \(\bm{\hat{\beta}}_{LASSO}\),
can be obtained by \[
 \bm{\hat{\beta}}_{LASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {|\beta_{j}|}\right\},
\] where \(\bm{x_{j}} = \left (x_{1j}, \dots, x_{nj}\right )^{T}\), and
\(\lambda\) is a non-negative tuning parameter. The LASSO estimator
reduces to the Ordinary Least Squares (OLS) estimator if \(\lambda\) is
equal to zero \autocite{Konzen2016}. Due to the nature of the penalty
applied, LASSO shrinks some of the coefficients towards zero, and sets
the others exactly to zero, where the estimation of coefficients and
variable selection are performed simultaneously \autocite{Konzen2016}.

While showing that the LASSO is not consistent for variable selection in
certain situations, \textcite{Zou2006} introduced \textbf{\emph{Adaptive
Lasso}} (popularly known as ``adaLASSO''); an extension of the LASSO
method, which uses adaptive weights to penalise coefficients using the
LASSO (i.e.~\(\ell_{1}\)) penalty. Thus, the adaLASSO objective function
can be written as \[
 \bm{\hat{\beta}}_{adaLASSO} = \min_{\bm{\beta}}\left\{\left\lVert\bm{y} - \sum_{j=1}^{p} {\bm{x_{j}}\beta_{j}}\right\rVert_{2}^{2} + \lambda\sum_{j=1}^{p} {w_{j}|\beta_{j}|}\right\},
\] where the vector of weights
\(\bm{w} = \left (w_{1}, \dots, w_{p} \right )^{T}\) is estimated by
\(\bm{\hat{w}} = 1/|\bm{\hat{\beta}}|^{\gamma}\) for \(\gamma > 0\),
which is a tuning parameter, and \(\bm{\hat{\beta}}\) being any
consistent estimator of \(\bm{\beta}\) \autocite{Zou2006}.

\textcite{Yuan2006} considered the problem of selecting groups of
variables, and discussed extensions of three variable selection and
estimation methods namely, \emph{LASSO} \autocite{Tibshirani1996},
\emph{Least Angle Regression Selection} \autocite[LARS,][]{Efron2004},
and \emph{Non-negative Garrotte} \autocite{Breiman1995}. Consider an
\(n\)-dimensional response vector \(\bm{y}\), and an \(n \times p\)
matrix of predictor values \(\bm{X}\). Then the \textbf{\emph{Group
Lasso}} estimator of the coefficients vector \(\bm{\beta}\) is obtained
by minimising \[
 \frac{1}{2}\left\lVert\bm{y} - \sum_{\ell=1}^{L} {\bm{X}_{\ell}\bm{\beta}_{\ell}}\right\rVert_{2}^{2} + \lambda\sum_{\ell=1}^{L} {\lVert\bm{\beta}_{\ell}\rVert}_{\bm{K}_{\ell}},
\] where \(\bm{X}_{\ell}\) is an \(n \times p_{\ell}\) sub-matrix in
\(\bm{X}\) that corresponds to the \(\ell^{th}\) group of predictors
(\(p_{\ell}\) is the number of predictors in \(\ell^{th}\) group),
\(\bm{\beta}_{\ell}\) is the corresponding vector of coefficients,
\(\ell = 1, \dots, L\),
\(\lVert\bm{\beta}_{\ell}\rVert_{\bm{K}_{\ell}} = (\bm{\beta}_{\ell}' \bm{K}_{\ell} \bm{\beta}_{\ell})^{\frac{1}{2}}\)
with \(\bm{K}_{1}, \dots, \bm{K}_{L}\) being a set of given positive
definite matrices, and \(\lambda\) is a non-negative tuning parameter.
Moreover, \textcite{Simon2013} proposed \textbf{\emph{Sparse-Group
Lasso}}, which is a convex combination of general Lasso and Group Lasso
methods, where the focus is on both ``groupwise sparsity'' (the number
of groups with at least one nonzero coefficient), and ``within group
sparsity'' (the number of nonzero coefficients within each nonzero
group).

According to \textcite{Fan2001}, a penalty function used in penalised
least squares approaches should have three properties. Firstly, it
should be singular at origin to generate a solution that is sparse.
Secondly, it should fulfill certain conditions to be stable in model
selection. Finally, it should be able to generate unbiased estimates for
large coefficients via being bounded by a constant. They argued that all
those three conditions are not satisfied by the penalisation methods
such as the bridge regression \autocite{Frank1993} and the LASSO
\autocite{Tibshirani1996}. Hence they proposed the \textbf{\emph{SCAD}}
penalty function, which is defined in terms of its first derivative as
\[
 p_{\lambda}'(\theta) = \lambda\left\{I(\theta\le\lambda) + \frac{(a\lambda - \theta)_{+}}{(a - 1)\lambda}I(\theta > \lambda)\right\},
\] for some \(a > 2\), and \(\theta > 0\) \autocite{Fan2001}. According
to \textcite{Fan2001}, the SCAD penalty function retains the favourable
properties of both best subset selection and ridge regression, while
having all three desired features, i.e., sparsity, stability, and
unbiasedness.

Based on the above penalisation methods that are originally developed
for linear models, \textcite{Huang2010} proposed a new penalisation
method for variable selection in nonparametric additive model
(Equation~\ref{eq-add}), named \textbf{\emph{Adaptive Group Lasso}}.
They approximated \(f_{j}\)'s using normalised B-spline bases, so that a
linear combination of B-spline basis functions is used to represent an
individual nonparametric component \(f_{j}\). The proposed method is a
generalisation of Adaptive Lasso method \autocite{Zou2006} to the Group
Lasso method \autocite{Yuan2006}.

When the nonparametric additive model in Equation~\ref{eq-add} is
considered, an obvious possibility is that some of the additive
components (i.e.~\(f_{j}\)'s) are being linear. For example, recall the
electricity demand forecasting problem \autocite{HF2010,FH2012}, where
some of the calendar effects are included into the model as linear
variables, whereas lagged temperature and lagged demand variables are
included using nonlinear additive components. Such situations suggest
the use of \emph{semi-parametric partially linear additive models} that
can be mathematically represented as \[
 y_{i} = \sum_{j=1}^{p} {f_{j}(x_{ij})} + \sum_{k=1}^{q} {w_{ik}\beta_{k}} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(\bm{x}_{j}\)'s, \(j = 1, \dots, p\), are a set of predictors
that enter the model as nonparametric components, whereas
\(\bm{w}_{k}\)'s, \(k = 1, \dots, q\) are another set of predictors that
are included as linear components. While several studies have assumed
that the number of nonparametric components are fixed, and performed
variable selection only among the linear components of the model
\autocite{Lian2012,Guo2013,Liu2011}, \textcite{Wang2014} introduced a
methodology for selecting both linear and nonlinear components
simultaneously, in the context of correlated, longitudinal data. They
proposed the use of a \textbf{\emph{Penalised Quadratic Inference
Function (PQIF) with double SCAD penalties}} for variable selection and
model estimation, where the correlation structure of the data was
incorporated into the estimation method (see \textcite{Wang2014} for
details).

\hypertarget{time-series-aspect}{%
\subsubsection{Time Series Aspect}\label{time-series-aspect}}

It is worthwhile to briefly mention that there are extensions of the
penalisation methods discussed above, which have specifically proposed
to take the autocorrelation and lag structures in time series data into
account.

\textcite{Wang2007} proposed an extension of the LASSO method for
Regression with Autoregressive Error (REGAR) models. \textcite{Park2013}
and \textcite{Konzen2016} proposed modifications to Adaptive Lasso
method to incorporate the lag structures presented in Autoregressive
Distributed Lag (ADL) models into the variable selection and estimation
methodology. The \textbf{\emph{Ordered Lasso}} was introduced by
\textcite{Tibshirani2016} to deal with time-lagged regression problems,
where we forecast the response value at time \(t\) using the predictor
values from \(K\) previous time points, assuming that the magnitude of
regression coefficients decreases as the lagged predictor moves away
from time \(t\).

However, it is important to note that all the models considered in the
above time series related work are linear; none of them include
nonparametric terms.

\hypertarget{sec-Index}{%
\subsection{Index Models}\label{sec-Index}}

\hypertarget{single-index-model}{%
\subsubsection{Single Index Model}\label{single-index-model}}

The nonparametric additive model (Equation~\ref{eq-add}) estimates the
relationship between the response and the predictors using a sum of
univariate nonlinear functions corresponding to each individual
predictor variable. Hence, it is incapable of handling the interactions
among the predictors, which are ubiquitous in real-world problems
\autocite{Zhang2008}.

As a remedy, the \textbf{\emph{Single Index Model}}, a generalisation of
the linear regression model where the linear predictor is replaced by a
semi-parametric component, is popularly being used in the literature
\autocite{Radchenko2015}. Let \(y_{i}\) be the response, and
\(\bm{x}_{i}\) be a \(p\)-dimensional predictor vector. Then the single
index model can be written as \[
  y_{i} = g \left ( \bm{\alpha}^{T} \bm{x}_{i} \right ) + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(\bm{\alpha}\) is a \(p\)-dimensional vector of unknown
coefficients (i.e.~parameters), \(g\) is an unknown univariate function,
and \(\varepsilon_{i}\) is the random error
\autocite{Stoker1986,Hardle1993}. The linear combination
\(\bm{\alpha}^{T} \bm{x}_{i}\) is called the \emph{index}. Single index
model is viewed as a viable alternative to the additive model since it
offers more flexibility and interpretability \autocite{Radchenko2015}.

According to \textcite{Radchenko2015}, single index models have widely
been used in scenarios with fairly low and moderate dimensionality,
where the corresponding estimation and variable selection techniques are
not directly applicable to the high-dimensional setting. The error sum
of squares of the model being non-convex with respect to index
coefficients, is the main reason behind the existence of very limited
number of methods in high-dimensional case \autocite{Radchenko2015}. For
an extensive summary of available methods, we refer to
\textcite{Radchenko2015}.

\hypertarget{sec-multi-index}{%
\subsubsection{Multiple Index Models}\label{sec-multi-index}}

\textbf{Projection Pursuit Regression}

\textcite{Friedman1981} introduced \textbf{\emph{Projection Pursuit
Regression (PPR)}} by extending the nonparametric additive model
(Equation~\ref{eq-add}) to enable the modelling of interactions among
predictor variables. On the other hand, PPR is an extension of the
single index model to an \textbf{\emph{Additive Index Model }}, given by
\[
  y_{i} = \sum_{j=1}^{q} {g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(y_{i}\) is the response, \(\bm{x}_{i}\) is a
\(p\)-dimensional predictor vector,
\(\bm{\alpha}_{j} = \left ( \alpha_{j1}, \dots, \alpha_{jp} \right )^{T}, j = 1, \dots, q\)
are \(p\)-dimensional projection vectors (or vectors of \emph{``index
coefficients''}), \(g_{j}\)'s are unknown univariate functions, and
\(\varepsilon_{i}\) is the random error.

Instead of estimating a single index, PPR estimates multiple indices and
connects them to the response through a sum of univariate nonlinear
functions. These indices are constructed through a \emph{Projection
Pursuit (PP)} \autocite{Kruskal1969,Friedman1974} algorithm, which is
considered to be ``interesting'' low-dimensional projections of a
high-dimensional feature space, obtained through the maximisation of an
appropriate objective function or a ``projection index''
\autocite{Huber1985}.

According to \textcite{Zhang2008}, PPR increases the power of additive
models in high-dimensional settings, but it has two major drawbacks.
Firstly, since PP increases the freedom of the additive model, it tends
to overfit in a situation, where there are a lot of unimportant
predictors. Secondly, the interpretation of the model estimated by PPR
will be troublesome as many non-zero elements will be present in each
projection vector \(\bm{\alpha}_{j}\). To overcome these issues,
\textcite{Zhang2008} introduced an \(\ell_{1}\) regularised projection
pursuit algorithm, where the resultant regression model is named as
\textbf{\emph{Sparse Projection Pursuit Regression}} (SpPPR). In SpPPR,
an \(\ell_{1}\) penalty (i.e.~a LASSO penalty) on index coefficients is
added to the cost function (the squared error) at each iteration of the
PP, thereby performing variable selection and model estimation
simultaneously. See \textcite{Zhang2008} for more details.

Although \textcite{Zhang2008} claimed that the SpPPR algorithm can
detect important predictors even in a noisy data set, our experiments
show that it is not particularly scalable for large data sets with both
higher number of predictors and observations. \newline

\textbf{Group-wise Additive Index Model}

Even though PPR introduces flexibility and the ability to model
interactions among predictors into additive models, the indices obtained
through PPR contain all the predictors at hand. Hence, even with a
variable selection mechanism like SpPPR \autocite{Zhang2008}, PPR
creates indices possibly by mixing heterogeneous variables in a single
linear combination, making very little sense in terms of
interpretability \autocite{Masselot2022}.

Typically, in many real-world problems, natural groupings can be
identified in predictor variables. For example, naturally interacting
variables can be grouped together, such as several lags of a predictor,
weather related variables, and genes or proteins that are grouped by
biological pathways in a biological study
\autocite{Masselot2022,Wang2015}.

This suggests the use of a \textbf{\emph{Group-wise Additive Index Model
(GAIM)}}, which can be written as \[
  y_{i} = \sum_{j = 1}^{p} g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \varepsilon_{i}, \quad i = 1, \dots, n, 
\] where \(y_{i}\) is the univariate response,
\(\bm{x}_{ij} \in \mathbb{R}^{l{j}}\), \(j = 1, \dots, p\) are naturally
occurring \(p\) groups of predictors, which are \(p\) non-overlapping
subsets of \(\bm{x}_{i}\) - the vector of all predictors,
\(\bm{\alpha}_{j}\) is a \(l_{j}\)-dimensional vector of index
coefficients corresponding to the index
\(h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}\), \(g_{j}\) is an unknown
(possibly nonlinear) component function, and \(\varepsilon_{i}\) is the
random error, which is independent of \(\bm{x}_{i}\)
\autocite{Wang2015-mp,Masselot2022}.

Since GAIM uses groups of predictors that are naturally or logically
belonging together to construct indices, such derived indices will be
more expressive and interpretable. However, at the same time, this
introduces a certain level of subjectivity into the model formulation as
different users can group the available predictors in different ways
based on different logical reasoning.

In this paper, our aim is to reduce that subjectivity induced by
personal judgment or domain expertise. Hence, we propose a methodology
that injects more objectivity into the estimation of multiple index
models by algorithmically grouping predictors into indices, resulting in
a model with a higher predictive accuracy. \newline

\textbf{Constrained Group-wise Additive Index Model}

The \textbf{\emph{Constrained Group-wise Additive Index Model (CGAIM)}}
was proposed by \textcite{Masselot2022} for constructing comprehensive
and easily interpretable indices from a large set of explanatory
variables. The model of interest is a \emph{semi-parametric group-wise
additive index model} given by \[
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(y_{i}\) is the univariate response, \(\beta_{0}\) is the
model intercept, \(\bm{x}_{ij} \in \mathbb{R}^{l_{j}}\),
\(j = 1, \dots, p\) are naturally occurring \(p\) groups of predictor
vectors (i.e.~it is assumed that the predictor groupings are known in
advance), which are \(p\) subsets of \(\bm{x}_{i}\) - the
\(q\)-dimensional vector of all predictors entering indices,
\(\bm{\alpha}_{j}\) is the vector of index coefficients corresponding to
the index \(h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}\), and \(g_{j}\) is
the corresponding nonlinear link function (possibly estimated by a
spline). The additional predictor variables that are helpful in
predicting \(y_{i}\), but do not enter any of the indices are two-fold:
a covariate that relates to the response through a nonlinear function
\(f_{k}\), denoted by \(w_{ik}\), and the vector of linear covariates
denoted by \(\bm{u}_{i}\).

This is an extension of the GAIM that allows to impose constraints on
the index coefficients as well as on the nonlinear link functions. In
CGAIM, linear constraints of the form
\(\bm{C}_{j}\bm{\alpha}_{j} \ge 0\) can be imposed on the index
coefficients, where \(\bm{C}_{j} \in \mathbb{R}^{d_{j} \times l_{j}}\),
and \(d_{j}\) is the number of constraints. Moreover, shape constraints
such as monotonicity, convexity or concavity can be imposed on the
nonparametric functions. This modification allows to incorporate prior
knowledge or operational requirements into the model estimation.

First, considering only the additive index part of the model, and given
\((y_{i}, x_{i1}, \dots, x_{iq}), \quad i = 1, \dots, n\) be the
observed data, where the \(q\) predictors are grouped into \(p\) groups,
the estimation problem of the CGAIM can be formulated as
\begin{equation}\protect\hypertarget{eq-3}{}{
\begin{aligned}
  \min_{\bm{\alpha}, \beta_{0}} \quad & \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij})\right ]^{2}, \\
  \text{s.t.} \quad & \bm{C}\bm{\alpha} \ge 0, \quad g_{j} \in m,
\end{aligned}
}\label{eq-3}\end{equation} where
\(\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}\),
\(\beta_{0}\) is the model intercept,
\(\bm{C} \in \mathbb{R}^{d \times q}\), \(d\) is the number of
constraints on the index coefficients vector \(\bm{\alpha}\), and \(m\)
is a shape constraint imposed on \(g_{j}\) \autocite{Masselot2022}.

Notice that \(\bm{\alpha}_{j}\)s behave non-linearly in
Equation~\ref{eq-3}, and hence, this is a non-linear least squares
problem. Accordingly, \textcite{Masselot2022} introduced an efficient
iterative algorithm for estimating the CGAIM based on
\textbf{\emph{Sequential Quadratic Programming}} (SQP), one of the most
successful techniques for solving nonlinear constrained optimisation
problems \autocite{Boggs1995}. For details of the CGAIM algorithm refer
to \textcite{Masselot2022}.

\hypertarget{mathematical-optimisation-for-variable-selection}{%
\subsection{Mathematical Optimisation for Variable
Selection}\label{mathematical-optimisation-for-variable-selection}}

\hypertarget{mathematical-optimisation}{%
\subsubsection{Mathematical
Optimisation}\label{mathematical-optimisation}}

\emph{Optimisation} plays a major role in both decision science and
physical systems evaluation. \textbf{\emph{Mathematical Optimisation}}
or \textbf{\emph{Mathematical Programming}} can be defined as the
minimisation (or maximisation) of a function subject to restrictions on
the unknowns/parameters of that function \autocite{Nocedal2006}. Hence,
a mathematical optimisation problem can be written as
\begin{equation}\protect\hypertarget{eq-opt}{}{
\begin{aligned}
  \min_{\bm{x}} \quad & f_{0}(\bm{x})\\
  \text{s.t.} \quad & f_{i}(\bm{x}) \le b_{i}, \quad i = 1, \dots, m
\end{aligned}
}\label{eq-opt}\end{equation} where the vector of unknowns or parameters
of the problem is given by
\(\bm{x} = \left ( x_{1}, \dots, x_{n} \right )^{T}\), the
\emph{objective function} is denoted by
\(f_{0} : \mathbb{R}^{n} \rightarrow \mathbb{R}\), the \emph{constraint
functions} are given by
\(f_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}\), \(i = 1, \dots, m\),
and the bounds of the constraints are denoted by
\(\bm{b} = \left (b_{1}, \dots, b_{m} \right )^{T}\). A vector of values
\(\bm{x^{*}}\) that results in the smallest value for the objective
function among all vectors that satisfy the stated constraints, is
called the \emph{optimal} value or the \emph{solution} to the problem
\autocite{Boyd2004}. After mathematically formulating the optimisation
problem as above (Equation~\ref{eq-opt}), an appropriate
\emph{optimisation algorithm} is used to obtain the solution
\(\bm{x^{*}}\) \autocite{Nocedal2006}.

Based on the form of the objective function and the constraints, various
types of optimisation problems are identified.

An optimisation problem is known as a \textbf{\emph{Linear Programming}}
(LP) when both the objective function and the constraints in
Equation~\ref{eq-opt} (i.e.~all \(f_{i}\), \(i = 0, \dots, m\)) are
linear. Hence, a LP can be written as
\begin{equation}\protect\hypertarget{eq-lp}{}{
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
}\label{eq-lp}\end{equation} where \(\bm{x}\) is the vector that
contains the parameters to be optimised, and
\(\bm{a_{0}} \in \mathbb{R}^{n}\) is the vector of coefficients of the
objective function. The matrix of coefficients in the constraints is
denoted by \(\bm{A} \in \mathbb{R}^{m \times n}\), and \(\bm{b}\) is the
vector containing the upper bounds of the constraints. All LPs are
\emph{convex} optimisation problems \autocite{Theusl2020}.

The LP problem given in Equation~\ref{eq-lp} can be generalised to
involve a quadratic term in the objective function, in which case it is
called a \textbf{\emph{Quadratic Programming}} (QP). A QP can be written
as \[
\begin{aligned}
  \min_{\bm{x}} \quad & \frac{1}{2} \bm{x}^{T} \bm{Q_{0}} \bm{x} + \bm{a_{0}}^{T}\bm{x}\\
  \text{s.t.} \quad & \bm{A}\bm{x} \le \bm{b},
\end{aligned}
\] where \(\bm{Q_{0}} \in \mathbb{R}^{n \times n}\). Unless the matrix
\(\bm{Q_{0}}\) is positive semi-definite, a QP is non-convex
\autocite{Theusl2020}.

If a linear objective function is minimised over a \emph{convex cone},
such an optimisation problem is called a \textbf{\emph{Conic
Programming}} (CP), which can be written as \[
\begin{aligned}
  \min_{\bm{x}} \quad & \bm{a_{0}}^{T}\bm{x} \\
  \text{s.t.} \quad & \bm{A}\bm{x} + s = \bm{b}, \quad s \in \mathcal{K},
\end{aligned}
\] where \(\mathcal{K}\) denotes a nonempty closed convex cone. CPs are
designed to model convex optimisation problems \autocite{Theusl2020}.

If we restrict some of the unknowns/parameters in an optimisation
problem to take only integer values, then that optimisation problem is
called a \textbf{\emph{Mixed Integer Programming}} (MIP). For example,
if we constraint \(x_{k} \in \mathbb{Z}\) for at least one \(k\),
\(k \in \{1, \dots, n\}\) in the optimisation problem given by
Equation~\ref{eq-opt}, then the optimisation problem becomes a MIP. If
all the unknowns of an optimisation problem are constrained to be
integers, such a problem is referred to as a pure \textbf{\emph{Integer
Programming}} (IP), whereas if all the unknowns are bounded between zero
and one (i.e.~\(\bm{x} \in \{ 0, 1 \}^{n}\)), the optimisation problem
is referred to as a \textbf{\emph{Binary (Integer) Programming}}
\autocite{Theusl2020}. MIPs are hard to solve as they are non-convex due
to the integer constraints. However, a growth in the number of
commercial as well as non-commercial MIP solvers has made it possible to
solve MIP problems conveniently and directly.

\hypertarget{variable-selection}{%
\subsubsection{Variable Selection}\label{variable-selection}}

Mathematical optimisation is fundamentally important in statistics, as
many statistical problems including regression, classification, and
other types of estimation/approximation problems can be re-interpreted
as optimisation problems \autocite{Theusl2020}. Thus, the problem of
variable selection - one of the prolonged interests of statisticians,
has also benefited from using optimisation concepts, particularly MIP
and convex optimisation, in the recent past.

For example, \textcite{Bertsimas2016} used a mixed integer optimisation
procedure to solve the classical best subset selection problem in a
linear regression. They developed a discrete optimisation method by
extending modern first-order continuous optimisation techniques. The
method can produce near-optimal solutions that would serve as warm
starts for a MIP algorithm, which would choose the best \(k\) features
out of \(p\) predictors. Similarly, \textcite{Hazimeh2020} developed
fast and efficient algorithms based on coordinate descent and local
combinatorial optimisation to solve the same best subset selection (or
\(\ell_{0}\)-regularised least squares) problem through re-formulating
local combinatorial search problems as structured MIPs.

Furthermore, \textcite{Hazimeh2023} proposed a group-wise variable
selection methodology, based on discrete mathematical optimisation,
which is applicable to both \(\ell_{0}\)-regularised linear regression
and nonparametric additive models in a high-dimensional setting. They
formulated the group \(\ell_{0}\)-based estimation problem as a
\textbf{\emph{Mixed Integer Second Order Cone Programming}} (MISOCP),
and proposed a new customised Branch-and-Bound (BnB) algorithm
\autocite{Land1960,Little1963} to obtain the global optimal solution to
the MISOCP.

Through the study of above literature, we noticed that the mathematical
optimisation based algorithms reduce computational cost of variable
selection procedures in high-dimensional settings. This is largely due
to the availability of efficient commercial solvers such as
\emph{Gurobi} and \emph{CPLEX}. This motivated us to focus on a
mathematical optimisation based procedure for developing our variable
selection methodology.

\hypertarget{sec-SMI}{%
\section{Sparse Multiple Index Model}\label{sec-SMI}}

In this section, we develop a \textbf{\emph{Sparse Multiple Index
Model}} (hereafter referred to as SMI Model) to establish an objective
and a principled methodology for estimating high-dimensional
nonparametric additive index models, highlighting predictor grouping
along with optimal predictor selection.

\hypertarget{sec-model}{%
\subsection{The SMI Model}\label{sec-model}}

The model of interest is a semi-parametric additive index model, which
can be written as \begin{equation}\protect\hypertarget{eq-semipara}{}{
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
}\label{eq-semipara}\end{equation} where \(y_{i}\) is the univariate
response, \(\beta_{0}\) is the model intercept,
\(\bm{x}_{ij} \in \mathbb{R}^{l_{j}}\), \(j = 1, \dots, p\) are \(p\)
subsets of all the predictors entering indices, \(\bm{\alpha}_{j}\) is a
vector of index coefficients corresponding to the index
\(h_{ij} = \bm{\alpha}_{j}^{T}\bm{x}_{ij}\), \(g_{j}\) is a nonlinear
link function (possibly estimated by a spline). Note that we also allow
for the inclusion of predictors that do not enter any of the indices.
These additional predictors are two-fold: a covariate \(w_{ik}\) that
relates to the response through the nonlinear function \(f_{k}\),
\(k = 1, \dots, d\), and linear covariates denoted by \(\bm{u}_{i}\).

We make three main assumptions as follows to define the \textbf{SMI
Model}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The number of indices (i.e.~the number of subsets of predictors) \(p\)
  is unknown;
\item
  The predictor grouping among indices is unknown; and
\item
  No predictor enters more than one index (i.e.~overlapping of
  predictors among indices is not allowed).
\end{enumerate}

These assumptions further imply that the index coefficient vectors
\(\bm{\alpha}_{j}\)s and the nonlinear link functions \(g_{j}\)s are
also unknown, and will need to be estimated.

One of the key features of the proposed SMI model is to allow for zero
index coefficients for predictors, so that the predictors with zero
coefficients are dropped out from the model, achieving variable
selection. The other key feature of the SMI model is its flexibility in
allowing a variable number of indices, ranging from \(1\) (i.e.~all
\(q\) predictors are passed to a single index) to \(q\) (i.e.~each
predictor goes into a separate index). Hence, both the Single Index
Model and the Additive Model are special cases of the proposed SMI
Model.

\hypertarget{optimisation-problem-formulation}{%
\subsection{Optimisation Problem
Formulation}\label{optimisation-problem-formulation}}

Let \(q\) be the total number of predictors entering \(p\)
non-overlapping subsets of size \(l_{j}\), \(j = 1, \dots, p\)
(i.e.~\(\sum_{j = 1}^{p} l_{j} = q\)). The optimisation problem we seek
to address is of the form below, where the sum of the squared error of
the model (Equation~\ref{eq-semipara}) is minimised together with an
\(\ell_{0}\) penalty term and an \(\ell_{2}\) (ridge) penalty term:
\begin{equation}\protect\hypertarget{eq-smi}{}{
\begin{aligned}
  \min_{\beta_{0}, p, \bm{\alpha}, \bm{g}, \bm{f}, \bm{\theta}} \quad \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{ij}) - \sum_{k = 1}^{d}f_{k}(w_{ik}) - \bm{\theta}^{T}\bm{u}_{i}\right]^{2} \\
  + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{l_{j}}\bm{1}\left(\alpha_{jm} \neq 0\right) + \lambda_{2}\sum_{j = 1}^{p}\|\bm{\alpha}_{j}\|_2^2
\end{aligned}
}\label{eq-smi}\end{equation} where
\(\bm{\alpha} = \left [\bm{\alpha}_{1}^{T}, \dots, \bm{\alpha}_{p}^{T} \right ]^{T}\),
\(\bm{g} = \{g_{1}, g_{2}, \dots, g_{p}\}\),
\(\bm{f} = \{f_{1}, f_{2}, \dots, f_{d}\}\), \(\bm{1}(\cdot)\) is the
indicator function, \(\lambda_{0} > 0\) is a tuning parameter that
controls the number of selected predictors entering indices, and
\(\lambda_{2} \ge 0\) is another tuning parameter that controls the
strength of the additional shrinkage imposed on the estimated index
coefficients.

Applying an \(\ell_{2}\)-penalty in addition to the \(\ell_{0}\)-penalty
is motivated by related literature
\autocite{Hazimeh2020,Mazumder2022,Hazimeh2023}, where it is suggested
that the prediction performance of best-subset selection is enhanced by
the inclusion of an additional ridge penalty, especially when a low
signal-to-noise ratio (SNR) is present.

\hypertarget{miqp-formulation}{%
\subsection{MIQP Formulation}\label{miqp-formulation}}

To solve the optimisation problem in Equation~\ref{eq-smi}, we present a
big-M based \textbf{\emph{Mixed Integer Quadratic Programming}} (MIQP)
formulation: \begin{equation}\protect\hypertarget{eq-smi-mip}{}{
\begin{aligned}
  \min_{\beta_{0}, p, \bm{\alpha}, \bm{g}, \bm{f}, \bm{\theta}, \bm{z}} \quad & \sum_{i = 1}^{n}\left [ y_{i} - \beta_{0} - \sum_{j = 1}^{p}{g_{j}(\bm{\alpha}_{j}^{T}\bm{x}_{i})} - \sum_{k = 1}^{d} {f_{k}(w_{ik})} - \bm{\theta}^{T}\bm{u}_{i}\right ]^{2} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q} \alpha_{jm}^{2} \\
  \text{s.t.} \quad & \left |\alpha_{jm}\right | \le Mz_{jm} \quad \forall j, \forall m, \\
  & \sum_{j = 1}^{p}z_{jm} \le 1 \quad \forall m, \\
  & z_{jm} \in \{0, 1\}, \\
  & j = 1, \dots, p, \quad m = 1, \dots, q,
\end{aligned}
}\label{eq-smi-mip}\end{equation} where \(p\) is the (unknown) number of
indices, \(\bm{x}_{i}\) is the \(q\)-dimensional vector of all
predictors entering indices, and
\(\bm{z} = \left (\bm{z}_{1}^{T}, \dots, \bm{z}_{p}^{T} \right )^{T}\),
\(\bm{z}_{j} = \left (z_{j1}, \dots, z_{jq} \right )^{T}\),
\(j = 1, \dots, p\) such that \(z_{jm} \in \{0, 1\}, m = 1, \dots, q\)
for all \(j\). In other words, we introduce a binary (i.e.~indicator)
variable corresponding to each predictor in each index. A pre-specified
\emph{big-M parameter} is denoted by \(M < \infty\), and it should be
sufficiently large. If \(\bm{\alpha^{*}}\) is the optimal solution to
the problem given in Equation~\ref{eq-smi-mip}, then the big-M parameter
should satisfy
\(\max \left (\left |\alpha_{jm}^{*}\right | \right ) \le M\), where
\(j \in \{1, \dots, p\}\), and \(m \in \{1, \dots, q\}\).

Notice that, here we formulate the MIQP to include all \(q\) predictors
in each index so that in this case, \(\bm{\alpha}_{j}\) is a
\(q\)-dimensional vector of index coefficients. However at the same
time, as mentioned earlier, we introduce a set of binary variables
corresponding to each predictor in each index, which serves two main
purposes: firstly, these binary variables are used to make the
\emph{``on-or-off''} decisions of the predictors in the model; secondly,
they contribute to decide which predictors belong to which index.

To further elaborate, first, the big-M constraints ensure that
\(\alpha_{jm}\) is zero if and only if \(z_{jm}\) is zero, and if
\(z_{jm} = 1\), then \(\left |\alpha_{jm}\right | \le M\). At the same
time, the \(\ell_{0}\)-penalty term
\(\lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm}\) influences some of
the binary variables \(z_{jm}\) to be zero, while the
\(\ell_{2}\)-penalty term
\(\lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q}\alpha_{jm}^{2}\) enforces
additional shrinkage on the estimated coefficients. Therefore, these
components together perform a variable selection.

Next, when a set of binary variables
\(\{z_{1m}, z_{2m}, \dots, z_{pm}\}\) corresponding to the \(m^{th}\),
\(m = 1, \dots, q\), predictor in all \(p\) indices is considered,
according to the constraint \(\sum_{j = 1}^{p}z_{jm} \le 1\), only one
or no binary variables in the set can take the value one, ensuring that
the \(m^{th}\) predictor does not repeat in more than one index. If all
the elements of the set are zero, then the \(m^{th}\) predictor will be
dropped out from the model.

Thus, our main contribution in this paper is two-fold. Firstly, we
propose a novel algorithm to objectively estimate a semi-parametric
additive index model, while contributing towards an estimated model with
a higher forecasting accuracy. Secondly, the proposed methodology will
contribute towards estimating a parsimonious model in a high-dimensional
setting, a crucial aspect of interpretability, even if the required
domain knowledge for selecting the optimal set of predictors is
unavailable.

\hypertarget{estimation-algorithm}{%
\subsection{Estimation Algorithm}\label{estimation-algorithm}}

In this section, we show how to efficiently find a minimiser for the
problem given in Equation~\ref{eq-smi-mip}. Since the number of indices
\(p\), the vector of index coefficients \(\bm{\alpha}\), as well as the
set of nonparametric functions \(\bm{g}\) are unknown, it is
mathematically impossible to solve the above MIQP given in
Equation~\ref{eq-smi-mip} directly. Hence, we propose an iterative
algorithm to solve the problem.

\hypertarget{sec-step1}{%
\subsubsection{Initialising the Index Structure and Index
Coefficients}\label{sec-step1}}

First, we need to provide an initialisation for the index structure
(i.e.~number of indices \(p\) and the grouping of predictors among
indices) and the index coefficients (\(\bm{\alpha}\)) of the model in
order to start solving the MIQP given in Equation~\ref{eq-smi-mip}.

Based on several pre-experiments on the new algorithm, we propose five
alternative methods for initialising the SMI Model as follows.

\textbf{1. ``PPR'' - Projection Pursuit Regression Based
Initialisation:}

As discussed in Section~\ref{sec-Index}, Projection Pursuit Regression
model is a multiple index model, where each index consists of all the
available predictors. Since in SMI Model we assume that there are no
overlapping indices, it is impossible to use an estimated PPR model
directly as a starting model for the algorithm. Thus, we follow the
steps presented below to come up with a feasible initialisation for the
index structure and the index coefficients.

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  Scale all the variables of the data set by dividing each variable by
  its standard deviation (so that it is possible to compare the
  estimated coefficients among predictors).
\item
  Fit a PPR model and obtain estimated index coefficients. (The user can
  decide the number of indices to be estimated through \emph{num\_ind};
  we use \emph{num\_ind = 5} as the default value.)
\item
  Calculate a threshold as
  \(threshold = 0.1 \times \max(\text{PPR coefficients})\).
\item
  Set to zero all coefficients that fall below the calculated threshold.
\item
  For predictors appearing in multiple indices, assign them to the index
  with the maximum coefficient and zero out their coefficients in other
  indices.
\item
  After performing the above steps i-v, if any originally estimated
  index has all \(\bm{z}\)ero coefficients, it will be excluded from the
  model.
\end{enumerate}

Now, the index structure and the index coefficients obtained through the
above steps are considered to be a feasible initialisation for the
proposed algorithm. Once the optimal SMI Model is obtained through the
algorithm, each index coefficient will be back-transformed into the
original scale of the respective predictor variable, reversing the
scaling effect applied at the beginning.

\textbf{2. ``Additive'' - Nonparametric Additive Model Based
Initialisation:}

As mentioned previously in Section~\ref{sec-model}, the nonparametric
additive model is a special case of the SMI Model, where the number of
indices equals the number of predictors entering indices (\(p = q\))
(i.e.~each index contains only one predictor). Hence, it is a feasible
starting point for our algorithm.

\textbf{3. ``Linear'' - Linear Regression Based Initialisation:}

In this option, we first regress the response variable on the predictors
using a multiple linear regression. Then, we construct a single index
(i.e.~\(p = 1\)) using the estimated regression coefficients as the
index coefficients of the predictors. Since single index model is also a
special case of the SMI Model, this will be a candidate starting point.

\textbf{4. ``Multiple'' - Selecting an Initial Model by Comparing
Multiple Models:}

Through our pre-experiments on the new algorithm, we identified that in
some situations, the final optimised SMI Model changes based on the
initialisation provided to the algorithm. Hence, for this initialisation
option, we consider several different models as initialisations,
optimise the SMI Model for each of them, and pick the initial model that
results in the lowest loss for the MIQP problem.

Here, users can decide on the number of models to be considered
(\emph{num\_models}) as well as the the number of indices for all models
(\emph{num\_ind} - same for all models). We use \emph{num\_models = 5}
and \emph{num\_ind = 5} as default values.

\textbf{5. ``User Input'' - User Specified Initialisation:}

This option allows users to provide their desired initialisation for the
algorithm by specifying the number of indices, the grouping of
predictors among indices and the initial index coefficients. This option
provides the freedom for users to utilise their domain expertise or
prior knowledge in initialising the algorithm.

In all of the above initialisation options, once the estimate for
\(\bm{\alpha}\) is obtained, the estimated initial index coefficients
for each index (\(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j, init}\)) are
scaled to have unit norm to ensure identifiability.

The characteristics and the performance of the proposed algorithm differ
based on the chosen initialisation options, depending on the application
scenario. Further insights into these aspects will be discussed in
Section~\ref{sec-simulation}.

\hypertarget{sec-step2}{%
\subsubsection{Estimating Nonlinear Functions}\label{sec-step2}}

Once we have an estimate for \(\bm{\alpha}\), estimating the SMI Model
is equivalent to estimating a GAM as \[
  y_{i} = \beta_{0} + \sum_{j = 1}^{p}g_{j}(\hat{h}_{ij}) + \sum_{k = 1}^{d}f_{k}(w_{ik}) + \bm{\theta}^{T}\bm{u}_{i} + \varepsilon_{i}, \quad i = 1, \dots, n,
\] where \(y_{i}\) is the response, and
\(\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{i}\) is the estimated
index.

The R packages \textbf{\emph{mgcv}} \autocite{Wood2011} and
\textbf{\emph{gam}} \autocite{Hastie2023}, for example, can be used to
fit GAMs.

\hypertarget{sec-step3}{%
\subsubsection{Updating the Index Structure and Index
Coefficients}\label{sec-step3}}

We obtain the updated index coefficients \(\bm{\alpha}^{new}\) through a
MIQP: \begin{equation}\protect\hypertarget{eq-smi-update}{}{
\begin{aligned}
  \min_{\bm{\alpha}^{new}, \bm{z}^{new}} & (\bm{\alpha}^{new} - \bm{\alpha}^{old})^{T}\bm{V}^{T}\bm{V}(\bm{\alpha}^{new} - \bm{\alpha}^{old}) - 2(\bm{\alpha}^{new} - \bm{\alpha}^{old})^{T}\bm{V}^{T}\bm{r} + \lambda_{0}\sum_{j = 1}^{p}\sum_{m = 1}^{q}z_{jm}^{new} + \lambda_{2}\sum_{j = 1}^{p}\sum_{m = 1}^{q}\alpha_{jm}^{(new)2} \\
  \text{s.t. } & \left |\alpha_{jm}^{new}\right | \le Mz_{jm}^{new} \quad \forall j, \forall m,\\
  & z_{jm}^{new} \in \{0, 1\}, \\
  & \sum_{j = 1}^{p}z_{jm}^{new} \le 1 \quad \forall m, \\
  & j = 1, \dots, p, \quad m = 1, \dots, q,
\end{aligned}
}\label{eq-smi-update}\end{equation} where \(\bm{\alpha}^{old}\) is the
current value of \(\bm{\alpha}\), and \(z_{jm}^{new}\) are the updated
set of binary variables to be estimated. \(\bm{V}\) is the matrix of
partial derivatives of the right hand side of
Equation~\ref{eq-semipara}, with respect to \(\bm{\alpha}_{j}\). The
\(i^{th}\) line of \(\bm{V}\) contains
\(\left [ \bm{v}_{i1}, \dots, \bm{v}_{ip} \right ]\), where
\(\bm{v}_{ij} = \bm{x}_{i}g_{j}^{'}(h_{ij})\). The current residual
vector, which contains
\(r_{i} = y_{i} - \beta_{0} - \sum_{j = 1}^{p}g_{j}(\bm{\alpha}_{j}^{(old)T}\bm{x}_{i})\),
is denoted by \(\bm{r}\). It is important to note that the additional
covariates \(w_{ik}\) and \(\bm{u}_{i}\) do not step in to the process
of updating \(\bm{\alpha}_{j}\), because they are constants with respect
to \(\bm{\alpha}_{j}\), and thus they disappear from \(\bm{V}\).

Similar to the explanation given by \textcite{Masselot2022}, the MIQP
objective function in above Equation~\ref{eq-smi-update} ignores the
Hessian (or the matrix of second derivatives of
Equation~\ref{eq-semipara}, with respect to \(\bm{\alpha}_{j}\)), and
considers only the matrix of first derivatives, which is a quasi-Newton
step. The quasi-Newton Method is an alternative to the Newton's Method,
avoiding the calculation of the Hessian to circumvent its computational
burden \autocite{Peng2022}. Therefore, the \(\bm{\alpha}\) updating step
given in above Equation~\ref{eq-smi-update} is assured to be in a
descent direction.

When \(\bm{\alpha}^{new}\) is obtained, if any of the estimated
individual index coefficient vectors \(\bm{\alpha}_{j}^{new}\) contains
all \(\bm{z}\)eros (i.e.~zero index), such indices will be dropped out
from the model. Furthermore, similar to Section~\ref{sec-step1}, once
the new estimate \(\bm{\alpha}^{new}\) is obtained, we scale each
estimated index coefficient vector
\(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j}^{new}\) to have unit norm.

The algorithm alternates updating the index coefficients \(\bm{\alpha}\)
and estimating nonlinear functions \(\bm{g}\) with the updated
\(\bm{\alpha}\) until meeting one of the three criteria: (i) the
reduction ratio of the objective (loss) function value in
Equation~\ref{eq-smi-mip}, calculated between consecutive iterations,
reaches a pre-specified convergence tolerance; (ii) the loss increases
consecutively for three iterations; or (iii) the maximum number of
iterations is reached. The selection of convergence tolerance and
maximum iterations depends on the specific problem or data. In the
empirical applications in Section~\ref{sec-application}, we used a
convergence tolerance of \(0.001\) and a maximum of \(50\) iterations,
stopping at the first criterion reached.

Next, we consider changing the index structure of the model to exploit
any benefits in terms of further minimising the loss function in
Equation~\ref{eq-smi-mip}. As indices can be automatically reduced by
dropping zero indices in each optimisation iteration, this step focuses
on potential index additions to the current model. Specifically, we
consider adding a new index to the current model by identifying dropped
predictors. If applicable, a new index is constructed with these dropped
predictors, and the alternating updating process in the previous step is
repeated. This increment step continues until one of these termination
criteria is met: (i) the number of indices reaches \(q\), selecting the
final model as output; (ii) loss increases after the increment,
selecting the previous iteration model as the final SMI model; or (iii)
the solution maintains the same number of indices as the previous
iteration, and the absolute difference of index coefficients between two
successive iterations is not larger than a pre-specified tolerance,
choosing the model with the smaller loss as the final SMI model.

Note that, to obtain an estimated model with the best possible
forecasting accuracy, it is important to select appropriate values for
the non-negative penalty parameters \(\lambda_{0}\) and \(\lambda_{2}\).
One possible way to do this is to estimate the model over a grid of
possible values for \(\lambda_{0}\) and \(\lambda_{2}\), and then select
the combination that yields the lowest loss function value. Moreover, it
is also crucial to choose a suitable value for the big-M parameter, as
the strength of the MIP formulation depends on the choice of a good
lower bound \autocite{Bertsimas2016}. According to
\textcite{Hazimeh2023}, several methods have been used to select \(M\)
in practice. For a description on estimating \(M\) in a linear
regression setting, refer to \textcite{Bertsimas2016}.

The following \textbf{\emph{Algorithm 1}} summarises the key steps of
the SMI Modelling algorithm. \newline

\textbf{\emph{Algorithm 1: SMI Modelling Algorithm}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialise index structure and index coefficients \(\bm{\alpha}\):\\
  Initialise \(p\), predictor grouping among indices, and obtain
  \(\bm{\alpha}^{init}\) using one of the five options in
  Section~\ref{sec-step1}. Then scale each
  \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j}^{init}\) to have unit norm.
\item
  Estimate nonlinear functions \(g_{j}\)s:\\
  Estimate \(g_{j}\)s using a GAM taking \(y_{i}\) as the response,
  \(\hat{h}_{ij} = \hat{\bm{\alpha}}_{j}^{T}\bm{x}_{i}\)s as predictors.
\item
  Update index coefficients \(\bm{\alpha}\):\\
  Estimate updated value \(\bm{\alpha}^{new}\) through the MIQP in
  Equation~\ref{eq-smi-update}, and scale each
  \(\hat{\bm{\alpha}}_{j} = \bm{\alpha}_{j}^{new}\) to have unit norm.
\item
  Iterate steps 2 and 3 until convergence, loss increase for three
  consecutive iterations, or reaching the maximum iterations.
\item
  Update index structure:\\
  Include a new index consisting of dropped predictors if applicable,
  and proceed to step 4. Otherwise, terminate the algorithm.
\item
  Iterate step 5 with increased number of indices \(p\):\\
  Increase \(p\) by one in each iteration of step 5 until meeting one of
  the termination criteria below.

  \begin{itemize}
  \tightlist
  \item
    The number of indices in the iteration reaches \(q\); select the
    final fitted model as output.
  \item
    Loss increases after the increment; select previous iteration model
    as the final SMI model.
  \item
    The solution maintains the same number of indices as the previous
    iteration, and the absolute difference of index coefficients between
    two successive iterations is not larger than a pre-specified
    tolerance; select the model with smaller loss as the final SMI
    model.
  \end{itemize}
\end{enumerate}

Throughout the experiments in the paper, we use \(M = 10\), a
convergence tolerance of \(0.001\), and a maximum of \(50\) iterations
in step 4 of Algorithm 1, and a convergence tolerance of \(0.001\) for
coefficients in step 6 of Algorithm 1, in estimating all the SMI Models.

\hypertarget{sec-simulation}{%
\section{Simulation Experiment}\label{sec-simulation}}

This section presents the results of a simple simulation experiment
designed to demonstrate the performance and characteristics of the
proposed SMI Modelling algorithm. Particularly, we try to investigate
how the estimated SMI Model varies depending on the initialisation (as
discussed in Section~\ref{sec-step1}) used.

\hypertarget{sec-datagen}{%
\subsection{Data Generation}\label{sec-datagen}}

\textbf{Generating predictor variables:}

First, we generate two series each of length 1205: \(x_{0}\), from a
uniform distribution on the interval \([0, 1]\), and \(z_{0}\), from
random normal distribution \(N(5, 4)\). Next, we construct lagged series
up to \(5^{th}\) lag of both \(x_{0}\) and \(z_{0}\). These current and
lagged series of \(x_{0}\) and \(z_{0}\)
(i.e.~\(\bm{x} = \{x_{0}, x_{1}, \dots, x_{5}\}\), and
\(\bm{z} = \{z_{0}, z_{1}, \dots, z_{5}\}\)) were taken as predictors in
the simulation experiment.

\textbf{Generating response variables:}

We generated two response variables \(y_{1}\) and \(y_{2}\), with two
different index structures: single-index and 2-index, and added a random
normal noise component with two different strengths as follows:

\begin{itemize}
\item
  Low noise level - \(N(\mu = 0, \sigma = 0.1)\):\\
  \(y_{1} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + \epsilon, \quad \epsilon\sim N(0, 0.01)\)\\
  \(y_{2} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + (0.35*x_{2} + 0.7*x_{5})^2 + \epsilon, \quad \epsilon\sim N(0, 0.01)\)
\item
  High noise level - \(N(\mu = 0, \sigma = 0.5)\):\\
  \(y_{1} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + \epsilon, \quad \epsilon\sim N(0, 0.25)\)\\
  \(y_{2} = (0.9*x_{0} + 0.6*x_{1} + 0.45*x_{3})^3 + (0.35*x_{2} + 0.7*x_{5})^2 + \epsilon, \quad \epsilon\sim N(0, 0.25)\)
\end{itemize}

Hence, the response \(y_{1}\) is constructed using a single index
consisting of the predictor variables \(x_{0}, x_{1}\), and \(x_{3}\),
whereas the other response \(y_{2}\) is constructed using two indices,
where the first index consists of the predictors \(x_{0}, x_{1}\), and
\(x_{3}\), and the second index consists of \(x_{2}\) and \(x_{5}\).
Neither the variable \(x_{4}\) nor any of the \(z\) variables were used
in generating \(y_{1}\) and \(y_{2}\).

Once the data set is generated, the first five observations are
discarded due to the missing values introduced by lagged variables,
leaving a data set of 1200 observations. We use the first 1000
observations as the training set, while the remaining 200 observations
are kept aside as the test set for evaluating the estimated models.

\hypertarget{sec-exp}{%
\subsection{Experiment Setup}\label{sec-exp}}

We estimated SMI Models through the proposed algorithm for each of the
two response variables (the two ``true models''), using three different
sets of predictors as inputs. Our aim was to assess the algorithm's
capability to correctly pick the relevant predictor variables (and drop
the irrelevant predictors), and to estimate the correct index structure
of the true model.

The three different sets of predictors considered are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  All \(\bm{x}\) variables (denoted as all \(\bm{x}\));
\item
  All \(\bm{x}\) variables and all \(\bm{z}\) variables (denoted as all
  \(\bm{x}\) + all \(\bm{z}\));
\item
  A part of \(\bm{x}\) variables (i.e.~\(x_{0}, x_{1}\) and \(x_{2}\))
  and all \(\bm{z}\) variables (denoted as some \(\bm{x}\) + all
  \(\bm{z}\)).
\end{enumerate}

We applied the proposed SMI Modelling algorithm with each of the above
predictor combinations, for both variations of the responses concerning
the noise level. Moreover, we considered each of the first four
initialisation options that we discussed in Section~\ref{sec-step1}, for
each of the two responses.

\hypertarget{sec-sim-results}{%
\subsection{Results}\label{sec-sim-results}}

We summarise the results of the simulation experiment in
Table~\ref{tbl-simulation}. In the columns, we indicate the index
structure (i.e.~the number of indices and the predictor grouping among
indices) estimated by the proposed algorithm under each of the four
initialisation options. This is detailed for each combination explored,
considering response, input predictors, and noise levels.

In the simulation experiment, we did not perform any tuning for the
penalty parameters \(\lambda_{0}\) and \(\lambda_{2}\). Our experiments
indicated that, for this simple example, different values of penalty
parameters have a negligible impact on the estimated models. The default
values \(\lambda_{0} = 1\) and \(\lambda_{2} = 1\) were used in
estimating all the models presented in Table~\ref{tbl-simulation}.

\hypertarget{tbl-simulation}{}
\begin{table}[!h]
\caption{\label{tbl-simulation}Simulation experiment results. }\tabularnewline

\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}{llllll}
\toprule
\textbf{True Model} & \textbf{Predictors} & \textbf{PPR} & \textbf{Additive} & \textbf{Linear} & \textbf{Multiple}\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Low noise level}}\\
 &  & 1 index & 1 index & 1 index & 1 index\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all $\bm{x}$} & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$)\\
\cmidrule{2-6}
 &  & 1 index & 1 index & 1 index & 1 index\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all $\bm{x}$ + all $\bm{z}$} & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$)\\
\cmidrule{2-6}
 &  & 1 index & 3 indices & 1 index & 1 index\\
\cmidrule{3-6}
\multirow{-6}{*}[1\dimexpr\aboverulesep+\belowrulesep+\cmidrulewidth]{\raggedright\arraybackslash \hspace{1em}$y_{1}$} & \multirow{-2}{*}{\raggedright\arraybackslash some $\bm{x}$ + all $\bm{z}$} & ($x_{0}, x_{1}, z_{2}, z_{4}$) & ($x_{0}, x_{1}$) ($z_{4}$) ($z_{1}$) & ($x_{0}, x_{1}, z_{2}, z_{4}$) & ($x_{0}, x_{1}, z_{2}, z_{4}$)\\
\cmidrule{1-6}
 &  & 2 indices & 2 indices & 1 index & 2 indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all $\bm{x}$} & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$)\\
\cmidrule{2-6}
 &  & 2 indices & 2 indices & 1 index & 2 \vphantom{1} indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all $\bm{x}$ + all $\bm{z}$} & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$)\\
\cmidrule{2-6}
 &  & 3 indices & 2 indices & 1 index & 2 \vphantom{1} indices\\
\cmidrule{3-6}
\multirow{-6}{*}[1\dimexpr\aboverulesep+\belowrulesep+\cmidrulewidth]{\raggedright\arraybackslash \hspace{1em}$y_{2}$} & \multirow{-2}{*}{\raggedright\arraybackslash some $\bm{x}$ + all $\bm{z}$} & ($x_{0}, x_{1}, z_{2}$) ($x_{2}$) ($z_{3}, z_{4}$) & ($x_{0}, x_{1}, z_{4}$) ($x_{2}$) & ($x_{0}, x_{1}, x_{2}, z_{2}$) & ($x_{0}, x_{1}$) ($x_{2}, z_{2}, z_{3}$)\\
\cmidrule{1-6}
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{High noise level}}\\
 &  & 1 index & 2 indices & 1 index & 1 index\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all $\bm{x}$} & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}$) ($x_{3}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$)\\
\cmidrule{2-6}
 &  & 1 index & 2 indices & 1 index & 2 indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all $\bm{x}$ + all $\bm{z}$} & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) ($z_{0}$) & ($x_{0}, x_{1}, x_{3}$) & ($x_{0}, x_{1}, x_{3}$) ($z_{0}$)\\
\cmidrule{2-6}
 &  & 3 indices & 3 indices & 1 index & 3 indices\\
\cmidrule{3-6}
\multirow{-6}{*}[1\dimexpr\aboverulesep+\belowrulesep+\cmidrulewidth]{\raggedright\arraybackslash \hspace{1em}$y_{1}$} & \multirow{-2}{*}{\raggedright\arraybackslash some $\bm{x}$ + all $\bm{z}$} & ($x_{0}, x_{1}$) ($z_{1}$) ($z_{4}$) & ($x_{0}, x_{1}$) ($z_{1}$) ($z_{4}$) & ($x_{0}, x_{1}, z_{2}, z_{4}$) & ($x_{0}, x_{1}$) ($z_{0}, z_{4}$) ($z_{1}$)\\
\cmidrule{1-6}
 &  & 3 indices & 3 indices & 2 indices & 3 indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all $\bm{x}$} & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) ($x_{4}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) ($x_{4}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}$) ($x_{4}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$) ($x_{4}$)\\
\cmidrule{2-6}
 &  & 2 indices & 2 indices & 1 index & 2 indices\\
\cmidrule{3-6}
 & \multirow{-2}{*}{\raggedright\arraybackslash all $\bm{x}$ + all $\bm{z}$} & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}, z_{1}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}, z_{1}$) & ($x_{0}, x_{1}, x_{2}, x_{3}, x_{5}, z_{0}$) & ($x_{0}, x_{1}, x_{3}$) ($x_{2}, x_{5}$)\\
\cmidrule{2-6}
 &  & 3 indices & 2 indices & 1 index & 2 indices\\
\cmidrule{3-6}
\multirow{-6}{*}[1\dimexpr\aboverulesep+\belowrulesep+\cmidrulewidth]{\raggedright\arraybackslash \hspace{1em}$y_{2}$} & \multirow{-2}{*}{\raggedright\arraybackslash some $\bm{x}$ + all $\bm{z}$} & ($x_{0}, x_{1}, z_{0}, z_{3}$) ($x_{2}$) ($z_{1}, z_{4}, z_{5}$) & ($x_{0}, x_{1}, z_{0}, z_{1}, z_{3}, z_{4}$) ($x_{2}$) & ($x_{0}, x_{1}, x_{2}, z_{0}, z_{3}, z_{4}$) & ($x_{0}, x_{1}, z_{0}, z_{1}, z_{3}, z_{4}$) ($x_{2}$)\\
\bottomrule
\end{tabular}}
\end{table}

\textcolor{orange}{XQ: I think it's too lengthy to explain table 1 with 14 paragraphs. Please simplify the results analysis using no more than five paragraphs. Use one paragraph each to interpret results under high and low noise levels respectively, with other two paragraphs discussing the insights, algorithm benefits and application scenarios based on the results. I've made a few changes, but you need to do some further work on it. I provided more detailed comments below.}

\textcolor{blue}{NP: Comments addressed.}

At low noise level, in both the cases ``all \(\bm{x}\)'' and ``all
\(\bm{x}\) + all \(\bm{z}\)'', all four initialisations enable the
algorithm to estimate the correct index structure for both \(y_{1}\) and
\(y_{2}\), with an exception in ``Linear'' option for \(y_{2}\). The
``Linear'' option for \(y_{2}\) selects the correct variables, but fails
to identify the 2-index structure. This suggests that initialising the
algorithm with a higher number of indices might be more effective than a
lower number. In the case of ``some \(\bm{x}\) + all \(\bm{z}\)'', for
both \(y_{1}\) and \(y_{2}\), the models estimated under all four
initialisations include some noise variables. This indicates that when
the available predictors are insufficient to capture the data signal,
the algorithm might select irrelevant variables to make up for the
missing signal.

When the fitted models are evaluated, for \(y_{1}\), in both the cases
``all \(\bm{x}\)'' and ``all \(\bm{x}\) + all \(\bm{z}\)'', all four
initialisations resulted in a test MSE of \(\approx 0.01\), which is the
random squared error of the true model. This confirms the accuracy with
which the SMI Modelling algorithm estimated the index structure for
\(y_{1}\). For \(y_{2}\), all the models estimated resulted in a test
MSE of \(\approx 0.16\). This is an interesting result as the test MSE
of an estimated model with incorrect index structure, but with correct
predictors (``Linear'') is similar to the models with correct index
structure (``PPR'', ``Additive'', and ``Multiple''). This suggests that
the selection of the predictor variables is more important than
determining the index structure of the model. For both \(y_{1}\) and
\(y_{2}\), in the case of ``some \(\bm{x}\) + all \(\bm{z}\)'', the test
MSEs increased in comparison to the above cases, probably due to the
inclusion of noise variables.

Moreover, in contrast to \(y_{1}\), the test MSE values for \(y_{2}\)
are higher than the random squared error of the corresponding true
model. Intuitively, the complexity of the model \(y_{2}\) is higher than
\(y_{1}\), where the total estimation error of two nonlinear link
functions (corresponding to the two indices) for \(y_{2}\), might be
higher than the error of estimating a single nonlinear function for
\(y_{1}\).

\textcolor{orange}{XQ: The above six paragraphs should be merged into one short paragraph, comparing the index structure and forecast errors in the case of low noise level.}

As expected, the accuracy with which the SMI Modelling algorithm
estimates the index structure is in general lower with the high noise
level, in comparison to the low noise level. For both \(y_{1}\) and
\(y_{2}\), most of the estimated models have selected irrelevant
variables. In both the cases ``all \(\bm{x}\)'' and ``all \(\bm{x}\) +
all \(\bm{z}\)'', all the models estimated for \(y_{1}\) (except for
``Additive'' option in ``all \(\bm{x}\)'' case) resulted in a test MSE
of \(\approx 0.23\) (which is slightly lower than the random squared
error of the true model; this probably indicates a slight level of
over-fitting), irrespective of the fact that in ``all \(\bm{x}\) + all
\(\bm{z}\)'' case, ``Additive'' and ``Multiple'' options included a
noise variable. This is an indication of the effect of the low
signal-to-noise ratio in the data. The observation is the same for
\(y_{2}\), where irrespective of the different index structures and
predictor choices, the estimated models in the above two predictor
combinations produced similar test MSE values.

Similar to the previous case of low noise level, when only a part of
\(\bm{x}\) variables are provided, the test MSE values increased for
both \(y_{1}\) and \(y_{2}\), where the estimated models for \(y_{2}\)
produced higher test MSE values in comparison to the models for
\(y_{1}\).

\textcolor{orange}{XQ: The above four paragraphs should be merged into one short paragraph, comparing the index structure and forecast errors in the case of high noise level.}

It is worth mentioning here that in real-world forecasting problems, the
true data generating process (DGP) is unknown, and we do not expect an
estimated model to precisely capture the true DGP. Therefore, as long as
the estimated model demonstrates good forecasting accuracy, the index
structure of the estimated model is less important.

Finally, the simulation study indicates that the choice of the
initialisation depends on the data and application. Thus, the users are
encouraged to follow a trial-and-error procedure to determine the most
suitable initial model for a given application.

\hypertarget{sec-application}{%
\section{Empirical Applications}\label{sec-application}}

\hypertarget{sec-mortality}{%
\subsection{Forecasting Daily Mortality}\label{sec-mortality}}

We apply the SMI Modelling algorithm to a data set from
\textcite{Masselot2022}, to forecast daily mortality based on heat
exposure. Studying the effects of various environmental exposures such
as weather related variables, pollutants and man-made environmental
conditions etc. on human health, is of significant importance in
environmental epidemiology. Therefore, forecasting daily deaths taking
heat related variables as predictors is an interesting application.

\hypertarget{description-of-the-data}{%
\subsubsection{Description of the Data}\label{description-of-the-data}}

For this analysis, we consider daily mortality and heat exposure data
for the Metropolitan Area of Montreal, Province of Quebec, Canada, from
1990 to 2014, for the months June, July, and August (i.e.~summer
season). The daily all-cause mortality data were obtained from the
National Institute of Public Health, Province of Quebec, while
\emph{DayMet} --- a 1 km  1 km grid data set \autocite{Thornton2021}
was used to extract daily temperature and humidity data
\autocite{Masselot2022}.

Figure~\ref{fig-deaths} shows the time plots of daily deaths during the
summer for the years from 1990 to 1993. The series for each of the four
years are presented separately in a faceted grid for visual clarity.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-deaths-1.pdf}

}

\caption{\label{fig-deaths}Daily mortality in summer in Montreal, Canada
from 1990 to 1993.}

\end{figure}

The three main predictors considered in this empirical study are maximum
temperature, minimum temperature, and vapour pressure (to represent the
level of humidity). The number of daily deaths are plotted against each
of these predictors in Figure~\ref{fig-Tmax}, Figure~\ref{fig-Tmin}, and
Figure~\ref{fig-Vp}, respectively, where we can observe that the
relationships between these predictors and the response are slightly
non-linear.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-Tmax-1.pdf}

}

\caption{\label{fig-Tmax}Daily mortality in summer (from 1990 to 2014)
plotted against maximum temperature.}

\end{figure}

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-Tmin-1.pdf}

}

\caption{\label{fig-Tmin}Daily mortality in summer (from 1990 to 2014)
plotted against minimum temperature.}

\end{figure}

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-Vp-1.pdf}

}

\caption{\label{fig-Vp}Daily mortality in summer (from 1990 to 2014)
plotted against vapour pressure.}

\end{figure}

\hypertarget{predictors-considered}{%
\subsubsection{Predictors Considered}\label{predictors-considered}}

\textbf{1) Current maximum/minimum temperatures and lags:}

In addition to current maximum and minimum temperatures, the temperature
measurements up to 14 days prior (i.e.~\(0^{th}\) to \(14^{th}\) lag)
are considered as predictors in the forecasting model. This accounts for
the cumulative impact of both current and recent past temperatures on a
person's heat exposure.

\textbf{2) Current vapour pressure and lags:}

Similar to temperature variables, the current value and 14 lags of
vapour pressure are considered as predictors, as a proxy to the level of
humidity.

\textbf{3) Calendar effects:}

Finally, a couple of calendar variables; \emph{day of the season (DOS)}
and \emph{Year}, are incorporated into the model to capture annual trend
and seasonality, and also to control the autocorrelation in residuals,
which is a common practice in environmental epidemiology
\autocite{Masselot2022}.

\hypertarget{modelling-framework}{%
\subsubsection{Modelling Framework}\label{modelling-framework}}

Maximum temperature lags, minimum temperature lags, and vapour pressure
lags are considered as predictors entering indices. The two calendar
variables, \emph{DOS} and \emph{Year}, are included into the model as
separate nonparametric components that do not enter any of the indices.

Hence, the relevant SMI Model can be written as
\begin{equation}\protect\hypertarget{eq-heat}{}{
\begin{aligned}
  \textbf{Deaths} = & \beta_{0} + \sum_{j = 1}^{p}{g_{j}(\bm{X}\bm{\alpha}_{j})} + f_{1}(\textbf{DOS}) + f_{2}(\textbf{Year})+ \bm{\varepsilon},
\end{aligned}
}\label{eq-heat}\end{equation} where

\begin{itemize}
\item
  \(\textbf{Deaths}\) is the vector containing daily deaths
  observations;
\item
  \(\beta_{0}\) is the model intercept;
\item
  \(p\) is the unknown number of indices that need to be estimated
  through the algorithm;
\item
  \(\bm{X}\) is the matrix containing the predictor variables that are
  entering indices (i.e.~maximum temperature lags, minimum temperature
  lags, and vapour pressure lags);
\item
  \(\bm{\alpha}_{j}, j = 1, \dots, p\) are the index coefficient
  vectors, each with a length equal to the number of predictors entering
  indices (\(q=45\));
\item
  \(g_{j}, j = 1, \dots, p\), \(f_{1}\), and \(f_{2}\) are unknown
  nonparametric functions; and
\item
  \(\bm{\varepsilon}\) is the error term.
\end{itemize}

The data from 1990 to 2012 are used as the training set to estimate the
model, while the data of year 2014 are separated to be the test set for
evaluating forecasting performance. The data from the three summer
months of year 2013 are kept aside as a validation set, which is used to
estimate benchmark models for comparison purposes.

Then we apply the proposed SMI Modelling algorithm to the training set
to estimate the model. Finally, the forecasting accuracy on the test set
is evaluated using MSE and Mean Absolute Error (MAE).

\hypertarget{results}{%
\subsubsection{Results}\label{results}}

We estimated SMI Models for the mortality data using three different
initialisation options: ``PPR'', ``Additive'' and ``Linear'', for
comparison purposes. Through our pre-experiments on the new algorithm,
we identified that the ``PPR'' initialisation option has a higher
probability of better performance, whereas ``Additive'' (i.e.~Additive
Model) and ``Linear'' (i.e.~Single Index Model) are two special cases of
the SMI Model. We did not consider ``Multiple'' and ``User Input''
initialisations here as both of these two options require user specific
inputs to some extend. Further, we tuned the penalty parameters
\(\lambda_{0}\) and \(\lambda_{2}\), over ranges of integers from 1 to
12, and 0 to 12 respectively, through a greedy search based on in-sample
MSE. Here, a greedy search is used instead of a grid search to reduce
computational time.

The penalty parameter combination
\((\lambda_{0} = 12, \lambda_{2} = 0)\) was selected for the model
fitted with ``PPR'' initialisation. The estimated model,
\textbf{\emph{SMI Model (12, 0) - PPR }}, resulted in five indices
wihout dropping any of the index variables. The optimal penalty
parameter combination for the model initiated with ``Additive'' was
\((\lambda_{0} = 1, \lambda_{2} = 0)\), resulting in the
\textbf{\emph{SMI Model (1, 0) - Additive }}, equivalent to a
nonparametric additive model (no index variables or indices were
dropped). The model estimated with ``Linear'' initialisation selected
\((\lambda_{0} = 12, \lambda_{2} = 5)\) (\textbf{\emph{SMI Model (12, 5)
- Linear }}), and resulted in two indices, without dropping any of the
index variables.

We evaluated forecasting errors of the estimated models using two
subsets of the original test set:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{\emph{Test Set 1:}} original test set spanning 3 months (June,
  July and August 2014); and
\item
  \textbf{\emph{Test Set 2:}} a test set covering 1 month (June 2014).
\end{enumerate}

Note that in this application, we assumed that the future values of the
maximum/minimum temperatures and vapour pressure are known to use in the
forecasting model.

The MSE and MAE values for the estimated SMI Models on two different
test sets are presented in Table~\ref{tbl-heat}. We observe that the SMI
Model estimated with ``PPR'' initialisation, \textbf{\emph{SMI Model
(12, 0) - PPR }}, shows the best forecasting performance on both test
sets, compared to the other two estimated SMI models.

Furthermore, we present the forecasting errors of three benchmark models
in Table~\ref{tbl-heat} for comparison with the estimated SMI Models.
The first benchmark is a nonparametric additive model formulated through
backward elimination, as proposed by \textcite{FH2012}
(Section~\ref{sec-backward}). Next, a GAIM
(Section~\ref{sec-multi-index}) is also presented. In the case of GAIM,
maximum temperature lags, minimum temperature lags, and vapour pressure
lags are categorised into three groups, where an index estimated for
each group. Finally, we present the forecasting errors of a PPR model.
The number of indices in the PPR model was taken as 3, matching the
number of indices estimated by the GAIM.

\hypertarget{tbl-heat}{}
\begin{table}[!h]
\caption{\label{tbl-heat}Daily mortality forecasting - Out-of-sample point forecast results. }\tabularnewline

\centering
\begin{tabular}{lrr>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Test Set 1} & \multicolumn{2}{c}{Test Set 2} \\
\cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7}
\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Predictors} & \multicolumn{1}{c}{Indices} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE}\\
\midrule
smimodel(12, 0) - PPR & 47 & 5 & \textbf{80.334} & \textbf{6.841} & \textbf{99.926} & \textbf{7.643}\\
smimodel(1, 0) - Additive & 47 & 45 & 151.408 & 9.816 & 190.880 & 11.107\\
smimodel(12, 5) - Linear & 47 & 2 & 164.629 & 10.153 & 207.040 & 11.141\\
Backward Elimination & 36 & NA & 148.387 & 9.808 & 162.608 & 10.034\\
GAIM & 47 & 3 & 85.145 & 7.257 & 103.494 & 8.480\\
\addlinespace
PPR & 47 & 3 & 82.877 & 7.202 & 104.217 & 8.404\\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tbl-heat} shows that \textbf{\emph{SMI Model (12, 0) - PPR}}
outperforms all three benchmark models in terms of forecasting accuracy,
for both \emph{Test Set 1} and \emph{Test Set 2}. However, the SMI
Models estimated using ``Additive'' or ``Linear'' initialisations have
inferior forecasting performance compared to all benchmark models
considered. The actual number of deaths and the predicted values from
the \textbf{\emph{SMI Model (12, 0) - PPR}} and benchmark models on
\emph{Test Set 2} are plotted in Figure~\ref{fig-heatPred} for further
comparison.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-heatPred-1.pdf}

}

\caption{\label{fig-heatPred}Actual number of deaths vs.~predicted
number of deaths from ``SMI Model (12, 0) - PPR'' and benchmark models
for Test Set 2.}

\end{figure}

\hypertarget{sec-solar}{%
\subsection{Forecasting Daily Solar Intensity}\label{sec-solar}}

Next, we utilise the SMI Modelling algorithm to forecast daily solar
intensity, using other weather conditions. As reported by
\textcite{EI2023}, renewable energy (excluding hydroelectricity)
contributed to 7.5\% of the world's primary energy consumption in 2022.
Solar and wind power saw a combined capacity addition of 266 GW, with
solar energy accounting for 72\% of the increase. Given this, accurate
forecasting of solar power generation, closely linked to solar
intensity, is crucial for effective power system planning and
management.

\hypertarget{description-of-the-data-1}{%
\subsubsection{Description of the
Data}\label{description-of-the-data-1}}

We use solar intensity and other weather variables measured at a Davis
weather station in Amherst, Massachusetts, obtained from the \emph{UMass
Trace Repository} \autocite{Umass2023}. The data was recorded at every
five minutes, from 21th February 2006 to 27th February 2013, using
sensors for measuring temperature, wind chill, humidity, dew point, wind
speed, wind direction, rain, pressure, solar intensity, and UV.

However, the data contained missing entries recorded as ``-100000'',
which we removed from the data set. Moreover, for this analysis, we
converted the five minutes data to daily data by averaging each variable
over days.

Figure~\ref{fig-solar} shows the time plot of daily solar intensity for
the entire period, which clearly depicts the annual seasonality in the
data. As observed in Figure~\ref{fig-solar}, there are days for which
the observations were missing. We excluded those days from the analysis,
and used only the days for which the data are available.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-solar-1.pdf}

}

\caption{\label{fig-solar}Daily solar intensity in Amherst,
Massachusetts - from February 2006 to February 2013.}

\end{figure}

The variables temperature, dew point, wind, rain and humidity were
considered to be the main set of predictors in the model. The daily
solar intensity is plotted against each of these predictors in
Figure~\ref{fig-preds}, where we can observe that the relationships
between these predictors and the response are non-linear.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-preds-1.pdf}

}

\caption{\label{fig-preds}Daily solar intensity against other weather
variables.}

\end{figure}

\hypertarget{predictors-considered-1}{%
\subsubsection{Predictors Considered}\label{predictors-considered-1}}

\textbf{1) Solar intensity lags:}

Three lags of the daily solar intensity itself are used as predictors to
incorporate the serial correlations presented in the data into the
modelling process. Intuitively, the solar intensity of a particular day
would have a relationship to the solar intensity of adjacent days.

\textbf{2) Current weather variables and lags:}

In addition to current temperature, dew point, wind speed, rain and
humidity, the measurements of three previous days (i.e.~\(0^{th}\) to
\(3^{rd}\) lag) for each of the these weather variables are also
included as predictors in the forecasting model.

\textbf{3) Calendar effects:}

Finally, a couple of calendar variables; \emph{Month} (12 months of the
year) and \emph{Season} (the four seasons: Spring, Summer, Autumn and
Winter), are incorporated into the model to capture annual seasonality,
and control for autocorrelation in residuals.

\hypertarget{modelling-framework-1}{%
\subsubsection{Modelling Framework}\label{modelling-framework-1}}

The lags of solar intensity, and the lags of weather variables are
considered as predictors that are entering indices. The two calendar
variables, \emph{Month} and \emph{Season}, are included into the model
as linear (categorical) predictor variables.

Hence, the relevant SMI Model can be written as
\begin{equation}\protect\hypertarget{eq-solar}{}{
\begin{aligned}
  \textbf{Solar} = & \beta_{0} + \sum_{j = 1}^{p}{g_{j}(\bm{X}\bm{\alpha}_{j})} + \theta_{1}\textbf{Month} + \theta_{2}\textbf{Season}+ \bm{\varepsilon},
\end{aligned}
}\label{eq-solar}\end{equation} where

\begin{itemize}
\item
  \(\textbf{Solar}\) is the vector containing daily observations of
  solar intensity;
\item
  \(\beta_{0}\) is the model intercept;
\item
  \(p\) is the unknown number of indices that will be estimated through
  the algorithm;
\item
  \(\bm{X}\) is the matrix containing the predictor variables that are
  entering indices (i.e.~solar intensity, temperature, dew point, wind
  speed, rain and humidity lags);
\item
  \(\bm{\alpha}_{j}, j = 1, \dots, p\) are the index coefficient
  vectors, each of lenth equal to the number of predictors entering
  indices (\(q= 23\));
\item
  \(g_{j}, j = 1, \dots, p\) are unknown nonparametric functions;
\item
  \(\theta_{1}\) and \(\theta_{2}\) are the two coefficients
  corresponding to the two linear predictor variables; and
\item
  \(\bm{\varepsilon}\) is the error term.
\end{itemize}

The data from February 2006 to October 2012 are used as the training set
to estimate the model, while the data of the months January and February
2013 are separated to be the test set to evaluate the forecasting
performance. The data from the months November and December 2013 are
kept aside as a validation set, which is required to estimate some of
the benchmark models for comparison.

Then we apply the proposed SMI Modelling algorithm to the training set
to estimate the model, and the forecasting accuracy on the test set is
evaluated using MSE and MAE.

\hypertarget{results-1}{%
\subsubsection{Results}\label{results-1}}

Similar to the previous empirical application, we estimated SMI Models
for the solar intensity data using three different initialisation
options: ``PPR'', ``Additive'' and ``Linear'', for comparison purposes.
We also tuned the penalty parameters \(\lambda_{0}\) and
\(\lambda_{2}\), over ranges of integers from 1 to 12, and 0 to 12
respectively.

The penalty parameter combination
\((\lambda_{0} = 12, \lambda_{2} = 0)\) was selected for the model
fitted with ``PPR'' initialisation. The estimated model,
\textbf{\emph{SMI Model (12, 0) - PPR }}, resulted in five indices
without dropping any of the index variables. The optimal penalty
parameter combination for the model estimated taking ``Additive'' model
as the starting point was \((\lambda_{0} = 1, \lambda_{2} = 0)\). The
estimated SMI Model did not drop any index variables or indices, and
thus the final model, \textbf{\emph{SMI Model (1, 0) - Additive }}, is
equivalent to a nonparametric additive model. The model estimated with
``Linear'' initialisation also selected
\((\lambda_{0} = 1, \lambda_{2} = 0)\). Unlike the above models, this
SMI Model dropped all index variables and resulted in null indices, and
hence, the final model, \textbf{\emph{SMI Model (1, 0) - Linear }}, is
just a linear model with the two linear variables \emph{Month} and
\emph{Season}. Notice that all three estimated SMI Models have
\(\lambda_{2} = 0\), indicating that all three models have omitted the
\(\ell_{2}\)-penalty in the estimation process.

Note that similar to the previous application of heat related mortality
forecasting, we assumed that the future values of the weather variables
are known to use in the forecasting model.

Table~\ref{tbl-solar} presents the MSE and MAE values for the estimated
SMI Models on the test set. The results indicate that the SMI Model
estimated with ``Additive'' initialisation, \textbf{\emph{SMI Model (1,
0) - Additive}}, shows the best forecasting performance among the three
estimated SMI Models.

Similar to Section~\ref{sec-mortality}, we also present forecasting
errors of three benchmark models in Table~\ref{tbl-solar}, to compare
with the estimated SMI Models. Here, the GAIM is fitted by grouping the
lags of each weather variable into a different group, resulting in six
indices. The number of indices of the PPR model was taken as six,
matching the number of indices estimated by the GAIM. Note that here,
the two categorical calendar variables were excluded when estimating the
PPR model.

\hypertarget{tbl-solar}{}
\begin{table}[!h]
\caption{\label{tbl-solar}Daily solar intensity forecasting - Out-of-sample point forecast
results. }\tabularnewline

\centering
\begin{tabular}{lrr>{}r>{}r}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Test Set} \\
\cmidrule(l{3pt}r{3pt}){4-5}
\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Predictors} & \multicolumn{1}{c}{Indices} & \multicolumn{1}{c}{MSE} & \multicolumn{1}{c}{MAE}\\
\midrule
SMI Model (12, 0) - PPR & 25 & 5 & 1745.030 & 33.246\\
SMI Model (1, 0) - Additive & 25 & 23 & 1112.181 & 26.975\\
SMI Model (1, 0) - Linear & 2 & 0 & 2009.847 & 35.346\\
Backward Elimination & 16 & NA & 911.570 & 25.035\\
GAIM & 25 & 6 & 2203.530 & 37.788\\
\addlinespace
PPR & 23 & 6 & \textbf{796.779} & \textbf{22.455}\\
\bottomrule
\end{tabular}
\end{table}

According to Table~\ref{tbl-solar}, the forecasting errors of
\textbf{\emph{SMI Model (1, 0) - Additive}} is lower than the GAIM.
However, the \textbf{\emph{SMI Model (1, 0) - Additive}} is unable to
outperform both the semi-parametric additive model with backward
elimination and the PPR model, where in this case, the estimated PPR
model has resulted in the best forecasting accuracy.

Here, it is worth considering the differences between the SMI Model and
the benchmark models that show superior forecasting performance. The
method proposed by \textcite{FH2012} formulates a semi-parametric
additive model using a backward elimination of predictors. When
estimating a PPR model, both the number of indices and the predictors
within each index (each index includes all provided predictors that are
entering indices) are pre-determined. In contrast, the SMI Model takes a
more general and objective approach, where the number of indices as well
as predictors within each index are automatically determined through the
proposed algorithm. Thus, the SMI Model faces a more challenging
estimation task due to the limited prior information provided.

The actual solar intensity and the predicted values from the SMI Model
(1, 0) - Additive and benchmark models are plotted in
Figure~\ref{fig-solarPred} for further comparison.

\begin{figure}

{\centering \includegraphics{smimodel_files/figure-pdf/fig-solarPred-1.pdf}

}

\caption{\label{fig-solarPred}Actual solar intensity vs.~predicted solar
intensity from ``SMI Model (1, 0) - Additive'' and benchmark models.}

\end{figure}

In summary, the two empirical applications presented above highlight the
challenge of finding a universally applicable initialisation option for
the SMI Model across various applications. As mentioned in
Section~\ref{sec-simulation}, we encourage users to follow a
trial-and-error procedure to identify the most effective initialisation
option for their specific application.

The two empirical applications were performed using R statistical
software \autocite{R2023}, and the Rstudio integrated development
environment \autocite[IDE,][]{Rstudio2024}. We used the commercial MIP
solver \textbf{\emph{Gurobi}} \autocite{gurobi2023} to solve the MIQPs
related to the proposed SGAIM algorithm, through the
\textbf{\emph{Gurobi plug-in}}
\autocite[ROI.plugin.gurobi,][]{Schwendinger2023} available from the
\textbf{\emph{R Optimization Infrastructure}}
\autocites[ROI,][]{Hornik2023,Theusl2020} package. Furthermore, the GAMs
were fitted using the R package \textbf{\emph{mgcv}}
\autocite[v1.9.1,][]{Wood2011}.

\hypertarget{sec-conclusion}{%
\section{Conclusions and Further Research}\label{sec-conclusion}}

In this paper, we presented a novel algorithm for estimating a
nonparametric additive index model with optimal predictor selection,
which we refer to as Sparse Multiple Index (SMI) Model. The SMI
Modelling algorithm is an iterative procedure that is developed based on
mixed integer programming to solve an \(\ell_{0}\)-regularised nonlinear
least squares optimisation problem with linear constraints.

The proposed SMI Modelling algorithm has a number of key features: 1) It
performs automatic selection of both the number of indices and the
predictor grouping when estimating the nonparametric additive index
model. Users need to input the set of predictors entering indices and a
starting model (index structure and a set of index coefficients) to
initiate the algorithm. 2) It performs automatic variable selection,
which is particularly beneficial in high-dimensional settings. This
feature contributes to an objective and principled estimation, reducing
subjectivity across different users. 3) It is capable of estimating a
wide spectrum of models, from single index models (one index) to
additive models (number of indices equals the number of predictors
entering indices). Hence, the SMI Modelling algorithm is a more general
estimation tool for nonparametric additive models. 4) It provides the
flexibility to include separate non-liner and linear predictors in the
model that are not entering any indices, allowing the estimation of
semi-parametric additive models.

Due to the limited input information provided to the algorithm, the
estimation of a SMI Model is a challenging problem. We demonstrated the
performance of the proposed algorithm through a simple simulation and
two empirical applications. Since we observed that the final estimated
model changes with the choosen initialisation, one limitation of the
proposed algorithm is the difficulty of specifying an initialisation
that works in general. Hence, an interesting future research problem
would be to explore the potential for determining a generalised
initialisation for the SMI Modelling algorithm that will work across
various applications.

Moreover, we admit that the empirical examples presented in the paper
may not be diverse enough to draw definitive conclusions about the
unique strengths or weaknesses of the proposed algorithm. This study
should be viewed as an attempt to develop a more objective methodology
for variable selection and model estimation in the broader class of
nonparametric additive models for forecasting. An important future
research problem is therefore, to assess the performance of the proposed
SMI Modelling algorithm across various data sets with diverse
properties, identifying scenarios where it outperforms other benchmark
methods.

Furthermore, the MIQP in the algorithm is somewhat analogous to the
\emph{best subset selection} method frequently used in least squares
problems. Thus, another limitation of the proposed algorithm is the
increase in computational time as the number of predictors and number of
indices increase. Therefore, it would be an interesting research to
obtain further insights regarding the algorithm to see what improvements
can be made to the algorithm design to reduce the computational cost in
a high-dimensional context.

\textbf{\large{Acknowledgements}}

We thank Professor Louise Ryan for joining the discussions during the
initial stage of the project, and for her valuable comments and feedback
on this research work.

Furthermore, this research is partially supported by the Monash
eResearch Centre through the use of the MonARCH (Monash Advanced
Research Computing Hybrid) HPC Cluster.

\printbibliography

\end{document}
